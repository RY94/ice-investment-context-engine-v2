# tests/test_unified_rag.py
# Comprehensive test suite for ICE Unified RAG system
# Tests engine switching, performance comparison, and unified interface
# Validates compatibility between LightRAG and LazyGraphRAG systems

import os
import time
import pytest
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional
from unittest.mock import patch, MagicMock

# Import test fixtures and unified RAG system
from test_fixtures import TestDataFixtures, TestEnvironmentManager, create_test_config

# Import Unified RAG components
import sys
sys.path.append('..')

try:
    from ice_unified_rag import ICEUnifiedRAG, RAGEngineInfo
    UNIFIED_RAG_AVAILABLE = True
except ImportError as e:
    print(f"Unified RAG not available: {e}")
    UNIFIED_RAG_AVAILABLE = False


class TestICEUnifiedRAG:
    """Comprehensive test suite for ICE Unified RAG system"""
    
    def __init__(self):
        self.fixtures = TestDataFixtures()
        self.env_manager = TestEnvironmentManager()
        self.config = create_test_config()
        self.unified_rag = None
        self.test_results = {}
    
    def setup_method(self):
        """Set up test environment before each test"""
        self.test_dir = self.env_manager.create_test_environment("unified_rag")
        
        if UNIFIED_RAG_AVAILABLE:
            self.unified_rag = ICEUnifiedRAG(
                default_engine="lazyrag",  # Start with LazyRAG as it's more reliable
                working_dir=os.path.join(self.test_dir, "unified_storage")
            )
        else:
            self.unified_rag = None
    
    def teardown_method(self):
        """Clean up after each test"""
        self.env_manager.cleanup_environment("unified_rag")
    
    def test_system_availability(self):
        """Test Unified RAG system availability and engine detection"""
        print("\nüß™ Testing Unified RAG System Availability...")
        
        # Test import availability
        assert UNIFIED_RAG_AVAILABLE is not None, "UNIFIED_RAG_AVAILABLE should be defined"
        
        if not UNIFIED_RAG_AVAILABLE:
            print("‚ö†Ô∏è Unified RAG not available - will skip integration tests")
            return True
        
        assert self.unified_rag is not None, "Unified RAG instance should be created"
        
        # Test engine detection
        available_engines = self.unified_rag.get_available_engines()
        assert isinstance(available_engines, dict), "Available engines should be dictionary"
        assert len(available_engines) > 0, "Should detect at least one engine"
        
        # Validate engine info structure
        for engine_name, engine_info in available_engines.items():
            assert isinstance(engine_info, RAGEngineInfo), "Engine info should be RAGEngineInfo instance"
            assert hasattr(engine_info, 'name'), "Engine info should have name"
            assert hasattr(engine_info, 'available'), "Engine info should have availability status"
            assert hasattr(engine_info, 'description'), "Engine info should have description"
            
            if engine_info.available:
                print(f"‚úÖ {engine_info.name} available: {engine_info.description}")
            else:
                print(f"‚ùå {engine_info.name} unavailable: {engine_info.error_message}")
        
        print("‚úÖ Unified RAG system availability tests passed")
        return True
    
    def test_initialization_and_readiness(self):
        """Test unified system initialization and readiness"""
        print("\nüß™ Testing Unified RAG Initialization...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping initialization test - Unified RAG not available")
            return True
        
        # Test current engine detection
        current_engine = self.unified_rag.get_current_engine()
        assert current_engine is not None, "Should have a current engine selected"
        
        # Test readiness check
        is_ready = self.unified_rag.is_ready()
        assert isinstance(is_ready, bool), "is_ready() should return boolean"
        
        if not is_ready:
            print(f"‚ö†Ô∏è Unified RAG not ready with current engine: {current_engine}")
            # Try to set any available engine
            available_engines = self.unified_rag.get_available_engines()
            for engine_name, engine_info in available_engines.items():
                if engine_info.available:
                    success = self.unified_rag.set_engine(engine_name)
                    if success and self.unified_rag.is_ready():
                        print(f"‚úÖ Successfully switched to {engine_name}")
                        break
        
        # Test stats retrieval
        stats = self.unified_rag.get_stats()
        assert isinstance(stats, dict), "Stats should return dictionary"
        assert "current_engine" in stats, "Stats should include current engine"
        assert "available_engines" in stats, "Stats should include available engines"
        
        print(f"‚úÖ Unified RAG initialization tests passed (current engine: {current_engine})")
        return True
    
    def test_engine_switching(self):
        """Test switching between different RAG engines"""
        print("\nüß™ Testing Engine Switching...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping engine switching test - Unified RAG not available")
            return True
        
        available_engines = self.unified_rag.get_available_engines()
        available_engine_names = [name for name, info in available_engines.items() if info.available]
        
        if len(available_engine_names) < 2:
            print(f"‚ö†Ô∏è Only {len(available_engine_names)} engine(s) available - limited switching test")
            if len(available_engine_names) == 1:
                # Test switching to the same engine
                engine_name = available_engine_names[0]
                original_engine = self.unified_rag.get_current_engine()
                success = self.unified_rag.set_engine(engine_name)
                assert success, f"Should successfully switch to available engine {engine_name}"
                assert self.unified_rag.get_current_engine() == engine_name, "Current engine should update"
            return True
        
        # Test switching between available engines
        engine_switch_results = {}
        original_engine = self.unified_rag.get_current_engine()
        
        for engine_name in available_engine_names:
            print(f"üîÑ Testing switch to {engine_name}...")
            
            success = self.unified_rag.set_engine(engine_name)
            engine_switch_results[engine_name] = success
            
            if success:
                assert self.unified_rag.get_current_engine() == engine_name, f"Current engine should be {engine_name}"
                
                # Test readiness after switch
                is_ready = self.unified_rag.is_ready()
                assert is_ready, f"Engine {engine_name} should be ready after switch"
                
                print(f"‚úÖ Successfully switched to {engine_name}")
            else:
                print(f"‚ùå Failed to switch to {engine_name}")
        
        # Test switching to invalid engine
        invalid_success = self.unified_rag.set_engine("invalid_engine_name")
        assert not invalid_success, "Should fail to switch to invalid engine"
        
        successful_switches = sum(engine_switch_results.values())
        assert successful_switches > 0, "Should successfully switch to at least one engine"
        
        print(f"‚úÖ Engine switching tests passed ({successful_switches}/{len(available_engine_names)} engines)")
        return True
    
    def test_unified_document_processing(self):
        """Test document processing through unified interface"""
        print("\nüß™ Testing Unified Document Processing...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping document processing test - Unified RAG not available")
            return True
        
        # Ensure we have a ready engine
        if not self.unified_rag.is_ready():
            available_engines = self.unified_rag.get_available_engines()
            for engine_name, engine_info in available_engines.items():
                if engine_info.available:
                    if self.unified_rag.set_engine(engine_name) and self.unified_rag.is_ready():
                        break
            else:
                print("‚ö†Ô∏è No ready engines available for document processing test")
                return True
        
        # Test single document processing
        sample_doc = self.fixtures.get_sample_document("earnings_transcript")
        
        start_time = time.time()
        result = self.unified_rag.add_document(sample_doc["text"], sample_doc["type"])
        processing_time = time.time() - start_time
        
        # Validate result
        assert isinstance(result, dict), "Result should be dictionary"
        assert "status" in result, "Result should have status field"
        
        if result["status"] == "error":
            print(f"‚ö†Ô∏è Document processing failed: {result.get('message', 'Unknown error')}")
            return True  # Don't fail completely - engine may have limitations
        
        assert result["status"] == "success", f"Document processing should succeed: {result}"
        assert processing_time < self.config["performance_benchmarks"]["max_document_processing_time"], f"Processing too slow: {processing_time:.2f}s"
        
        # Test batch document processing across different engines
        documents_processed_per_engine = {}
        available_engines = [name for name, info in self.unified_rag.get_available_engines().items() if info.available]
        
        for engine_name in available_engines[:2]:  # Test first 2 available engines
            if self.unified_rag.set_engine(engine_name) and self.unified_rag.is_ready():
                docs_processed = 0
                
                for doc in self.fixtures.test_documents[:2]:  # Test 2 documents per engine
                    result = self.unified_rag.add_document(doc["text"], doc["type"])
                    if result["status"] == "success":
                        docs_processed += 1
                
                documents_processed_per_engine[engine_name] = docs_processed
        
        total_processed = sum(documents_processed_per_engine.values())
        assert total_processed > 0, "Should process at least one document across all engines"
        
        print(f"‚úÖ Document processing tests passed ({total_processed} total documents)")
        for engine, count in documents_processed_per_engine.items():
            print(f"   {engine}: {count} documents")
        
        return True
    
    def test_unified_querying(self):
        """Test querying through unified interface"""
        print("\nüß™ Testing Unified Querying...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping unified querying test - Unified RAG not available")
            return True
        
        # Ensure we have a ready engine with some data
        if not self.unified_rag.is_ready():
            available_engines = self.unified_rag.get_available_engines()
            for engine_name, engine_info in available_engines.items():
                if engine_info.available:
                    if self.unified_rag.set_engine(engine_name) and self.unified_rag.is_ready():
                        break
            else:
                print("‚ö†Ô∏è No ready engines available for querying test")
                return True
        
        # Add some test data for querying
        sample_doc = self.fixtures.get_sample_document()
        doc_result = self.unified_rag.add_document(sample_doc["text"], sample_doc["type"])
        
        # Test different query modes and parameters
        test_queries = [
            {"query": "What are NVIDIA's main risks?", "mode": "hybrid"},
            {"query": "Tell me about semiconductor supply chains", "mode": "semantic"},
            {"query": "How does geopolitical tension affect tech companies?", "mode": "local"}
        ]
        
        successful_queries = 0
        query_results = []
        
        for test_case in test_queries:
            query_text = test_case["query"]
            mode = test_case["mode"]
            
            start_time = time.time()
            result = self.unified_rag.query(query_text, mode=mode)
            query_time = time.time() - start_time
            
            # Validate result structure
            assert isinstance(result, dict), f"Query result should be dictionary for: {query_text}"
            assert "status" in result, f"Result should have status for: {query_text}"
            assert "engine" in result, f"Result should include engine name for: {query_text}"
            assert "query_time" in result, f"Result should include timing for: {query_text}"
            
            if result["status"] == "error":
                print(f"‚ö†Ô∏è Query failed: {query_text} - {result.get('message', 'Unknown error')}")
                continue
            
            assert result["status"] == "success", f"Query should succeed for: {query_text}"
            assert "answer" in result, f"Should have answer field for: {query_text}"
            assert len(result["answer"]) > 0, f"Answer should not be empty for: {query_text}"
            assert query_time < self.config["performance_benchmarks"]["max_query_time"], f"Query too slow: {query_time:.2f}s"
            
            # Validate response quality
            response_issues = self.fixtures.validate_response({"result": result["answer"]}, "general")
            if response_issues:
                print(f"‚ö†Ô∏è Response quality issues for {mode}: {response_issues}")
            
            successful_queries += 1
            query_results.append({
                "query": query_text,
                "mode": mode,
                "engine": result["engine"],
                "time": query_time,
                "answer_length": len(result["answer"])
            })
        
        assert successful_queries > 0, "Should have at least one successful query"
        
        # Test query history
        stats = self.unified_rag.get_stats()
        if "query_history_count" in stats:
            assert stats["query_history_count"] >= successful_queries, "Query history should track queries"
        
        print(f"‚úÖ Unified querying tests passed ({successful_queries}/{len(test_queries)} queries)")
        return True
    
    def test_engine_comparison(self):
        """Test engine comparison functionality"""
        print("\nüß™ Testing Engine Comparison...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping engine comparison test - Unified RAG not available")
            return True
        
        available_engines = self.unified_rag.get_available_engines()
        available_count = sum(1 for info in available_engines.values() if info.available)
        
        if available_count < 2:
            print(f"‚ö†Ô∏è Only {available_count} engine(s) available - limited comparison test")
            if available_count == 1:
                # Test comparison with single engine
                test_query = "What are the main risks in semiconductor industry?"
                result = self.unified_rag.compare_engines(test_query, modes=["hybrid"])
                
                assert isinstance(result, dict), "Comparison result should be dictionary"
                assert "question" in result, "Should include original question"
                assert "results" in result, "Should include results"
                
                print("‚úÖ Single engine comparison test passed")
            return True
        
        # Test comparison with multiple engines
        test_query = "How do supply chain disruptions affect semiconductor companies?"
        
        # Add some test data first
        sample_doc = self.fixtures.get_sample_document()
        self.unified_rag.add_document(sample_doc["text"], sample_doc["type"])
        
        start_time = time.time()
        comparison_result = self.unified_rag.compare_engines(
            test_query,
            modes=["hybrid", "semantic"]
        )
        comparison_time = time.time() - start_time
        
        # Validate comparison result structure
        assert isinstance(comparison_result, dict), "Comparison result should be dictionary"
        assert "question" in comparison_result, "Should include original question"
        assert "modes_tested" in comparison_result, "Should include tested modes"
        assert "results" in comparison_result, "Should include results"
        assert "timestamp" in comparison_result, "Should include timestamp"
        
        # Validate individual engine results
        results = comparison_result["results"]
        tested_engines = 0
        successful_engines = 0
        
        for engine_name, engine_results in results.items():
            if engine_name in available_engines and available_engines[engine_name].available:
                tested_engines += 1
                
                for mode, mode_result in engine_results.items():
                    assert isinstance(mode_result, dict), f"Mode result should be dict for {engine_name}:{mode}"
                    assert "success" in mode_result, f"Should have success status for {engine_name}:{mode}"
                    assert "query_time" in mode_result, f"Should have timing for {engine_name}:{mode}"
                    
                    if mode_result["success"]:
                        successful_engines += 1
                        assert "answer_length" in mode_result, f"Should have answer length for {engine_name}:{mode}"
                        assert mode_result["answer_length"] > 0, f"Answer should not be empty for {engine_name}:{mode}"
        
        assert tested_engines >= 2, "Should test at least 2 engines"
        assert successful_engines > 0, "Should have at least one successful engine comparison"
        assert comparison_time < self.config["performance_benchmarks"]["max_query_time"] * tested_engines * 2, "Comparison taking too long"
        
        print(f"‚úÖ Engine comparison tests passed ({successful_engines} successful comparisons in {comparison_time:.2f}s)")
        return True
    
    def test_edge_management_unified(self):
        """Test edge management through unified interface"""
        print("\nüß™ Testing Unified Edge Management...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping edge management test - Unified RAG not available")
            return True
        
        # Find an engine that supports edge management (likely LazyRAG)
        available_engines = self.unified_rag.get_available_engines()
        edge_capable_engine = None
        
        for engine_name, engine_info in available_engines.items():
            if engine_info.available and "lazy" in engine_name.lower():
                if self.unified_rag.set_engine(engine_name) and self.unified_rag.is_ready():
                    edge_capable_engine = engine_name
                    break
        
        if not edge_capable_engine:
            print("‚ö†Ô∏è No edge-capable engines available - testing fallback behavior")
            
            # Test edge management with non-supporting engine
            sample_edges = self.fixtures.get_sample_edges(3)
            result = self.unified_rag.add_edges_from_data(sample_edges)
            
            assert isinstance(result, dict), "Edge result should be dictionary"
            assert "status" in result, "Should have status field"
            
            # Should gracefully handle unsupported operation
            if result["status"] == "warning":
                print("‚úÖ Gracefully handled unsupported edge operation")
            
            return True
        
        # Test edge management with capable engine
        sample_edges = self.fixtures.get_sample_edges(5)
        
        start_time = time.time()
        result = self.unified_rag.add_edges_from_data(sample_edges)
        processing_time = time.time() - start_time
        
        # Validate result
        assert isinstance(result, dict), "Result should be dictionary"
        assert "status" in result, "Result should have status field"
        
        if result["status"] == "error":
            print(f"‚ö†Ô∏è Edge addition failed: {result.get('message', 'Unknown error')}")
            return False
        
        assert result["status"] == "success", f"Edge addition should succeed: {result}"
        assert processing_time < self.config["performance_benchmarks"]["max_edge_addition_time"] * len(sample_edges), f"Edge processing too slow: {processing_time:.2f}s"
        
        # Test entity context after edge addition
        if hasattr(self.unified_rag, 'get_entity_context'):
            test_entity = sample_edges[0]["source"]
            context = self.unified_rag.get_entity_context(test_entity)
            
            assert isinstance(context, dict), "Entity context should be dictionary"
        
        # Test path finding if supported
        if hasattr(self.unified_rag, 'find_paths'):
            paths = self.unified_rag.find_paths("NVDA", "China", max_hops=3)
            assert isinstance(paths, list), "Paths should be list"
        
        print(f"‚úÖ Unified edge management tests passed ({len(sample_edges)} edges with {edge_capable_engine})")
        return True
    
    def test_performance_comparison(self):
        """Test performance comparison between engines"""
        print("\nüß™ Testing Performance Comparison...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping performance comparison test - Unified RAG not available")
            return True
        
        available_engines = [name for name, info in self.unified_rag.get_available_engines().items() if info.available]
        
        if len(available_engines) < 2:
            print(f"‚ö†Ô∏è Only {len(available_engines)} engine(s) available - limited performance comparison")
            return True
        
        # Add test data
        sample_doc = self.fixtures.get_sample_document()
        
        performance_results = {}
        test_query = "What are the key risk factors for technology companies?"
        
        for engine_name in available_engines[:2]:  # Test first 2 engines
            if self.unified_rag.set_engine(engine_name) and self.unified_rag.is_ready():
                # Add document to engine
                doc_result = self.unified_rag.add_document(sample_doc["text"], sample_doc["type"])
                
                if doc_result["status"] == "success":
                    # Measure query performance
                    times = []
                    successful_queries = 0
                    
                    for i in range(3):  # 3 iterations for average
                        start_time = time.time()
                        result = self.unified_rag.query(test_query, mode="hybrid")
                        query_time = time.time() - start_time
                        
                        if result["status"] == "success":
                            times.append(query_time)
                            successful_queries += 1
                    
                    if times:
                        performance_results[engine_name] = {
                            "avg_time": sum(times) / len(times),
                            "min_time": min(times),
                            "max_time": max(times),
                            "success_rate": successful_queries / 3 * 100,
                            "total_queries": 3
                        }
        
        # Analyze performance results
        if len(performance_results) >= 2:
            engine_names = list(performance_results.keys())
            engine1, engine2 = engine_names[0], engine_names[1]
            
            perf1 = performance_results[engine1]
            perf2 = performance_results[engine2]
            
            # Performance comparison
            faster_engine = engine1 if perf1["avg_time"] < perf2["avg_time"] else engine2
            speed_difference = abs(perf1["avg_time"] - perf2["avg_time"])
            
            print(f"üìä Performance Comparison Results:")
            print(f"   {engine1}: {perf1['avg_time']:.3f}s avg, {perf1['success_rate']:.1f}% success")
            print(f"   {engine2}: {perf2['avg_time']:.3f}s avg, {perf2['success_rate']:.1f}% success") 
            print(f"   Faster engine: {faster_engine} (by {speed_difference:.3f}s)")
            
            # Validate reasonable performance
            for engine_name, perf in performance_results.items():
                assert perf["max_time"] < self.config["performance_benchmarks"]["max_query_time"], f"{engine_name} queries too slow"
                assert perf["success_rate"] >= 50, f"{engine_name} success rate too low"
        
        print(f"‚úÖ Performance comparison tests passed ({len(performance_results)} engines compared)")
        return True
    
    def test_error_handling_and_fallbacks(self):
        """Test error handling and fallback behavior"""
        print("\nüß™ Testing Error Handling and Fallbacks...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping error handling test - Unified RAG not available")
            return True
        
        original_engine = self.unified_rag.get_current_engine()
        
        # Test switching to invalid engine
        invalid_switch = self.unified_rag.set_engine("completely_invalid_engine")
        assert not invalid_switch, "Should fail to switch to invalid engine"
        assert self.unified_rag.get_current_engine() == original_engine, "Should maintain original engine after failed switch"
        
        # Test querying with no ready engine (mock scenario)
        empty_query_result = self.unified_rag.query("test query", mode="hybrid")
        assert isinstance(empty_query_result, dict), "Should return structured result even when not ready"
        assert "status" in empty_query_result, "Should have status field"
        
        # Test document processing with invalid data
        try:
            invalid_doc_result = self.unified_rag.add_document("", "invalid_type")
            assert isinstance(invalid_doc_result, dict), "Should handle invalid document gracefully"
        except Exception as e:
            print(f"‚ö†Ô∏è Document error handling exception: {e}")
        
        # Test edge addition error handling
        try:
            invalid_edge_result = self.unified_rag.add_edges_from_data([{"invalid": "edge"}])
            assert isinstance(invalid_edge_result, dict), "Should handle invalid edges gracefully"
        except Exception as e:
            print(f"‚ö†Ô∏è Edge error handling exception: {e}")
        
        # Test stats retrieval with potential errors
        try:
            stats = self.unified_rag.get_stats()
            assert isinstance(stats, dict), "Stats should always return dict"
        except Exception as e:
            print(f"‚ö†Ô∏è Stats error handling exception: {e}")
        
        print("‚úÖ Error handling and fallback tests passed")
        return True
    
    def test_integration_scenarios(self):
        """Test realistic integration scenarios"""
        print("\nüß™ Testing Integration Scenarios...")
        
        if not UNIFIED_RAG_AVAILABLE or not self.unified_rag:
            print("‚ö†Ô∏è Skipping integration test - Unified RAG not available")
            return True
        
        # Scenario 1: Multi-engine workflow
        print("üìã Scenario 1: Multi-engine workflow...")
        
        available_engines = [name for name, info in self.unified_rag.get_available_engines().items() if info.available]
        
        if len(available_engines) >= 2:
            # Process documents with different engines
            documents_per_engine = {}
            
            for i, engine_name in enumerate(available_engines[:2]):
                if self.unified_rag.set_engine(engine_name) and self.unified_rag.is_ready():
                    doc = self.fixtures.test_documents[i % len(self.fixtures.test_documents)]
                    result = self.unified_rag.add_document(doc["text"], doc["type"])
                    
                    documents_per_engine[engine_name] = 1 if result["status"] == "success" else 0
            
            print(f"üìä Documents processed per engine: {documents_per_engine}")
        
        # Scenario 2: Query routing and comparison
        print("üìã Scenario 2: Query routing and comparison...")
        
        test_queries = [
            "What are semiconductor industry risks?",
            "How do supply chains affect tech companies?",
            "What are the geopolitical impacts on technology?"
        ]
        
        query_results = {}
        total_successful = 0
        
        for query in test_queries:
            # Try query with current engine
            result = self.unified_rag.query(query, mode="hybrid")
            query_successful = result["status"] == "success"
            
            query_results[query] = {
                "engine": result.get("engine", "unknown"),
                "success": query_successful,
                "response_length": len(result.get("answer", ""))
            }
            
            if query_successful:
                total_successful += 1
        
        assert total_successful > 0, "Should have at least one successful query in integration test"
        
        # Scenario 3: Performance under mixed load
        print("üìã Scenario 3: Mixed load testing...")
        
        start_time = time.time()
        mixed_operations = []
        
        # Mix of document additions and queries
        for i in range(5):
            if i % 2 == 0:
                # Add document
                doc = self.fixtures.get_sample_document()
                result = self.unified_rag.add_document(f"{doc['text']} (iteration {i})", doc["type"])
                mixed_operations.append(("document", result["status"] == "success"))
            else:
                # Run query
                result = self.unified_rag.query(f"What about iteration {i}?", mode="hybrid")
                mixed_operations.append(("query", result["status"] == "success"))
        
        total_time = time.time() - start_time
        successful_operations = sum(1 for _, success in mixed_operations if success)
        
        assert total_time < 30.0, f"Mixed load test took too long: {total_time:.2f}s"
        assert successful_operations >= 2, f"Too few successful operations: {successful_operations}/{len(mixed_operations)}"
        
        # Final validation
        final_stats = self.unified_rag.get_stats()
        assert isinstance(final_stats, dict), "Final stats should be dictionary"
        
        print(f"‚úÖ Integration scenarios passed ({successful_operations}/{len(mixed_operations)} operations successful)")
        return True
    
    def run_all_tests(self) -> Dict[str, bool]:
        """Run complete Unified RAG test suite"""
        print("üöÄ Starting ICE Unified RAG Test Suite...")
        print("=" * 60)
        
        # Define all tests
        tests = [
            ("System Availability", self.test_system_availability),
            ("Initialization and Readiness", self.test_initialization_and_readiness),
            ("Engine Switching", self.test_engine_switching),
            ("Unified Document Processing", self.test_unified_document_processing),
            ("Unified Querying", self.test_unified_querying),
            ("Engine Comparison", self.test_engine_comparison),
            ("Edge Management Unified", self.test_edge_management_unified),
            ("Performance Comparison", self.test_performance_comparison),
            ("Error Handling and Fallbacks", self.test_error_handling_and_fallbacks),
            ("Integration Scenarios", self.test_integration_scenarios)
        ]
        
        results = {}
        passed_count = 0
        
        # Run all tests
        for test_name, test_func in tests:
            try:
                self.setup_method()
                result = test_func()
                results[test_name] = result
                if result:
                    passed_count += 1
                self.teardown_method()
            except Exception as e:
                print(f"‚ùå {test_name} failed with exception: {e}")
                results[test_name] = False
                self.teardown_method()
        
        # Print summary
        print("\n" + "=" * 60)
        print("üìä ICE Unified RAG Test Results:")
        print(f"Passed: {passed_count}/{len(tests)}")
        print(f"Success Rate: {passed_count/len(tests)*100:.1f}%")
        
        for test_name, result in results.items():
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"  {status} - {test_name}")
        
        # Overall assessment
        if passed_count == len(tests):
            print("\nüéâ All Unified RAG tests passed! System provides excellent engine integration.")
        elif passed_count >= len(tests) * 0.8:  # 80% pass rate
            print(f"\n‚úÖ Most tests passed ({passed_count}/{len(tests)}). Unified system is highly functional.")
        elif passed_count >= len(tests) * 0.6:  # 60% pass rate
            print(f"\n‚ö†Ô∏è Many tests passed ({passed_count}/{len(tests)}). System is functional with some limitations.")
        else:
            print(f"\n‚ùå Many tests failed ({len(tests) - passed_count}/{len(tests)}). Check engine availability and configuration.")
        
        # Cleanup
        self.env_manager.cleanup_all_environments()
        
        return results


def run_unified_rag_tests():
    """Main entry point for running Unified RAG tests"""
    test_suite = TestICEUnifiedRAG()
    return test_suite.run_all_tests()


# Pytest integration (if pytest is available)
@pytest.fixture
def unified_rag_test_suite():
    """Pytest fixture for Unified RAG test suite"""
    return TestICEUnifiedRAG()


class TestUnifiedRAGPytest:
    """Pytest-compatible test class for Unified RAG"""
    
    def test_unified_rag_availability(self, unified_rag_test_suite):
        """Test Unified RAG system availability using pytest"""
        assert unified_rag_test_suite.test_system_availability()
    
    def test_unified_rag_initialization(self, unified_rag_test_suite):
        """Test Unified RAG initialization using pytest"""
        unified_rag_test_suite.setup_method()
        assert unified_rag_test_suite.test_initialization_and_readiness()
        unified_rag_test_suite.teardown_method()
    
    def test_unified_rag_engine_switching(self, unified_rag_test_suite):
        """Test Unified RAG engine switching using pytest"""
        unified_rag_test_suite.setup_method()
        assert unified_rag_test_suite.test_engine_switching()
        unified_rag_test_suite.teardown_method()
    
    def test_unified_rag_querying(self, unified_rag_test_suite):
        """Test Unified RAG querying using pytest"""
        unified_rag_test_suite.setup_method()
        assert unified_rag_test_suite.test_unified_querying()
        unified_rag_test_suite.teardown_method()


if __name__ == "__main__":
    # Run tests if script is executed directly
    run_unified_rag_tests()
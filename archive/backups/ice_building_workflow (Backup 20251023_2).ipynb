{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICE Building Workflow - Knowledge Graph Construction\n",
    "\n",
    "**Purpose**: Comprehensive data ingestion and knowledge graph building for investment intelligence\n",
    "**Architecture**: ICE Simplified (2,508 lines) with LightRAG integration\n",
    "**Input**: Financial data from multiple sources ‚Üí **Output**: Searchable knowledge graph\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Environment Setup** - Initialize ICE system and configure data sources\n",
    "2. **Workflow Mode Selection** - Choose between initial build or incremental update\n",
    "3. **Data Ingestion** - Fetch financial data from APIs and process documents\n",
    "4. **Knowledge Graph Building** - Extract entities, relationships, and build LightRAG graph\n",
    "5. **Storage & Validation** - Verify graph construction and monitor storage\n",
    "6. **Metrics & Monitoring** - Track processing metrics and system health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ICE Building Workflow\n",
      "üìÖ 2025-10-23 22:45\n",
      "üìÅ Working Directory: /Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configure environment\n",
    "os.environ.setdefault('ICE_WORKING_DIR', './src/ice_lightrag/storage')\n",
    "\n",
    "print(f\"üöÄ ICE Building Workflow\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"üìÅ Working Directory: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ICE System Initialized\n",
      "üß† LightRAG Status: Ready\n",
      "üìä Architecture: ICE Simplified (2,508 lines)\n",
      "üîó Components: Core + Ingester + QueryEngine\n"
     ]
    }
   ],
   "source": [
    "# Initialize ICE system\n",
    "from updated_architectures.implementation.ice_simplified import create_ice_system\n",
    "\n",
    "try:\n",
    "    ice = create_ice_system()\n",
    "    system_ready = ice.is_ready()\n",
    "    print(f\"‚úÖ ICE System Initialized\")\n",
    "    print(f\"üß† LightRAG Status: {'Ready' if system_ready else 'Initializing'}\")\n",
    "    print(f\"üìä Architecture: ICE Simplified (2,508 lines)\")\n",
    "    print(f\"üîó Components: Core + Ingester + QueryEngine\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization Error: {e}\")\n",
    "    raise  # Let errors surface for proper debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ LightRAG Storage Architecture Verification\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "LightRAG Storage Components:\n",
      "  chunks_vdb: ‚ö†Ô∏è Not created yet\n",
      "    Purpose: Vector database for document chunks\n",
      "    File: vdb_chunks.json\n",
      "  entities_vdb: ‚ö†Ô∏è Not created yet\n",
      "    Purpose: Vector database for extracted entities\n",
      "    File: vdb_entities.json\n",
      "  relationships_vdb: ‚ö†Ô∏è Not created yet\n",
      "    Purpose: Vector database for entity relationships\n",
      "    File: vdb_relationships.json\n",
      "  graph: ‚ö†Ô∏è Not created yet\n",
      "    Purpose: NetworkX graph structure\n",
      "    File: graph_chunk_entity_relation.graphml\n",
      "\n",
      "üìÅ Working Directory: ice_lightrag/storage\n",
      "üóÑÔ∏è Storage Backend: File-based (development mode)\n",
      "üíæ Total Storage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify storage architecture and components\n",
    "print(f\"üì¶ LightRAG Storage Architecture Verification\")\n",
    "print(f\"‚îÅ\" * 40)\n",
    "\n",
    "if not (ice and ice.core.is_ready()):\n",
    "    raise RuntimeError(\"ICE system not ready - cannot verify storage\")\n",
    "\n",
    "# Get storage statistics using new method\n",
    "storage_stats = ice.core.get_storage_stats()\n",
    "\n",
    "print(f\"LightRAG Storage Components:\")\n",
    "for component_name, component_info in storage_stats['components'].items():\n",
    "    status = \"‚úÖ Initialized\" if component_info['exists'] else \"‚ö†Ô∏è Not created yet\"\n",
    "    size_mb = component_info['size_bytes'] / (1024 * 1024) if component_info['size_bytes'] > 0 else 0\n",
    "    print(f\"  {component_name}: {status}\")\n",
    "    print(f\"    Purpose: {component_info['description']}\")\n",
    "    print(f\"    File: {component_info['file']}\")\n",
    "    if size_mb > 0:\n",
    "        print(f\"    Size: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìÅ Working Directory: {storage_stats['working_dir']}\")\n",
    "print(f\"üóÑÔ∏è Storage Backend: File-based (development mode)\")\n",
    "print(f\"üíæ Total Storage: {storage_stats['total_storage_bytes'] / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì° Data Sources Available: 7\n",
      "  ‚úÖ newsapi\n",
      "  ‚úÖ alpha_vantage\n",
      "  ‚úÖ fmp\n",
      "  ‚úÖ polygon\n",
      "  ‚úÖ finnhub\n",
      "  ‚úÖ benzinga\n",
      "  ‚úÖ marketaux\n",
      "\n",
      "üîë OpenAI API: ‚úÖ Configured\n"
     ]
    }
   ],
   "source": [
    "# Data sources configuration status\n",
    "if not (ice and hasattr(ice, 'ingester')):\n",
    "    raise RuntimeError(\"Data ingester not initialized\")\n",
    "\n",
    "available_services = ice.ingester.available_services\n",
    "print(f\"\\nüì° Data Sources Available: {len(available_services)}\")\n",
    "for service in available_services:\n",
    "    print(f\"  ‚úÖ {service}\")\n",
    "\n",
    "if not available_services:\n",
    "    print(f\"  ‚ö†Ô∏è No APIs configured - will use sample data\")\n",
    "    print(f\"  üí° Set NEWSAPI_ORG_API_KEY for real news\")\n",
    "    print(f\"  üí° Set ALPHA_VANTAGE_API_KEY for financial data\")\n",
    "\n",
    "# Validate OpenAI for LightRAG\n",
    "openai_configured = bool(os.getenv('OPENAI_API_KEY'))\n",
    "print(f\"\\nüîë OpenAI API: {'‚úÖ Configured' if openai_configured else '‚ùå Required for full functionality'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Workflow Mode Selection & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Model Provider Configuration\n",
    "\n",
    "ICE supports **OpenAI** (paid) or **Ollama** (free local) for LLM and embeddings:\n",
    "\n",
    "#### Option 1: OpenAI (Default - No setup required)\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n",
    "- **Cost**: ~$5/month for typical usage\n",
    "- **Quality**: Highest accuracy for entity extraction and reasoning\n",
    "- **Setup**: Just set API key\n",
    "\n",
    "#### Option 2: Ollama (Free Local - Requires setup)\n",
    "```bash\n",
    "# Set provider\n",
    "export LLM_PROVIDER=\"ollama\"\n",
    "\n",
    "# One-time setup:\n",
    "ollama serve                      # Start Ollama service\n",
    "ollama pull qwen3:30b-32k        # Pull LLM model (32k context required)\n",
    "ollama pull nomic-embed-text      # Pull embedding model\n",
    "```\n",
    "- **Cost**: $0/month (completely free)\n",
    "- **Quality**: Good for most investment analysis tasks\n",
    "- **Setup**: Requires local Ollama installation and model download\n",
    "\n",
    "#### Option 3: Hybrid (Recommended for cost-conscious users)\n",
    "```bash\n",
    "export LLM_PROVIDER=\"ollama\"           # Use Ollama for LLM\n",
    "export EMBEDDING_PROVIDER=\"openai\"     # Use OpenAI for embeddings\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n",
    "- **Cost**: ~$2/month (embeddings only)\n",
    "- **Quality**: Balanced - free LLM with high-quality embeddings\n",
    "\n",
    "**Current configuration will be logged when you run the next cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Docling Document Processing (Switchable Architecture)\n",
    "\n",
    "ICE supports **switchable document processing** for SEC filings and email attachments:\n",
    "\n",
    "#### Docling Integration (Default - Professional-grade table extraction)\n",
    "```bash\n",
    "export USE_DOCLING_SEC=true          # SEC filings: 0% ‚Üí 97.9% table extraction\n",
    "export USE_DOCLING_EMAIL=true        # Email attachments: 42% ‚Üí 97.9% accuracy\n",
    "```\n",
    "- **Accuracy**: 97.9% table extraction (vs 42% with PyPDF2)\n",
    "- **Cost**: $0/month (local execution)\n",
    "- **Setup**: Models auto-download on first use (~500MB, one-time)\n",
    "\n",
    "#### Original Implementations (For comparison testing)\n",
    "```bash\n",
    "export USE_DOCLING_SEC=false         # Use metadata-only SEC extraction\n",
    "export USE_DOCLING_EMAIL=false       # Use PyPDF2/openpyxl processors\n",
    "```\n",
    "- **Purpose**: A/B testing and backward compatibility\n",
    "- **Use when**: Comparing extraction accuracy or troubleshooting\n",
    "\n",
    "**Note**: Docling uses IBM's AI models (DocLayNet, TableFormer, Granite-Docling VLM) for professional-grade document parsing. Both implementations coexist - toggle switches instantly without code changes.\n",
    "\n",
    "**More info**: See `md_files/DOCLING_INTEGRATION_TESTING.md` for detailed testing procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Crawl4AI URL Fetching (Switchable Architecture)\n",
    "\n",
    "ICE supports **hybrid URL fetching** for email links with intelligent routing:\n",
    "\n",
    "#### Smart Routing Strategy (Default - Simple HTTP only)\n",
    "```bash\n",
    "export USE_CRAWL4AI_LINKS=false      # Simple HTTP only (fast, free, default)\n",
    "```\n",
    "- **Approach**: Use simple HTTP (aiohttp) for all URLs\n",
    "- **Works for**: DBS research URLs with embedded tokens, direct PDFs, SEC EDGAR\n",
    "- **Cost**: $0/month (no browser automation)\n",
    "- **Speed**: <2 seconds per URL average\n",
    "\n",
    "#### Crawl4AI Hybrid Routing (Enable for complex sites)\n",
    "```bash\n",
    "export USE_CRAWL4AI_LINKS=true       # Enable browser automation for complex URLs\n",
    "export CRAWL4AI_TIMEOUT=60           # Timeout in seconds (default: 60)\n",
    "export CRAWL4AI_HEADLESS=true        # Headless mode (default: true)\n",
    "```\n",
    "- **Routing**: Automatically classifies URLs and uses appropriate method\n",
    "  - Simple HTTP: DBS URLs, direct file downloads, SEC EDGAR, static content\n",
    "  - Crawl4AI: Premium research portals (Goldman, Morgan Stanley), JS-heavy IR sites\n",
    "- **Fallback**: Graceful degradation to simple HTTP if Crawl4AI fails\n",
    "- **Cost**: CPU cost for browser automation, free tier usage\n",
    "- **Use when**: Accessing JavaScript-heavy sites or login-required portals\n",
    "\n",
    "**Note**: DBS research portal URLs work with simple HTTP (embedded auth tokens in `?E=...` parameter) - no browser automation needed.\n",
    "\n",
    "**More info**: See `md_files/CRAWL4AI_INTEGRATION_PLAN.md` for detailed integration strategy and `CLAUDE.md` Pattern #6 for code examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Switched to OpenAI\n"
     ]
    }
   ],
   "source": [
    "# ### Provider Switching - Uncomment ONE option below, then restart kernel\n",
    "\n",
    "#############################################################################\n",
    "#                              Model Selection                              #\n",
    "#############################################################################\n",
    "\n",
    "### Option 1: OpenAI ($5/mo, highest quality)\n",
    "import os; os.environ['LLM_PROVIDER'] = 'openai'\n",
    "print(\"‚úÖ Switched to OpenAI\")\n",
    "\n",
    "# ###Option 2: Hybrid ($2/mo, 60% savings, recommended)\n",
    "# import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'openai'\n",
    "# print(\"‚úÖ Switched to Hybrid\")\n",
    "\n",
    "# ### Option 3: Full Ollama ($0/mo, faster model)\n",
    "# import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'ollama'; \n",
    "# # os.environ['LLM_MODEL'] = 'llama3.1:8b'\n",
    "# os.environ['LLM_MODEL'] = 'qwen3:8b'\n",
    "# # os.environ['LLM_MODEL'] = 'qwen3:14b'\n",
    "# # os.environ['LLM_MODEL'] = 'qwen3:30b-32k'\n",
    "# print(\"‚úÖ Switched to Full Ollama Model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóëÔ∏è Graph Management (Optional)\n",
    "\n",
    "**When to clear the graph:**\n",
    "- ‚úÖ Switching to Full Ollama (1536-dim ‚Üí 768-dim embeddings)\n",
    "- ‚úÖ Graph corrupted or very old (>30 days without updates)\n",
    "- ‚úÖ Testing fresh graph builds from scratch\n",
    "\n",
    "**When NOT to clear:**\n",
    "- ‚ùå Just switching LLM provider (OpenAI ‚Üî Hybrid use same embeddings)\n",
    "- ‚ùå Adding new documents (incremental updates work fine)\n",
    "- ‚ùå Changing query modes (local, hybrid, etc.)\n",
    "\n",
    "**How to clear:**\n",
    "Run the code cell below (uncomment lines to activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚ö†Ô∏è  vdb_entities.json: not found\n",
      "  ‚ö†Ô∏è  vdb_relationships.json: not found\n",
      "  ‚ö†Ô∏è  vdb_chunks.json: not found\n",
      "  ‚ö†Ô∏è  graph_chunk_entity_relation.graphml: not found\n",
      "  üíæ Total: 0.00 MB\n",
      "\n",
      "üß¨ Graph Health Metrics:\n",
      "  üìä Content Coverage:\n",
      "    Tickers: None (0/4 portfolio holdings)\n",
      "\n",
      "  üï∏Ô∏è Graph Structure:\n",
      "    Total entities: 0\n",
      "    Total relationships: 0\n",
      "\n",
      "  üíº Investment Signals:\n",
      "    BUY signals: 0\n",
      "    SELL signals: 0\n",
      "    Price targets: 0\n",
      "\n",
      "  üì¶ Graph Storage:\n",
      "    Total chunks: 0\n",
      "    Email chunks: 0\n",
      "    API/SEC chunks: 0\n",
      "\n",
      "\n",
      "‚úÖ Log saved: logs/graph_processing_log_20251023_224534.md\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                    Check graph info                    #\n",
    "##########################################################\n",
    "from updated_architectures.implementation.ice_simplified import create_ice_system\n",
    "ice = create_ice_system()\n",
    "\n",
    "# Toggle to enable/disable logging output to file\n",
    "SAVE_PROCESSING_LOG = True  # Set to False to disable logging\n",
    "\n",
    "if SAVE_PROCESSING_LOG:\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    \n",
    "    output_buffer = StringIO()\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = output_buffer\n",
    "    error_buffer = StringIO()\n",
    "    original_stderr = sys.stderr\n",
    "    sys.stderr = error_buffer\n",
    "\n",
    "    # Reconfigure logging handlers to use redirected stderr\n",
    "    import logging\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        handler.stream = sys.stderr  # Now points to error_buffer\n",
    "\n",
    "\n",
    "##########################################################\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def check_storage(storage_path):\n",
    "    \"\"\"Check and display storage file inventory\"\"\"\n",
    "    files = ['vdb_entities.json', 'vdb_relationships.json', 'vdb_chunks.json', 'graph_chunk_entity_relation.graphml']\n",
    "    total_size = 0\n",
    "    for fname in files:\n",
    "        fpath = storage_path / fname\n",
    "        if fpath.exists():\n",
    "            size_mb = fpath.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úÖ {fname}: {size_mb:.2f} MB\")\n",
    "            total_size += size_mb\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  {fname}: not found\")\n",
    "    print(f\"  üíæ Total: {total_size:.2f} MB\")\n",
    "\n",
    "# Use actual config path instead of hardcoded path to avoid path mismatches\n",
    "storage_path = Path(ice.config.working_dir)\n",
    "\n",
    "check_storage(storage_path)\n",
    "\n",
    "#####################################################################\n",
    "ice.core.get_graph_stats()\n",
    "\n",
    "##########################################################\n",
    "#              Graph Health Metrics (P0)                 #\n",
    "##########################################################\n",
    "\n",
    "def check_graph_health(storage_path):\n",
    "    \"\"\"Check critical graph health metrics (P0 only)\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    TICKERS = {'NVDA', 'TSMC', 'AMD', 'ASML'}  # Known portfolio tickers\n",
    "    \n",
    "    result = {\n",
    "        'tickers_covered': set(),\n",
    "        'total_entities': 0,\n",
    "        'total_relationships': 0,\n",
    "        'buy_signals': 0,\n",
    "        'sell_signals': 0,\n",
    "        'price_targets': 0\n",
    "    }\n",
    "    \n",
    "    # Parse entities\n",
    "    entities_file = Path(storage_path) / 'vdb_entities.json'\n",
    "    if entities_file.exists():\n",
    "        data = json.loads(entities_file.read_text())\n",
    "        result['total_entities'] = len(data.get('data', []))\n",
    "        \n",
    "        for entity in data.get('data', []):\n",
    "            text = f\"{entity.get('entity_name', '')} {entity.get('content', '')}\".upper()\n",
    "            \n",
    "            # Detect tickers\n",
    "            for ticker in TICKERS:\n",
    "                if ticker in text:\n",
    "                    result['tickers_covered'].add(ticker)\n",
    "            \n",
    "            # Detect signals\n",
    "            if 'BUY' in text:\n",
    "                result['buy_signals'] += 1\n",
    "            if 'SELL' in text:\n",
    "                result['sell_signals'] += 1\n",
    "            if 'PRICE TARGET' in text or 'PRICE_TARGET' in text:\n",
    "                result['price_targets'] += 1\n",
    "    \n",
    "    # Parse relationships\n",
    "    rels_file = Path(storage_path) / 'vdb_relationships.json'\n",
    "    if rels_file.exists():\n",
    "        data = json.loads(rels_file.read_text())\n",
    "        result['total_relationships'] = len(data.get('data', []))\n",
    "    \n",
    "    result['tickers_covered'] = sorted(list(result['tickers_covered']))\n",
    "    return result\n",
    "\n",
    "def get_extended_graph_stats(storage_path):\n",
    "    \"\"\"Get additional graph statistics for comprehensive analysis\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    stats = {\n",
    "        'chunk_count': 0,\n",
    "        'email_chunks': 0,\n",
    "        'api_sec_chunks': 0\n",
    "    }\n",
    "    \n",
    "    # Parse chunks\n",
    "    chunks_file = Path(storage_path) / 'vdb_chunks.json'\n",
    "    if chunks_file.exists():\n",
    "        data = json.loads(chunks_file.read_text())\n",
    "        chunks = data.get('data', [])\n",
    "        stats['chunk_count'] = len(chunks)\n",
    "        \n",
    "        # Infer source from content markers\n",
    "        for chunk in chunks:\n",
    "            content = chunk.get('content', '')\n",
    "            # Email documents contain investment signal markup\n",
    "            if any(marker in content for marker in ['[TICKER:', '[RATING:', '[PRICE_TARGET:']):\n",
    "                stats['email_chunks'] += 1\n",
    "            else:\n",
    "                stats['api_sec_chunks'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run health check\n",
    "health = check_graph_health(storage_path)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüß¨ Graph Health Metrics:\")\n",
    "print(f\"  üìä Content Coverage:\")\n",
    "print(f\"    Tickers: {', '.join(health['tickers_covered']) if health['tickers_covered'] else 'None'} ({len(health['tickers_covered'])}/4 portfolio holdings)\")\n",
    "\n",
    "print(f\"\\n  üï∏Ô∏è Graph Structure:\")\n",
    "print(f\"    Total entities: {health['total_entities']:,}\")\n",
    "print(f\"    Total relationships: {health['total_relationships']:,}\")\n",
    "if health['total_entities'] > 0:\n",
    "    avg_conn = health['total_relationships'] / health['total_entities']\n",
    "    print(f\"    Avg connections: {avg_conn:.2f}\")\n",
    "\n",
    "print(f\"\\n  üíº Investment Signals:\")\n",
    "print(f\"    BUY signals: {health['buy_signals']}\")\n",
    "print(f\"    SELL signals: {health['sell_signals']}\")\n",
    "print(f\"    Price targets: {health['price_targets']}\")\n",
    "\n",
    "# Run extended stats\n",
    "extended_stats = get_extended_graph_stats(storage_path)\n",
    "\n",
    "print(f\"\\n  üì¶ Graph Storage:\")\n",
    "print(f\"    Total chunks: {extended_stats['chunk_count']:,}\")\n",
    "print(f\"    Email chunks: {extended_stats['email_chunks']:,}\")\n",
    "print(f\"    API/SEC chunks: {extended_stats['api_sec_chunks']:,}\")\n",
    "\n",
    "# Save output to log file if enabled\n",
    "if SAVE_PROCESSING_LOG:\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "    captured_out = output_buffer.getvalue()\n",
    "    captured_err = error_buffer.getvalue()\n",
    "    combined = captured_err + captured_out\n",
    "    print(combined)  # Display in notebook\n",
    "    \n",
    "    # Save to markdown file with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = Path(f'logs/graph_processing_log_{timestamp}.md')\n",
    "    log_file.parent.mkdir(exist_ok=True)\n",
    "    log_file.write_text(\n",
    "        f'# Graph Processing Log\\n'\n",
    "        f'**Timestamp:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\n'\n",
    "        f'```\\n{combined}\\n```\\n'\n",
    "    )\n",
    "    print(f'\\n‚úÖ Log saved: {log_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test: Hybrid Entity Categorization with Qwen2.5-3B\n",
    "\n",
    "**Purpose**: Compare categorization accuracy between keyword-only and hybrid (keyword+LLM) approaches\n",
    "\n",
    "**What this tests**:\n",
    "- Baseline: Fast keyword pattern matching (~1ms per entity)\n",
    "- Enhanced: Confidence scoring to identify ambiguous cases\n",
    "- Hybrid: LLM fallback for low-confidence entities (~40ms per entity)\n",
    "\n",
    "**Prerequisites**:\n",
    "- ‚úÖ Ollama installed with qwen2.5:3b model (optional - degrades gracefully)\n",
    "- ‚úÖ LightRAG graph built (previous cells completed)\n",
    "\n",
    "**Expected runtime**: ~0.5 seconds for 12 sample entities (hybrid mode)\n",
    "\n",
    "**Configuration**: `src/ice_lightrag/graph_categorization.py` - Change `CATEGORIZATION_MODE` to enable hybrid by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ Entity Categorization Test (4 Modes) - FIXED\n",
      "======================================================================\n",
      "‚úÖ Categorization functions imported successfully\n",
      "üîß Ollama model configured: qwen2.5:3b\n",
      "\n",
      "‚úÖ Ollama service running with qwen2.5:3b model\n",
      "‚ùå Storage file not found: ice_lightrag/storage/vdb_entities.json\n",
      "   ‚Üí Run previous cells to build the knowledge graph first\n",
      "\n",
      "   ‚ö†Ô∏è  Categorization tests will be skipped\n",
      "\n",
      "‚ùå No entities found in storage\n",
      "   ‚ö†Ô∏è  No data - tests will be skipped\n",
      "\n",
      "‚úÖ Loaded 0 entities from knowledge graph\n",
      "   Sampling mode: Reproducible (seed=42, same entities each run)\n",
      "   Testing with 0 entities\n",
      "   üí° To change: Set RANDOM_SEED = None (line 17) or OLLAMA_MODEL_OVERRIDE (line 18)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEST 1: Keyword-Only Categorization (Baseline)\n",
      "======================================================================\n",
      "\n",
      "Results (Keyword Matching):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\\n‚è±Ô∏è  Time: 0.1ms (no entitys to test)\n",
      "\n",
      "======================================================================\n",
      "TEST 2: Keyword + Confidence Scoring\n",
      "======================================================================\n",
      "\n",
      "Results (with confidence scores):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\n",
      "‚úÖ All entities have high confidence (‚â•0.70) - no LLM fallback needed\n",
      "\\n‚è±Ô∏è  Time: 0.1ms (no entitys to test)\n",
      "\n",
      "======================================================================\n",
      "TEST 3: Hybrid Categorization (Keyword + LLM Fallback)\n",
      "======================================================================\n",
      "‚è±Ô∏è  Note: LLM calls may take 5-10 seconds total...\n",
      "\n",
      "\n",
      "Results (Hybrid mode - keyword + LLM):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\\nü§ñ LLM calls: 0/0 (no entitys to test)\n",
      "\\n‚è±Ô∏è  Time: 0.0ms (no entitys to test)\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY (Keyword vs Hybrid)\n",
      "======================================================================\n",
      "No entitys - skipping recategorization analysis\n",
      "\n",
      "‚úÖ Hybrid categorization complete!\n",
      "\n",
      "======================================================================\n",
      "TEST 4: Pure LLM Categorization (All entities)\n",
      "======================================================================\n",
      "‚è±Ô∏è  Note: This will call LLM for ALL entities (may take 30-60 seconds)...\n",
      "\n",
      "\n",
      "Results (Pure LLM mode):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\n",
      "ü§ñ LLM calls: 0/0 (100%)\n",
      "\\n‚è±Ô∏è  Time: 0.0ms (no entitys to test)\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY (Keyword vs Pure LLM)\n",
      "======================================================================\n",
      "No entitys - skipping recategorization analysis\n",
      "\n",
      "‚úÖ Pure LLM categorization complete!\n",
      "\n",
      "======================================================================\n",
      "üéØ TESTING COMPLETE - 4 Categorization Modes Evaluated\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Test hybrid entity categorization with configurable sampling and model selection\n",
    "# Location: ice_building_workflow.ipynb Cell 12 (FIXED VERSION)\n",
    "# Dependencies: graph_categorization.py, entity_categories.py, LightRAG storage\n",
    "\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Force reload of graph_categorization module to pick up new functions\n",
    "import importlib\n",
    "if 'src.ice_lightrag.graph_categorization' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.ice_lightrag.graph_categorization'])\n",
    "\n",
    "# ===== CONFIGURATION: User-editable settings =====\n",
    "RANDOM_SEED = 42  # Set to None for different entities each run, or keep 42 for reproducible testing\n",
    "OLLAMA_MODEL_OVERRIDE = 'qwen2.5:3b'  # Change to use different model (e.g., 'llama3.1:8b', 'qwen3:8b')\n",
    "\n",
    "# ===== SETUP: Imports with error handling =====\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ Entity Categorization Test (4 Modes) - FIXED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure src is in path for notebook context\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    from src.ice_lightrag.graph_categorization import (\n",
    "        categorize_entity,\n",
    "        categorize_entity_with_confidence,\n",
    "        categorize_entity_hybrid,\n",
    "        categorize_entity_llm_only  # NEW: Pure LLM mode\n",
    "    )\n",
    "    from src.ice_lightrag.entity_categories import CATEGORY_DISPLAY_ORDER\n",
    "    print(\"‚úÖ Categorization functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   ‚Üí Ensure previous cells completed successfully\")\n",
    "    print(\"   ‚Üí Check that src/ice_lightrag/graph_categorization.py exists\\n\")\n",
    "    raise\n",
    "\n",
    "# Patch module constant for Ollama model selection\n",
    "import src.ice_lightrag.graph_categorization as graph_cat_module\n",
    "graph_cat_module.OLLAMA_MODEL = OLLAMA_MODEL_OVERRIDE\n",
    "print(f\"üîß Ollama model configured: {OLLAMA_MODEL_OVERRIDE}\\n\")\n",
    "\n",
    "# ===== HEALTH CHECK: Ollama service availability =====\n",
    "def check_ollama_service():\n",
    "    \"\"\"Check if Ollama service is running and configured model is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            # Use exact match to avoid accepting wrong model versions\n",
    "            model_available = any(m.get('name', '') == OLLAMA_MODEL_OVERRIDE for m in models)\n",
    "            return True, model_available\n",
    "        return False, False\n",
    "    except requests.RequestException:\n",
    "        return False, False\n",
    "    except (KeyError, json.JSONDecodeError):\n",
    "        return True, False\n",
    "\n",
    "ollama_running, model_available = check_ollama_service()\n",
    "\n",
    "if ollama_running and model_available:\n",
    "    print(f\"‚úÖ Ollama service running with {OLLAMA_MODEL_OVERRIDE} model\")\n",
    "elif ollama_running:\n",
    "    print(f\"‚ö†Ô∏è  Ollama running but {OLLAMA_MODEL_OVERRIDE} not found\")\n",
    "    print(f\"   ‚Üí Install: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama not running - TEST 3 & 4 will be skipped\")\n",
    "    print(\"   ‚Üí Start Ollama: brew services start ollama (macOS)\\n\")\n",
    "\n",
    "# ===== DATA LOADING: Entities with validation =====\n",
    "storage_path = Path(ice.config.working_dir) / \"vdb_entities.json\"\n",
    "entities_data = {}  # Initialize to prevent NameError if file missing or error occurs\n",
    "\n",
    "if not storage_path.exists():\n",
    "    print(f\"‚ùå Storage file not found: {storage_path}\")\n",
    "    print(\"   ‚Üí Run previous cells to build the knowledge graph first\\n\")\n",
    "    print(\"   ‚ö†Ô∏è  Categorization tests will be skipped\\n\")\n",
    "else:\n",
    "    try:\n",
    "        with open(storage_path) as f:\n",
    "            entities_data = json.load(f)\n",
    "\n",
    "        if not isinstance(entities_data, dict) or 'data' not in entities_data:\n",
    "            print(f\"‚ùå Invalid storage format (expected dict with 'data' key)\")\n",
    "            print(\"   ‚ö†Ô∏è  Invalid storage - tests will be skipped\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Storage error: {e}\")\n",
    "        print(\"   ‚ö†Ô∏è  Tests will be skipped\\n\")\n",
    "\n",
    "entities_list = entities_data.get('data', [])\n",
    "\n",
    "if not isinstance(entities_list, list) or len(entities_list) == 0:\n",
    "    print(f\"‚ùå No entities found in storage\")\n",
    "    print(\"   ‚ö†Ô∏è  No data - tests will be skipped\\n\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(entities_list)} entities from knowledge graph\")\n",
    "\n",
    "# Configurable random sampling\n",
    "if RANDOM_SEED is not None:\n",
    "    random.seed(RANDOM_SEED)\n",
    "    sampling_mode = f\"Reproducible (seed={RANDOM_SEED}, same entities each run)\"\n",
    "else:\n",
    "    sampling_mode = \"Random (different entities each run)\"\n",
    "\n",
    "test_entities = random.sample(entities_list, min(12, len(entities_list)))\n",
    "print(f\"   Sampling mode: {sampling_mode}\")\n",
    "print(f\"   Testing with {len(test_entities)} entities\")\n",
    "print(f\"   üí° To change: Set RANDOM_SEED = None (line 17) or OLLAMA_MODEL_OVERRIDE (line 18)\\n\")\n",
    "\n",
    "# ===== HELPER: Compact result display =====\n",
    "def display_results(results, title, show_confidence=False, show_llm=False):\n",
    "    \"\"\"Display categorization results in compact format\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, (name, category, confidence, used_llm) in enumerate(results, 1):\n",
    "        display_name = name[:40] + \"...\" if len(name) > 40 else name\n",
    "\n",
    "        if show_llm and used_llm:\n",
    "            indicator = \"ü§ñ\"\n",
    "        else:\n",
    "            indicator = \"‚ö°\"\n",
    "\n",
    "        if show_confidence:\n",
    "            print(f\"{i:2d}. {indicator} {display_name:43s} ‚Üí {category:20s} (conf: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(f\"{i:2d}. {display_name:45s} ‚Üí {category}\")\n",
    "\n",
    "    category_counts = Counter(cat for _, cat, _, _ in results)\n",
    "    print(f\"\\nüìä Distribution: {dict(category_counts)}\")\n",
    "\n",
    "# ===== TEST 1: Keyword-Only Baseline =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 1: Keyword-Only Categorization (Baseline)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "keyword_results = []\n",
    "\n",
    "for entity in test_entities:\n",
    "    name = entity.get('entity_name', '')\n",
    "    content = entity.get('content', '')\n",
    "    category = categorize_entity(name, content)\n",
    "    keyword_results.append((name, category, 1.0, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(keyword_results, \"Results (Keyword Matching):\", show_confidence=False)\n",
    "if len(test_entities) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "# ===== TEST 2: Confidence Scoring Analysis =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: Keyword + Confidence Scoring\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "confidence_results = []\n",
    "\n",
    "for entity in test_entities:\n",
    "    name = entity.get('entity_name', '')\n",
    "    content = entity.get('content', '')\n",
    "    category, confidence = categorize_entity_with_confidence(name, content)\n",
    "    confidence_results.append((name, category, confidence, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(confidence_results, \"Results (with confidence scores):\", show_confidence=True)\n",
    "\n",
    "low_confidence = [(n, c, conf) for n, c, conf, _ in confidence_results if conf < 0.70]\n",
    "\n",
    "if low_confidence:\n",
    "    print(f\"\\nüîç Ambiguous entities (confidence < 0.70): {len(low_confidence)}\")\n",
    "    for name, cat, conf in low_confidence:\n",
    "        print(f\"   - {name[:50]:50s} ‚Üí {cat:20s} (conf: {conf:.2f})\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All entities have high confidence (‚â•0.70) - no LLM fallback needed\")\n",
    "\n",
    "if len(test_entities) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "# ===== TEST 3: Hybrid Mode (if Ollama available) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 3: Hybrid Categorization (Keyword + LLM Fallback)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: LLM calls may take 5-10 seconds total...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    hybrid_results = []\n",
    "    llm_call_count = 0\n",
    "\n",
    "    for entity in test_entities:\n",
    "        name = entity.get('entity_name', '')\n",
    "        content = entity.get('content', '')\n",
    "\n",
    "        keyword_cat, keyword_conf = categorize_entity_with_confidence(name, content)\n",
    "\n",
    "        if keyword_conf >= 0.70:\n",
    "            category, confidence = keyword_cat, keyword_conf\n",
    "            used_llm = False\n",
    "        else:\n",
    "            category, confidence = categorize_entity_hybrid(name, content, confidence_threshold=0.70)\n",
    "            used_llm = (confidence == 0.90)\n",
    "            if used_llm:\n",
    "                llm_call_count += 1\n",
    "\n",
    "        hybrid_results.append((name, category, confidence, used_llm))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(hybrid_results, \"Results (Hybrid mode - keyword + LLM):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"\\nü§ñ LLM calls: {llm_call_count}/{len(test_entities)} ({100*llm_call_count/len(test_entities):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\\\nü§ñ LLM calls: 0/0 (no entitys to test)\")\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "    # ===== COMPARISON SUMMARY =====\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Hybrid)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    changes = 0\n",
    "    for i in range(len(test_entities)):\n",
    "        if keyword_results[i][1] != hybrid_results[i][1]:\n",
    "            changes += 1\n",
    "\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"Entities recategorized by LLM: {changes}/{len(test_entities)} ({100*changes/len(test_entities):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No entitys - skipping recategorization analysis\")\n",
    "\n",
    "    if changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_entities)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            hyb_cat = hybrid_results[i][1]\n",
    "            if kw_cat != hyb_cat:\n",
    "                name = test_entities[i].get('entity_name', '')[:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {hyb_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Hybrid categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 3 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable hybrid mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "# ===== TEST 4: Pure LLM Mode (NEW) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 4: Pure LLM Categorization (All entities)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: This will call LLM for ALL entities (may take 30-60 seconds)...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm_results = []\n",
    "\n",
    "    for entity in test_entities:\n",
    "        name = entity.get('entity_name', '')\n",
    "        content = entity.get('content', '')\n",
    "        category, confidence = categorize_entity_llm_only(name, content)\n",
    "        llm_results.append((name, category, confidence, True))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(llm_results, \"Results (Pure LLM mode):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    print(f\"\\nü§ñ LLM calls: {len(test_entities)}/{len(test_entities)} (100%)\")\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "    # Compare keyword vs pure LLM (FIXED INDENTATION)\n",
    "    llm_changes = sum(1 for i in range(len(test_entities)) if keyword_results[i][1] != llm_results[i][1])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Pure LLM)\")\n",
    "    print(\"=\" * 70)\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"Entities recategorized by pure LLM: {llm_changes}/{len(test_entities)} ({100*llm_changes/len(test_entities):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No entitys - skipping recategorization analysis\")\n",
    "\n",
    "    if llm_changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_entities)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            llm_cat = llm_results[i][1]\n",
    "            if kw_cat != llm_cat:\n",
    "                name = test_entities[i].get('entity_name', '')[:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {llm_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Pure LLM categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 4 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable pure LLM mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ TESTING COMPLETE - 4 Categorization Modes Evaluated\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ Relationship Categorization Test (4 Modes) - FIXED\n",
      "======================================================================\n",
      "‚úÖ Categorization functions imported successfully\n",
      "üîß Ollama model configured: qwen2.5:3b\n",
      "\n",
      "‚úÖ Ollama service running with qwen2.5:3b model\n",
      "‚ùå Storage file not found: ice_lightrag/storage/vdb_relationships.json\n",
      "   ‚Üí Run previous cells to build the knowledge graph first\n",
      "\n",
      "   ‚ö†Ô∏è  Categorization tests will be skipped\n",
      "\n",
      "‚ùå No relationships found in storage\n",
      "   ‚ö†Ô∏è  No data - tests will be skipped\n",
      "\n",
      "‚úÖ Loaded 0 relationships from knowledge graph\n",
      "   Sampling mode: Reproducible (seed=42, same relationships each run)\n",
      "   Testing with 0 relationships\n",
      "   üí° To change: Set RANDOM_SEED = None (line 17) or OLLAMA_MODEL_OVERRIDE (line 18)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEST 1: Keyword-Only Categorization (Baseline)\n",
      "======================================================================\n",
      "\n",
      "Results (Keyword Matching):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\\n‚è±Ô∏è  Time: 0.1ms (no relationships to test)\n",
      "\n",
      "======================================================================\n",
      "TEST 2: Keyword + Confidence Scoring\n",
      "======================================================================\n",
      "\n",
      "Results (with confidence scores):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\n",
      "‚úÖ All relationships have high confidence (‚â•0.70) - no LLM fallback needed\n",
      "\\n‚è±Ô∏è  Time: 0.1ms (no relationships to test)\n",
      "\n",
      "======================================================================\n",
      "TEST 3: Hybrid Categorization (Keyword + LLM Fallback)\n",
      "======================================================================\n",
      "‚è±Ô∏è  Note: LLM calls may take 5-10 seconds total...\n",
      "\n",
      "\n",
      "Results (Hybrid mode - keyword + LLM):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\\nü§ñ LLM calls: 0/0 (no relationships to test)\n",
      "\\n‚è±Ô∏è  Time: 0.0ms (no relationships to test)\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY (Keyword vs Hybrid)\n",
      "======================================================================\n",
      "No relationships - skipping recategorization analysis\n",
      "\n",
      "‚úÖ Hybrid categorization complete!\n",
      "\n",
      "======================================================================\n",
      "TEST 4: Pure LLM Categorization (All relationships)\n",
      "======================================================================\n",
      "‚è±Ô∏è  Note: This will call LLM for ALL relationships (may take 30-60 seconds)...\n",
      "\n",
      "\n",
      "Results (Pure LLM mode):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Distribution: {}\n",
      "\n",
      "ü§ñ LLM calls: 0/0 (100%)\n",
      "\\n‚è±Ô∏è  Time: 0.0ms (no relationships to test)\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY (Keyword vs Pure LLM)\n",
      "======================================================================\n",
      "No relationships - skipping recategorization analysis\n",
      "\n",
      "‚úÖ Pure LLM categorization complete!\n",
      "\n",
      "======================================================================\n",
      "üéØ TESTING COMPLETE - 4 Categorization Modes Evaluated\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Test relationship categorization with configurable sampling and model selection\n",
    "# Location: ice_building_workflow.ipynb (FIXED VERSION)\n",
    "# Dependencies: graph_categorization.py, relationship_categories.py, LightRAG storage\n",
    "\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Force reload of graph_categorization module to pick up new functions\n",
    "import importlib\n",
    "if 'src.ice_lightrag.graph_categorization' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.ice_lightrag.graph_categorization'])\n",
    "\n",
    "# ===== CONFIGURATION: User-editable settings =====\n",
    "RANDOM_SEED = 42  # Set to None for different relationships each run, or keep 42 for reproducible testing\n",
    "OLLAMA_MODEL_OVERRIDE = 'qwen2.5:3b'  # Change to use different model (e.g., 'llama3.1:8b', 'qwen3:8b')\n",
    "\n",
    "# ===== SETUP: Imports with error handling =====\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ Relationship Categorization Test (4 Modes) - FIXED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure src is in path for notebook context\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    from src.ice_lightrag.graph_categorization import (\n",
    "        categorize_relationship,\n",
    "        categorize_relationship_with_confidence,\n",
    "        categorize_relationship_hybrid,\n",
    "        categorize_relationship_llm_only\n",
    "    )\n",
    "    from src.ice_lightrag.relationship_categories import CATEGORY_DISPLAY_ORDER, extract_relationship_types\n",
    "    print(\"‚úÖ Categorization functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   ‚Üí Ensure previous cells completed successfully\")\n",
    "    print(\"   ‚Üí Check that src/ice_lightrag/graph_categorization.py exists\\n\")\n",
    "    raise\n",
    "\n",
    "# Patch module constant for Ollama model selection\n",
    "import src.ice_lightrag.graph_categorization as graph_cat_module\n",
    "graph_cat_module.OLLAMA_MODEL = OLLAMA_MODEL_OVERRIDE\n",
    "print(f\"üîß Ollama model configured: {OLLAMA_MODEL_OVERRIDE}\\n\")\n",
    "\n",
    "# ===== HEALTH CHECK: Ollama service availability =====\n",
    "def check_ollama_service():\n",
    "    \"\"\"Check if Ollama service is running and configured model is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            # Use exact match to avoid accepting wrong model versions\n",
    "            model_available = any(m.get('name', '') == OLLAMA_MODEL_OVERRIDE for m in models)\n",
    "            return True, model_available\n",
    "        return False, False\n",
    "    except requests.RequestException:\n",
    "        return False, False\n",
    "    except (KeyError, json.JSONDecodeError):\n",
    "        return True, False\n",
    "\n",
    "ollama_running, model_available = check_ollama_service()\n",
    "\n",
    "if ollama_running and model_available:\n",
    "    print(f\"‚úÖ Ollama service running with {OLLAMA_MODEL_OVERRIDE} model\")\n",
    "elif ollama_running:\n",
    "    print(f\"‚ö†Ô∏è  Ollama running but {OLLAMA_MODEL_OVERRIDE} not found\")\n",
    "    print(f\"   ‚Üí Install: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama not running - TEST 3 & 4 will be skipped\")\n",
    "    print(\"   ‚Üí Start Ollama: brew services start ollama (macOS)\\n\")\n",
    "\n",
    "# ===== DATA LOADING: Relationships with validation =====\n",
    "storage_path = Path(ice.config.working_dir) / \"vdb_relationships.json\"\n",
    "relationships_data = {}  # Initialize to prevent NameError if file missing or error occurs\n",
    "\n",
    "if not storage_path.exists():\n",
    "    print(f\"‚ùå Storage file not found: {storage_path}\")\n",
    "    print(\"   ‚Üí Run previous cells to build the knowledge graph first\\n\")\n",
    "    print(\"   ‚ö†Ô∏è  Categorization tests will be skipped\\n\")\n",
    "else:\n",
    "    try:\n",
    "        with open(storage_path) as f:\n",
    "            relationships_data = json.load(f)\n",
    "\n",
    "        if not isinstance(relationships_data, dict) or 'data' not in relationships_data:\n",
    "            print(f\"‚ùå Invalid storage format (expected dict with 'data' key)\")\n",
    "            print(\"   ‚ö†Ô∏è  Invalid storage - tests will be skipped\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Storage error: {e}\")\n",
    "        print(\"   ‚ö†Ô∏è  Tests will be skipped\\n\")\n",
    "\n",
    "relationships_list = relationships_data.get('data', [])\n",
    "\n",
    "if not isinstance(relationships_list, list) or len(relationships_list) == 0:\n",
    "    print(f\"‚ùå No relationships found in storage\")\n",
    "    print(\"   ‚ö†Ô∏è  No data - tests will be skipped\\n\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(relationships_list)} relationships from knowledge graph\")\n",
    "\n",
    "# Configurable random sampling\n",
    "if RANDOM_SEED is not None:\n",
    "    random.seed(RANDOM_SEED)\n",
    "    sampling_mode = f\"Reproducible (seed={RANDOM_SEED}, same relationships each run)\"\n",
    "else:\n",
    "    sampling_mode = \"Random (different relationships each run)\"\n",
    "\n",
    "test_relationships = random.sample(relationships_list, min(12, len(relationships_list)))\n",
    "print(f\"   Sampling mode: {sampling_mode}\")\n",
    "print(f\"   Testing with {len(test_relationships)} relationships\")\n",
    "print(f\"   üí° To change: Set RANDOM_SEED = None (line 17) or OLLAMA_MODEL_OVERRIDE (line 18)\\n\")\n",
    "\n",
    "# ===== HELPER: Compact result display =====\n",
    "def display_results(results, title, show_confidence=False, show_llm=False):\n",
    "    \"\"\"Display categorization results in compact format\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, (name, category, confidence, used_llm) in enumerate(results, 1):\n",
    "        display_name = name[:40] + \"...\" if len(name) > 40 else name\n",
    "\n",
    "        if show_llm and used_llm:\n",
    "            indicator = \"ü§ñ\"\n",
    "        else:\n",
    "            indicator = \"‚ö°\"\n",
    "\n",
    "        if show_confidence:\n",
    "            print(f\"{i:2d}. {indicator} {display_name:43s} ‚Üí {category:20s} (conf: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(f\"{i:2d}. {display_name:45s} ‚Üí {category}\")\n",
    "\n",
    "    category_counts = Counter(cat for _, cat, _, _ in results)\n",
    "    print(f\"\\nüìä Distribution: {dict(category_counts)}\")\n",
    "\n",
    "# ===== TEST 1: Keyword-Only Baseline =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 1: Keyword-Only Categorization (Baseline)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "keyword_results = []\n",
    "\n",
    "for relationship in test_relationships:\n",
    "    content = relationship.get('content', '')\n",
    "    rel_types = extract_relationship_types(content)\n",
    "    display_name = rel_types if rel_types else content[:50]\n",
    "    category = categorize_relationship(content)\n",
    "    keyword_results.append((display_name, category, 1.0, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(keyword_results, \"Results (Keyword Matching):\", show_confidence=False)\n",
    "if len(test_relationships) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "# ===== TEST 2: Confidence Scoring Analysis =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: Keyword + Confidence Scoring\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "confidence_results = []\n",
    "\n",
    "for relationship in test_relationships:\n",
    "    content = relationship.get('content', '')\n",
    "    rel_types = extract_relationship_types(content)\n",
    "    display_name = rel_types if rel_types else content[:50]\n",
    "    category, confidence = categorize_relationship_with_confidence(content)\n",
    "    confidence_results.append((display_name, category, confidence, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(confidence_results, \"Results (with confidence scores):\", show_confidence=True)\n",
    "\n",
    "low_confidence = [(n, c, conf) for n, c, conf, _ in confidence_results if conf < 0.70]\n",
    "\n",
    "if low_confidence:\n",
    "    print(f\"\\nüîç Ambiguous relationships (confidence < 0.70): {len(low_confidence)}\")\n",
    "    for name, cat, conf in low_confidence:\n",
    "        print(f\"   - {name[:50]:50s} ‚Üí {cat:20s} (conf: {conf:.2f})\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All relationships have high confidence (‚â•0.70) - no LLM fallback needed\")\n",
    "\n",
    "if len(test_relationships) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "# ===== TEST 3: Hybrid Mode (if Ollama available) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 3: Hybrid Categorization (Keyword + LLM Fallback)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: LLM calls may take 5-10 seconds total...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    hybrid_results = []\n",
    "    llm_call_count = 0\n",
    "\n",
    "    for relationship in test_relationships:\n",
    "        content = relationship.get('content', '')\n",
    "        rel_types = extract_relationship_types(content)\n",
    "        display_name = rel_types if rel_types else content[:50]\n",
    "\n",
    "        keyword_cat, keyword_conf = categorize_relationship_with_confidence(content)\n",
    "\n",
    "        if keyword_conf >= 0.70:\n",
    "            category, confidence = keyword_cat, keyword_conf\n",
    "            used_llm = False\n",
    "        else:\n",
    "            category, confidence = categorize_relationship_hybrid(content, confidence_threshold=0.70)\n",
    "            used_llm = (confidence == 0.90)\n",
    "            if used_llm:\n",
    "                llm_call_count += 1\n",
    "\n",
    "        hybrid_results.append((display_name, category, confidence, used_llm))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(hybrid_results, \"Results (Hybrid mode - keyword + LLM):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"\\nü§ñ LLM calls: {llm_call_count}/{len(test_relationships)} ({100*llm_call_count/len(test_relationships):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\\\nü§ñ LLM calls: 0/0 (no relationships to test)\")\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "    # ===== COMPARISON SUMMARY =====\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Hybrid)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    changes = 0\n",
    "    for i in range(len(test_relationships)):\n",
    "        if keyword_results[i][1] != hybrid_results[i][1]:\n",
    "            changes += 1\n",
    "\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"Relationships recategorized by LLM: {changes}/{len(test_relationships)} ({100*changes/len(test_relationships):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No relationships - skipping recategorization analysis\")\n",
    "\n",
    "    if changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_relationships)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            hyb_cat = hybrid_results[i][1]\n",
    "            if kw_cat != hyb_cat:\n",
    "                name = keyword_results[i][0][:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {hyb_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Hybrid categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 3 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable hybrid mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "# ===== TEST 4: Pure LLM Mode (NEW) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 4: Pure LLM Categorization (All relationships)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: This will call LLM for ALL relationships (may take 30-60 seconds)...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm_results = []\n",
    "\n",
    "    for relationship in test_relationships:\n",
    "        content = relationship.get('content', '')\n",
    "        rel_types = extract_relationship_types(content)\n",
    "        display_name = rel_types if rel_types else content[:50]\n",
    "        category, confidence = categorize_relationship_llm_only(content)\n",
    "        llm_results.append((display_name, category, confidence, True))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(llm_results, \"Results (Pure LLM mode):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    print(f\"\\nü§ñ LLM calls: {len(test_relationships)}/{len(test_relationships)} (100%)\")\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "    # Compare keyword vs pure LLM (FIXED INDENTATION)\n",
    "    llm_changes = sum(1 for i in range(len(test_relationships)) if keyword_results[i][1] != llm_results[i][1])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Pure LLM)\")\n",
    "    print(\"=\" * 70)\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"Relationships recategorized by pure LLM: {llm_changes}/{len(test_relationships)} ({100*llm_changes/len(test_relationships):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No relationships - skipping recategorization analysis\")\n",
    "\n",
    "    if llm_changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_relationships)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            llm_cat = llm_results[i][1]\n",
    "            if kw_cat != llm_cat:\n",
    "                name = keyword_results[i][0][:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {llm_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Pure LLM categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 4 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable pure LLM mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ TESTING COMPLETE - 4 Categorization Modes Evaluated\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Clear graph storage (COMMENTED BY DEFAULT FOR SAFETY)\n",
    "# ## Uncomment lines below to clear existing graph:\n",
    "# from pathlib import Path\n",
    "\n",
    "# from updated_architectures.implementation.ice_simplified import create_ice_system\n",
    "# ice = create_ice_system()\n",
    "# ##########################################################\n",
    "# #                      Clear Graph                       #\n",
    "# ##########################################################\n",
    "\n",
    "# # FIX: Reset storage_path to directory (Cell 12 redefined it to file)\n",
    "# storage_path = Path(ice.config.working_dir)\n",
    "\n",
    "# if storage_path.exists():\n",
    "#     print(\"üìä PRE-DELETION CHECK\")\n",
    "#     check_storage(storage_path)\n",
    "    \n",
    "#     shutil.rmtree(storage_path) # Deletes directory + all contents.\n",
    "#     storage_path.mkdir(parents=True, exist_ok=True) # This re-creates empty directory.\n",
    "    \n",
    "#     print(\"\\n‚úÖ POST-DELETION CHECK\")\n",
    "#     check_storage(storage_path)\n",
    "#     print(\"\\n‚úÖ Graph cleared - will rebuild from scratch\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è  Storage path doesn't exist - nothing to clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Portfolio Configuration\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Holdings: AAPL, MSFT, NVDA, JNJ, UNH, LLY, JPM, V, XOM, CVX (10 stocks)\n",
      "Sector: Technology\n",
      "Data Range: 2 years historical (editable in Cell 21)\n",
      "üìÑ Source: portfolio_holdings.csv\n"
     ]
    }
   ],
   "source": [
    "# Portfolio configuration\n",
    "import pandas as pd\n",
    "\n",
    "# portfolio_df = pd.read_csv('portfolio_holdings.csv')\n",
    "portfolio_df = pd.read_csv('portfolio_holdings_folder/portfolio_holdings_diversified_10.csv')\n",
    "\n",
    "# Basic validation\n",
    "if portfolio_df.empty:\n",
    "    raise ValueError(\"Portfolio CSV is empty\")\n",
    "if 'ticker' not in portfolio_df.columns:\n",
    "    raise ValueError(\"CSV must have 'ticker' column\")\n",
    "\n",
    "holdings = portfolio_df['ticker'].tolist()\n",
    "\n",
    "print(f\"üéØ Portfolio Configuration\")\n",
    "print(f\"‚îÅ\" * 40)\n",
    "print(f\"Holdings: {', '.join(holdings)} ({len(holdings)} stocks)\")\n",
    "print(f\"Sector: {portfolio_df['sector'].iloc[0] if len(portfolio_df) > 0 else 'N/A'}\")\n",
    "print(f\"Data Range: 2 years historical (editable in Cell 21)\")\n",
    "print(f\"üìÑ Source: portfolio_holdings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Source Summary\n",
      "==================================================\n",
      "üíæ Current Graph Size: 0.00 MB\n",
      "‚ÑπÔ∏è Data source metrics available after ingestion completes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Data Source Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show ACTUAL metrics if available (not fake percentages)\n",
    "if ice and ice.core.is_ready():\n",
    "    storage_stats = ice.core.get_storage_stats()\n",
    "    print(f\"üíæ Current Graph Size: {storage_stats['total_storage_bytes'] / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Show real source info if ingestion has run\n",
    "    if 'ingestion_result' in locals() and 'metrics' in ingestion_result:\n",
    "        metrics = ingestion_result['metrics']\n",
    "        if 'data_sources_used' in metrics:\n",
    "            print(f\"‚úÖ Active sources: {', '.join(metrics['data_sources_used'])}\")\n",
    "        print(f\"üìÑ Total documents: {ingestion_result.get('total_documents', 0)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Data source metrics available after ingestion completes\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Knowledge graph not ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Data Source Contribution Visualization (Week 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_ready': True,\n",
       " 'storage_indicators': {'all_components_present': False,\n",
       "  'chunks_file_size': 0.0,\n",
       "  'entities_file_size': 0.0,\n",
       "  'relationships_file_size': 0.0,\n",
       "  'graph_file_size': 0.0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice.core.get_graph_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ICE Data Sources Summary (Phase 1 Integration)\n",
      "============================================================\n",
      "\n",
      "‚ÑπÔ∏è  Phase 1 focuses on architecture and data flow patterns.\n",
      "Actual data ingestion depends on configured API keys.\n",
      "\n",
      "\n",
      "1Ô∏è‚É£ API/MCP Sources:\n",
      "   - NewsAPI: Real-time financial news\n",
      "   - SEC EDGAR: Regulatory filings (10-K, 10-Q, 8-K)\n",
      "   - Alpha Vantage: Market data\n",
      "\n",
      "2Ô∏è‚É£ Email Pipeline (Phase 1 Enhanced Documents):\n",
      "   - Broker research with BUY/SELL signals\n",
      "   - Enhanced documents: [TICKER:NVDA|confidence:0.95]\n",
      "   - See detailed demo: imap_email_ingestion_pipeline/investment_email_extractor_simple.ipynb\n",
      "\n",
      "3Ô∏è‚É£ SEC Filings:\n",
      "   - Management commentary and financial statements\n",
      "   - Integrated via SEC EDGAR connector\n",
      "\n",
      "üí° All sources ‚Üí Single LightRAG knowledge graph via ice_simplified.py\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä ICE Data Sources Summary (Phase 1 Integration)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ÑπÔ∏è  Phase 1 focuses on architecture and data flow patterns.\")\n",
    "print(\"Actual data ingestion depends on configured API keys.\\n\")\n",
    "print(\"\\n1Ô∏è‚É£ API/MCP Sources:\")\n",
    "print(\"   - NewsAPI: Real-time financial news\")\n",
    "print(\"   - SEC EDGAR: Regulatory filings (10-K, 10-Q, 8-K)\")\n",
    "print(\"   - Alpha Vantage: Market data\")\n",
    "print(\"\\n2Ô∏è‚É£ Email Pipeline (Phase 1 Enhanced Documents):\")\n",
    "print(\"   - Broker research with BUY/SELL signals\")\n",
    "print(\"   - Enhanced documents: [TICKER:NVDA|confidence:0.95]\")\n",
    "print(\"   - See detailed demo: imap_email_ingestion_pipeline/investment_email_extractor_simple.ipynb\")\n",
    "print(\"\\n3Ô∏è‚É£ SEC Filings:\")\n",
    "print(\"   - Management commentary and financial statements\")\n",
    "print(\"   - Integrated via SEC EDGAR connector\")\n",
    "print(\"\\nüí° All sources ‚Üí Single LightRAG knowledge graph via ice_simplified.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. ICE Data Sources Integration (Week 5)\n",
    "\n",
    "ICE integrates 3 heterogeneous data sources into unified knowledge graph:\n",
    "\n",
    "**Detailed Demonstrations Available**:\n",
    "- üìß **Email Pipeline**: See `imap_email_ingestion_pipeline/investment_email_extractor_simple.ipynb` (25 cells)\n",
    "  - Entity extraction (tickers, ratings, price targets)\n",
    "  - BUY/SELL signal extraction with confidence scores\n",
    "  - Enhanced document creation with inline metadata\n",
    "  \n",
    "- üìä **Quick Summary Below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TINY Portfolio Selected\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Holdings: NVDA, AMD (2 stocks)\n",
      "\n",
      "üì¶ Default Category Limits (can be overridden in Cell 26):\n",
      "  üìß Email: 4 broker research emails\n",
      "  üì∞ News: 2/stock\n",
      "  üíπ Financial: 2/stock\n",
      "  üìà Market: 1/stock\n",
      "  üìë SEC: 2/stock\n",
      "  üî¨ Research: 0/stock\n",
      "\n",
      "üìä Estimated Docs: ~18 (before source switches in Cell 26)\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PORTFOLIO SIZE SELECTOR - Sets holdings + default limits\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "PORTFOLIO_SIZE = 'tiny'  # Options: 'tiny' | 'small' | 'medium' | 'full'\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# VALIDATION: Ensure valid configuration\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Validate PORTFOLIO_SIZE value\n",
    "valid_sizes = ['tiny', 'small', 'medium', 'full']\n",
    "if PORTFOLIO_SIZE not in valid_sizes:\n",
    "    raise ValueError(f\"‚ùå Invalid PORTFOLIO_SIZE='{PORTFOLIO_SIZE}'. Choose from: {', '.join(valid_sizes)}\")\n",
    "\n",
    "# Validate dependency for 'full' portfolio\n",
    "if PORTFOLIO_SIZE == 'full' and 'holdings' not in dir():\n",
    "    raise RuntimeError(\"‚ùå PORTFOLIO_SIZE='full' requires Cell 16 to run first! (Loads holdings from CSV)\")\n",
    "\n",
    "portfolios = {\n",
    "    'tiny': {\n",
    "        'holdings': ['NVDA', 'AMD'],\n",
    "        'email_limit': 4,\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 2,\n",
    "        'research_limit': 0\n",
    "    },\n",
    "    'small': {\n",
    "        'holdings': ['NVDA', 'AMD'],\n",
    "        'email_limit': 25,\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 2,\n",
    "        'research_limit': 0\n",
    "    },\n",
    "    'medium': {\n",
    "        'holdings': ['NVDA', 'AMD', 'TSMC'],\n",
    "        'email_limit': 50,\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 3,\n",
    "        'research_limit': 0\n",
    "    },\n",
    "    'full': {\n",
    "        'holdings': holdings,  # ‚Üê Use ALL stocks from CSV (Cell 16)\n",
    "        'email_limit': 71,\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 3,\n",
    "        'research_limit': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "portfolio = portfolios[PORTFOLIO_SIZE]\n",
    "\n",
    "# Set ALL variables (holdings + 6 category limits)\n",
    "test_holdings = portfolio['holdings']\n",
    "email_limit = portfolio['email_limit']\n",
    "news_limit = portfolio['news_limit']\n",
    "financial_limit = portfolio['financial_limit']\n",
    "market_limit = portfolio['market_limit']\n",
    "sec_limit = portfolio['sec_limit']\n",
    "research_limit = portfolio['research_limit']\n",
    "\n",
    "print(f\"üìä {PORTFOLIO_SIZE.upper()} Portfolio Selected\")\n",
    "print(f\"{'‚ïê'*70}\")\n",
    "print(f\"Holdings: {', '.join(test_holdings)} ({len(test_holdings)} stocks)\")\n",
    "print(f\"\\nüì¶ Default Category Limits (can be overridden in Cell 26):\")\n",
    "print(f\"  üìß Email: {email_limit} broker research emails\")\n",
    "print(f\"  üì∞ News: {news_limit}/stock\")\n",
    "print(f\"  üíπ Financial: {financial_limit}/stock\")\n",
    "print(f\"  üìà Market: {market_limit}/stock\")\n",
    "print(f\"  üìë SEC: {sec_limit}/stock\")\n",
    "print(f\"  üî¨ Research: {research_limit}/stock\")\n",
    "\n",
    "estimated = (\n",
    "    email_limit + \n",
    "    len(test_holdings) * (news_limit + financial_limit + market_limit + sec_limit + research_limit)\n",
    ")\n",
    "print(f\"\\nüìä Estimated Docs: ~{estimated} (before source switches in Cell 26)\")\n",
    "print(f\"{'‚ïê'*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EMAIL SELECTOR: crawl4ai_test\n",
      "======================================================================\n",
      "Description: 5 DBS research emails with URLs (tests link processing)\n",
      "Emails to process: 5\n",
      "  1. CH_HK_ Foshan Haitian Flavouring (3288 HK)_ Brewing the sauce of success (NOT RATED).eml\n",
      "  2. CH_HK_ Lianlian Digitech Co Ltd (2598 HK)_ Capturing stablecoin opportunity in crossborder payments (NOT RATED).eml\n",
      "  3. CH_HK_ Nongfu Spring Co. Ltd (9633 HK)_ Leading the pack (NOT RATED).eml\n",
      "  4. CH_HK_ Tencent Music Entertainment (1698 HK)_ Stronger growth with expanding revenue streams  (NOT RATED).eml\n",
      "  5. DBS Economics & Strategy_ India_ Effects of 50% tariff.eml\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# EMAIL SELECTOR - Choose which emails to process\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "EMAIL_SELECTOR = 'crawl4ai_test'  # Options: 'all' | 'crawl4ai_test' | 'docling_test' | 'custom'\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# VALIDATION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Validate EMAIL_SELECTOR\n",
    "valid_email = ['all', 'crawl4ai_test', 'docling_test', 'custom']\n",
    "if EMAIL_SELECTOR not in valid_email:\n",
    "    raise ValueError(f\"‚ùå Invalid EMAIL_SELECTOR='{EMAIL_SELECTOR}'. Choose from: {', '.join(valid_email)}\")\n",
    "\n",
    "email_sets = {\n",
    "    'all': {\n",
    "        'email_files': None,  # Process all 71 emails\n",
    "        'description': 'All 71 sample emails'\n",
    "    },\n",
    "    'crawl4ai_test': {\n",
    "        'email_files': [\n",
    "            'CH_HK_ Foshan Haitian Flavouring (3288 HK)_ Brewing the sauce of success (NOT RATED).eml',\n",
    "            'CH_HK_ Lianlian Digitech Co Ltd (2598 HK)_ Capturing stablecoin opportunity in crossborder payments (NOT RATED).eml',\n",
    "            'CH_HK_ Nongfu Spring Co. Ltd (9633 HK)_ Leading the pack (NOT RATED).eml',\n",
    "            'CH_HK_ Tencent Music Entertainment (1698 HK)_ Stronger growth with expanding revenue streams  (NOT RATED).eml',\n",
    "            'DBS Economics & Strategy_ India_ Effects of 50% tariff.eml'\n",
    "        ],\n",
    "        'description': '5 DBS research emails with URLs (tests link processing)'\n",
    "    },\n",
    "    'docling_test': {\n",
    "        'email_files': [\n",
    "            'Yupi Indo IPO calculations.eml',\n",
    "            'CGSI Futuristic Tour 2.0 Shenzhen & Guangzhou 14-15 April 2025.eml',\n",
    "            'DBS Economics & Strategy_ Macro Strategy_ Fed noise; Singapore GDP; lower USD.eml',\n",
    "            'CGS Global AI & Robotic Conference 2025 - Hangzhou_ 27 March 2025 | some key takeaways from Supermarket _ Sports retailers (Anta, Li Ning, 361, Xtep).eml',\n",
    "            'DBS Economics & Strategy_ China_ Capacity reduction campaign weighs on activity.eml',\n",
    "            'Tencent Q2 2025 Earnings.eml'\n",
    "        ],\n",
    "        'description': '6 emails with attachments (tests Docling PDF/Excel/image processing)'\n",
    "    },\n",
    "    'custom': {\n",
    "        'email_files': [\n",
    "            # Add your custom email files here\n",
    "            # Example: 'your_email.eml'\n",
    "        ],\n",
    "        'description': 'Custom email selection'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get selected email set\n",
    "selected_emails = email_sets[EMAIL_SELECTOR]\n",
    "email_files_to_process = selected_emails['email_files']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EMAIL SELECTOR: {EMAIL_SELECTOR}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Description: {selected_emails['description']}\")\n",
    "if email_files_to_process:\n",
    "    print(f\"Emails to process: {len(email_files_to_process)}\")\n",
    "    for i, email_file in enumerate(email_files_to_process, 1):\n",
    "        print(f\"  {i}. {email_file}\")\n",
    "else:\n",
    "    print(f\"Emails to process: All (up to email_limit)\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA SOURCE CONFIGURATION (Two-Layer Control)\n",
      "======================================================================\n",
      "\n",
      "LAYER 1: Source Type Switches\n",
      "  ‚úÖ Email Source\n",
      "  ‚úÖ API Source (News + Financial + Market + SEC)\n",
      "  ‚ùå MCP Source (Research/Search)\n",
      "\n",
      "LAYER 2: Category Limits (Active Categories Only)\n",
      "\n",
      "  ‚úÖ Email: 5 specific files (EMAIL_SELECTOR ignores email_limit)\n",
      "  ‚úÖ News: 2/stock (NewsAPI, Benzinga, Finnhub, MarketAux)\n",
      "  ‚úÖ Financial: 2/stock (FMP, Alpha Vantage)\n",
      "  ‚úÖ Market: 1/stock (Polygon)\n",
      "  ‚úÖ SEC: 2/stock (SEC EDGAR)\n",
      "  ‚ùå Research: 0/stock (MCP source disabled)\n",
      "\n",
      "üìä Estimated Documents: 19\n",
      "  - Email: 5\n",
      "  - News: 2 tickers √ó 2 = 4\n",
      "  - Financial: 2 tickers √ó 2 = 4\n",
      "  - Market: 2 tickers √ó 1 = 2\n",
      "  - SEC: 2 tickers √ó 2 = 4\n",
      "  - Research: 2 tickers √ó 0 = 0\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TWO-LAYER DATA SOURCE CONTROL SYSTEM\n",
    "# Layer 1: Source Type Switches (Boolean - Master Kill Switches)\n",
    "# Layer 2: Category Limits (Integer - Granular Control per stock)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LAYER 1: SOURCE TYPE SWITCHES (Master Kill Switches)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "email_source_enabled = True      # Controls EmailConnector\n",
    "api_source_enabled = True        # Controls ALL API sources (News, Financial, Market, SEC)\n",
    "mcp_source_enabled = False       # Controls MCP sources (Research/Search via Exa MCP)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LAYER 2: CATEGORY LIMITS (Granular Control - only active when source type enabled)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Email category (not per-stock, portfolio-wide)\n",
    "email_limit = 25                 # Top X latest emails\n",
    "\n",
    "# API categories (per stock)\n",
    "news_limit = 2                   # News articles per stock (NewsAPI, Benzinga, Finnhub, MarketAux)\n",
    "financial_limit = 2              # Financial fundamentals per stock (FMP, Alpha Vantage)\n",
    "market_limit = 1                 # Market data per stock (Polygon)\n",
    "sec_limit = 2                    # SEC filings per stock (SEC EDGAR)\n",
    "\n",
    "# MCP categories (per stock)\n",
    "research_limit = 0               # Research documents per stock (Exa MCP, on-demand only)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SPECIAL SELECTOR: EMAIL_SELECTOR (Email Category Only)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# EMAIL_SELECTOR precedence:\n",
    "# - If 'all' ‚Üí use email_limit\n",
    "# - If specific selector ‚Üí bypass email_limit, use email_files_to_process\n",
    "\n",
    "# (EMAIL_SELECTOR defined earlier in notebook, typically 'all')\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PRECEDENCE HIERARCHY (Highest to Lowest):\n",
    "# 1. Source Type Switch (email_source_enabled, api_source_enabled, mcp_source_enabled)\n",
    "# 2. Category Limit (news_limit, financial_limit, market_limit, sec_limit, research_limit)\n",
    "# 3. Special Selector (EMAIL_SELECTOR for emails only)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Apply precedence: Source type disabled ‚Üí override all category limits to 0\n",
    "if not email_source_enabled:\n",
    "    email_limit = 0\n",
    "    email_files_to_process = None  # Clear selector too\n",
    "\n",
    "if not api_source_enabled:\n",
    "    news_limit = 0\n",
    "    financial_limit = 0\n",
    "    market_limit = 0\n",
    "    sec_limit = 0\n",
    "\n",
    "if not mcp_source_enabled:\n",
    "    research_limit = 0\n",
    "\n",
    "# Email special case: EMAIL_SELECTOR can bypass email_limit (but not email_source_enabled)\n",
    "if email_source_enabled:\n",
    "    if EMAIL_SELECTOR == 'all':\n",
    "        # Use email_limit for 'all' mode\n",
    "        actual_email_count = email_limit\n",
    "        email_display = f\"{email_limit} emails (up to limit)\"\n",
    "    else:\n",
    "        # Specific selector bypasses email_limit\n",
    "        actual_email_count = len(email_files_to_process) if email_files_to_process else 0\n",
    "        email_display = f\"{actual_email_count} specific files (EMAIL_SELECTOR ignores email_limit)\"\n",
    "else:\n",
    "    # Source disabled - override everything\n",
    "    actual_email_count = 0\n",
    "    email_display = \"0 (source disabled)\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DISPLAY: Show configuration after all precedence rules applied\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DATA SOURCE CONFIGURATION (Two-Layer Control)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"LAYER 1: Source Type Switches\")\n",
    "print(f\"  {'‚úÖ' if email_source_enabled else '‚ùå'} Email Source\")\n",
    "print(f\"  {'‚úÖ' if api_source_enabled else '‚ùå'} API Source (News + Financial + Market + SEC)\")\n",
    "print(f\"  {'‚úÖ' if mcp_source_enabled else '‚ùå'} MCP Source (Research/Search)\")\n",
    "\n",
    "print(f\"\\nLAYER 2: Category Limits (Active Categories Only)\\n\")\n",
    "\n",
    "# Category 1: Email\n",
    "print(f\"  {'‚úÖ' if email_source_enabled and actual_email_count > 0 else '‚ùå'} Email: {email_display}\")\n",
    "\n",
    "# Categories 2-5: API sources\n",
    "if api_source_enabled:\n",
    "    print(f\"  {'‚úÖ' if news_limit > 0 else '‚ùå'} News: {news_limit}/stock (NewsAPI, Benzinga, Finnhub, MarketAux)\")\n",
    "    print(f\"  {'‚úÖ' if financial_limit > 0 else '‚ùå'} Financial: {financial_limit}/stock (FMP, Alpha Vantage)\")\n",
    "    print(f\"  {'‚úÖ' if market_limit > 0 else '‚ùå'} Market: {market_limit}/stock (Polygon)\")\n",
    "    print(f\"  {'‚úÖ' if sec_limit > 0 else '‚ùå'} SEC: {sec_limit}/stock (SEC EDGAR)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå News: 0/stock (API source disabled)\")\n",
    "    print(f\"  ‚ùå Financial: 0/stock (API source disabled)\")\n",
    "    print(f\"  ‚ùå Market: 0/stock (API source disabled)\")\n",
    "    print(f\"  ‚ùå SEC: 0/stock (API source disabled)\")\n",
    "\n",
    "# Category 6: MCP source\n",
    "if mcp_source_enabled:\n",
    "    print(f\"  {'‚úÖ' if research_limit > 0 else '‚ùå'} Research: {research_limit}/stock (Exa MCP)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Research: 0/stock (MCP source disabled)\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CALCULATE: Estimated documents (after ALL overrides and precedence)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "estimated_docs = (\n",
    "    actual_email_count +\n",
    "    len(test_holdings) * news_limit +\n",
    "    len(test_holdings) * financial_limit +\n",
    "    len(test_holdings) * market_limit +\n",
    "    len(test_holdings) * sec_limit +\n",
    "    len(test_holdings) * research_limit\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Estimated Documents: {estimated_docs}\")\n",
    "print(f\"  - Email: {actual_email_count}\")\n",
    "print(f\"  - News: {len(test_holdings)} tickers √ó {news_limit} = {len(test_holdings) * news_limit}\")\n",
    "print(f\"  - Financial: {len(test_holdings)} tickers √ó {financial_limit} = {len(test_holdings) * financial_limit}\")\n",
    "print(f\"  - Market: {len(test_holdings)} tickers √ó {market_limit} = {len(test_holdings) * market_limit}\")\n",
    "print(f\"  - SEC: {len(test_holdings)} tickers √ó {sec_limit} = {len(test_holdings) * sec_limit}\")\n",
    "print(f\"  - Research: {len(test_holdings)} tickers √ó {research_limit} = {len(test_holdings) * research_limit}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Fetching Portfolio Data\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "üîÑ Fetching data for 2 holdings...\n",
      "üöÄ Starting ingestion for 2 holdings (1 years)...\n",
      "\n",
      "üìä Pre-fetching documents to calculate totals...\n",
      "  ‚è≥ Fetching emails...\n",
      "     ‚úì Found 5 emails\n",
      "  ‚è≥ Fetching NVDA data...\n",
      "     ‚úì Found 7 documents (news: 2, financial: 2, market: 1, SEC: 2, research: 0)\n",
      "  ‚è≥ Fetching AMD data...\n",
      "     ‚úì Found 7 documents (news: 2, financial: 2, market: 1, SEC: 2, research: 0)\n",
      "\n",
      "üìä Total documents to process: 19\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìß DOCUMENT 1/19                                                             ‚îÉ\n",
      "‚îÉ Source: Email                                                                ‚îÉ\n",
      "‚îÉ Symbol: PORTFOLIO                                                            ‚îÉ\n",
      "‚îÉ Title: success (NOT RATED)]                                                 ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìß DOCUMENT 2/19                                                             ‚îÉ\n",
      "‚îÉ Source: Email                                                                ‚îÉ\n",
      "‚îÉ Symbol: PORTFOLIO                                                            ‚îÉ\n",
      "‚îÉ Title: opportunity in crossborder payments (NOT RATED)]                     ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìß DOCUMENT 3/19                                                             ‚îÉ\n",
      "‚îÉ Source: Email                                                                ‚îÉ\n",
      "‚îÉ Symbol: PORTFOLIO                                                            ‚îÉ\n",
      "‚îÉ Title: === ORIGINAL EMAIL CONTENT ===                                       ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìß DOCUMENT 4/19                                                             ‚îÉ\n",
      "‚îÉ Source: Email                                                                ‚îÉ\n",
      "‚îÉ Symbol: PORTFOLIO                                                            ‚îÉ\n",
      "‚îÉ Title: expanding revenue streams  (NOT RATED) ]                             ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìß DOCUMENT 5/19                                                             ‚îÉ\n",
      "‚îÉ Source: Email                                                                ‚îÉ\n",
      "‚îÉ Symbol: PORTFOLIO                                                            ‚îÉ\n",
      "‚îÉ Title: === INVESTMENT CONTEXT ===                                           ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [_] Created new empty graph fiel: ice_lightrag/storage/graph_chunk_entity_relation.graphml\n",
      "INFO: [_] Process 85110 KV load full_docs with 0 records\n",
      "INFO: [_] Process 85110 KV load text_chunks with 0 records\n",
      "INFO: [_] Process 85110 KV load full_entities with 0 records\n",
      "INFO: [_] Process 85110 KV load full_relations with 0 records\n",
      "INFO: [_] Process 85110 KV load llm_response_cache with 0 records\n",
      "INFO: [_] Process 85110 doc status load doc_status with 0 records\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-2bec4caf35ab1d8632ed0545a8994bfd\n",
      "INFO: Embedding func: 8 new workers initialized  (Timeouts: Func: 30s, Worker: 60s, Health Check: 75s)\n",
      "INFO: LLM func: 4 new workers initialized  (Timeouts: Func: 180s, Worker: 360s, Health Check: 375s)\n",
      "INFO:  == LLM cache == saving: default:extract:35c52a51ab9846301b946fa34ddb9bed\n",
      "INFO:  == LLM cache == saving: default:extract:fd37206aaf85245a62e911929c4885d4\n",
      "INFO: Chunk 1 of 6 extracted 5 Ent + 4 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:bc525b8af3029b7daac9d6f192911418\n",
      "INFO:  == LLM cache == saving: default:extract:d4ce5fbbbdd207f4d27de8232d43e025\n",
      "INFO:  == LLM cache == saving: default:extract:50fd58692ce49bbc0df4f95653d3f58c\n",
      "INFO: Chunk 2 of 6 extracted 9 Ent + 7 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:683aa6a452d851a91eb3e1363b68af13\n",
      "INFO:  == LLM cache == saving: default:extract:f6a5c7a5867a97a6a073062368710bb7\n",
      "INFO:  == LLM cache == saving: default:extract:d47abf42593dd987fb3bcb87baceb3cd\n",
      "INFO: Chunk 3 of 6 extracted 5 Ent + 4 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:52418ed290ead217ed736bf542db36e8\n",
      "INFO: Chunk 4 of 6 extracted 15 Ent + 9 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:0de4f11c57d33f9b9636256162ac95c2\n",
      "INFO:  == LLM cache == saving: default:extract:316dcd524bef905c150f3a0f64788272\n",
      "INFO: Chunk 5 of 6 extracted 6 Ent + 4 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:5c526157cb73f28c815c66c19e3aeb12\n",
      "INFO: Chunk 6 of 6 extracted 15 Ent + 13 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 49 entities from doc-2bec4caf35ab1d8632ed0545a8994bfd (async: 8)\n",
      "INFO: Merged: `Haitian` | 0+1\n",
      "INFO: Merged: `Chinese Condiments Market` | 0+1\n",
      "INFO: Merged: `Market Share` | 0+1\n",
      "INFO: Merged: `Revenue Growth` | 0+1\n",
      "INFO: Merged: `Product Innovation` | 0+1\n",
      "INFO: Merged: `Earnings Growth` | 0+1\n",
      "INFO: Merged: `Haday Brand` | 0+1\n",
      "INFO: Merged: `R&D and Distribution Network` | 0+1\n",
      "INFO: Merged: `A-share` | 0+1\n",
      "INFO: Merged: `A-H Listing` | 0+1\n",
      "INFO: Merged: `Key Risks` | 0+1\n",
      "INFO: Merged: `RMB45/HKD40` | 0+1\n",
      "INFO: Merged: `Gross Margin` | 0+1\n",
      "INFO: Merged: `CAGR` | 0+1\n",
      "INFO: Merged: `Southeast Asia` | 0+1\n",
      "INFO: Merged: `DBS Group` | 0+2\n",
      "INFO: Merged: `Confidential Note` | 0+1\n",
      "INFO: Merged: `Disclaimer` | 0+1\n",
      "INFO: Merged: `research report` | 0+1\n",
      "INFO: Merged: `General Line` | 0+1\n",
      "INFO: Merged: `Foshan Haitian Flavouring` | 0+2\n",
      "INFO: Merged: `3288 HK` | 0+1\n",
      "INFO: Merged: `603288 CH` | 0+1\n",
      "INFO: Merged: `Clement Xu` | 0+3\n",
      "INFO: Merged: `Mavis Hui` | 0+2\n",
      "INFO: Merged: `Market Capitalization` | 0+1\n",
      "INFO: Merged: `Average Daily Value` | 0+1\n",
      "INFO: Merged: `Last Traded Price` | 0+1\n",
      "INFO: Merged: `Potential Target Price` | 0+1\n",
      "INFO: Merged: `Brewing the Sauce of Success` | 0+1\n",
      "INFO: Merged: `Leading Condiment Player` | 0+1\n",
      "INFO: Merged: `7% Industry CAGR` | 0+1\n",
      "INFO: Merged: `Double-Digit Online Growth` | 0+1\n",
      "INFO: Merged: `Favorable Soybean Prices` | 0+1\n",
      "INFO: Merged: `38% Gross Margin by FY26F` | 0+1\n",
      "INFO: Merged: `DBS Bank` | 0+1\n",
      "INFO: Merged: `Image 1` | 0+1\n",
      "INFO: Merged: `Image 2` | 0+1\n",
      "INFO: Merged: `DBS Vickers Securities` | 0+1\n",
      "INFO: Merged: `DBS Bank Ltd` | 0+1\n",
      "INFO: Merged: `Email from \"DBS Group Research\"` | 0+1\n",
      "INFO: Merged: `DBS Group Research` | 0+2\n",
      "INFO: Merged: `Conflicts of Interest` | 0+1\n",
      "INFO: Merged: `20 Aug 2025` | 0+1\n",
      "INFO: Merged: `HKG 3288` | 0+1\n",
      "INFO: Merged: `EMAIL_CONTENT` | 0+1\n",
      "INFO: Merged: `Price Target` | 0+1\n",
      "INFO: Merged: `Financial Metrics` | 0+1\n",
      "INFO: Merged: `Confidence Values` | 0+1\n",
      "INFO: Phase 2: Processing 41 relations from doc-2bec4caf35ab1d8632ed0545a8994bfd (async: 8)\n",
      "INFO: Merged: `Chinese Condiments Market`~`Haitian` | 0+1\n",
      "INFO: Merged: `Haitian`~`Revenue Growth` | 0+1\n",
      "INFO: Merged: `Haitian`~`Product Innovation` | 0+1\n",
      "INFO: Merged: `Confidential Note`~`DBS Group` | 0+1\n",
      "INFO: Merged: `Haday Brand`~`Haitian` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Disclaimer` | 0+1\n",
      "INFO: Merged: `DBS Group`~`research report` | 0+1\n",
      "INFO: Merged: `DBS Group`~`General Line` | 0+1\n",
      "INFO: Merged: `3288 HK`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `603288 CH`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `Clement Xu`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `Foshan Haitian Flavouring`~`Mavis Hui` | 0+1\n",
      "INFO: Merged: `Foshan Haitian Flavouring`~`Market Capitalization` | 0+1\n",
      "INFO: Merged: `Average Daily Value`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `3288 HK`~`Last Traded Price` | 0+1\n",
      "INFO: Merged: `3288 HK`~`Potential Target Price` | 0+1\n",
      "INFO: Merged: `Foshan Haitian Flavouring`~`Leading Condiment Player` | 0+1\n",
      "INFO: Merged: `7% Industry CAGR`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `Double-Digit Online Growth`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `Favorable Soybean Prices`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `38% Gross Margin by FY26F`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `Clement Xu`~`DBS Bank` | 0+1\n",
      "INFO: Merged: `DBS Bank`~`Mavis Hui` | 0+1\n",
      "INFO: Merged: `Clement Xu`~`Image 1` | 0+1\n",
      "INFO: Merged: `Image 2`~`Mavis Hui` | 0+1\n",
      "INFO: Merged: `DBS Group`~`DBS Vickers Securities` | 0+1\n",
      "INFO: Merged: `DBS Bank Ltd`~`DBS Group` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Email from \"DBS Group Research\"` | 0+1\n",
      "INFO: Merged: `Conflicts of Interest`~`DBS Group Research` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `20 Aug 2025`~`DBS Group Research` | 0+1\n",
      "INFO: Merged: `Foshan Haitian Flavouring`~`HKG 3288` | 0+1\n",
      "INFO: Merged: `Foshan Haitian Flavouring`~`Price Target` | 0+1\n",
      "INFO: Merged: `Financial Metrics`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `Clement Xu`~`DBS Group Research` | 0+1\n",
      "INFO: Merged: `Confidence Values`~`Foshan Haitian Flavouring` | 0+1\n",
      "INFO: Merged: `A-share`~`Haitian` | 0+1\n",
      "INFO: Merged: `Haitian`~`Key Risks` | 0+1\n",
      "INFO: Merged: `Gross Margin`~`Haitian` | 0+1\n",
      "INFO: Merged: `CAGR`~`Haitian` | 0+1\n",
      "INFO: Merged: `Haitian`~`Southeast Asia` | 0+1\n",
      "INFO: Phase 3: Updating final 49(49+0) entities and  41 relations from doc-2bec4caf35ab1d8632ed0545a8994bfd\n",
      "INFO: Completed merging: 49 entities, 0 extra entities, 41 relations\n",
      "INFO: [_] Writing graph with 49 nodes, 41 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-bf806a61685479119754d410ed734caa\n",
      "INFO:  == LLM cache == saving: default:extract:ebb05958e71c73aa4a06d838ad2c4621\n",
      "INFO:  == LLM cache == saving: default:extract:e15e801ea1690768154d1ffe47230c65\n",
      "INFO: Chunk 1 of 6 extracted 3 Ent + 2 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:e3c8643c91cd11c5d9eefed3cb356dc8\n",
      "INFO:  == LLM cache == saving: default:extract:390d2ae3f101ae347b8a334579acb291\n",
      "INFO:  == LLM cache == saving: default:extract:d59ef7c90b6462e1169e8700a84dbdf0\n",
      "INFO: Chunk 2 of 6 extracted 6 Ent + 4 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:0efd6d10b40a325deaf64dc383ddacb1\n",
      "INFO: Chunk 3 of 6 extracted 9 Ent + 5 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:ccb18edeaf896f324d47ada5a1921c03\n",
      "INFO:  == LLM cache == saving: default:extract:588fc363169879b229e02569be6806ce\n",
      "INFO:  == LLM cache == saving: default:extract:bed7142e1d2bc35e1594d6b159f6d278\n",
      "INFO: Chunk 4 of 6 extracted 12 Ent + 7 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:072d2881b818773e38d77ad3941bd780\n",
      "INFO:  == LLM cache == saving: default:extract:241b99aa4c448ae0b303c5af21a245e3\n",
      "INFO: Chunk 5 of 6 extracted 7 Ent + 4 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:dd7dbc344880990518bf1bb294e0a068\n",
      "INFO: Chunk 6 of 6 extracted 11 Ent + 9 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 41 entities from doc-bf806a61685479119754d410ed734caa (async: 8)\n",
      "INFO: Merged: `Lianlian Digitech Co Ltd` | 0+2\n",
      "INFO: Merged: `Manyi Lu` | 0+2\n",
      "INFO: Merged: `Cross-Border Payments` | 0+1\n",
      "INFO: Merged: `Stablecoin` | 0+3\n",
      "INFO: Merged: `Market Cap` | 0+1\n",
      "INFO: Merged: `Last Traded Price` | 1+1\n",
      "INFO: Merged: `Potential Target` | 0+1\n",
      "INFO: Merged: `Revenue Growth` | 1+1\n",
      "INFO: Merged: `VATP DFX Lab` | 0+2\n",
      "INFO: Merged: `DBS Group` | 2+2\n",
      "INFO: Merged: `DBS Group Research` | 2+2\n",
      "INFO: Merged: `Email` | 0+1\n",
      "INFO: Merged: `Investment Banking` | 0+1\n",
      "INFO: Merged: `Futures Contracts` | 0+1\n",
      "INFO: Merged: `Securities` | 0+1\n",
      "INFO: Merged: `Investment Context` | 0+1\n",
      "INFO: Merged: `DBS Vickers Securities` | 1+1\n",
      "INFO: Merged: `DBS Bank Ltd` | 1+1\n",
      "INFO: Merged: `Confidential Note` | 1+1\n",
      "INFO: Merged: `Disclaimer` | 1+1\n",
      "INFO: Merged: `general line` | 0+1\n",
      "INFO: Merged: `DBS` | 0+1\n",
      "INFO: Merged: `Email Address` | 0+1\n",
      "INFO: Merged: `LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `Cross-Border Payment` | 0+1\n",
      "INFO: Merged: `Hong Kong (HK)` | 0+1\n",
      "INFO: Merged: `20%-30% y/y Revenue Growth` | 0+1\n",
      "INFO: Merged: `FY25/26F` | 0+1\n",
      "INFO: Merged: `10%-15% y/y Growth for Domestic Payments` | 0+1\n",
      "INFO: Merged: `Fair Value at HKD13.9` | 0+1\n",
      "INFO: Merged: `Recent Share Price Pull-back` | 0+1\n",
      "INFO: Merged: `37% Upside` | 0+1\n",
      "INFO: Merged: `Crossborder Payments` | 0+1\n",
      "INFO: Merged: `Email Communication` | 0+1\n",
      "INFO: Merged: `Financial Metrics` | 1+1\n",
      "INFO: Merged: `Analyst Manyi Lu` | 0+1\n",
      "INFO: Merged: `Email Date` | 0+1\n",
      "INFO: Merged: `Bullish Sentiment` | 0+1\n",
      "INFO: Merged: `HKD` | 0+1\n",
      "INFO: Merged: `Price Target` | 1+1\n",
      "INFO: Merged: `Financial Performance Metrics` | 0+1\n",
      "INFO: Phase 2: Processing 30 relations from doc-bf806a61685479119754d410ed734caa (async: 8)\n",
      "INFO: Merged: `Cross-Border Payments`~`Lianlian Digitech Co Ltd` | 0+1\n",
      "INFO: Merged: `DBS Group`~`DBS Group Research` | 0+1\n",
      "INFO: Merged: `Lianlian Digitech Co Ltd`~`Manyi Lu` | 0+2\n",
      "INFO: Merged: `DBS Group Research`~`Email` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Futures Contracts` | 0+1\n",
      "INFO: Merged: `DBS Bank Ltd`~`DBS Vickers Securities` | 0+1\n",
      "INFO: Merged: `Lianlian Digitech Co Ltd`~`Stablecoin` | 0+1\n",
      "INFO: Merged: `Confidential Note`~`Disclaimer` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Securities` | 0+1\n",
      "INFO: Merged: `DBS`~`Manyi Lu` | 0+1\n",
      "INFO: Merged: `Lianlian Digitech Co Ltd`~`Revenue Growth` | 0+1\n",
      "INFO: Merged: `Email Address`~`Manyi Lu` | 0+1\n",
      "INFO: Merged: `LianLian DigiTech`~`VATP DFX Lab` | 0+1\n",
      "INFO: Merged: `DBS Group`~`DBS Vickers Securities` | 1+1\n",
      "INFO: Merged: `DBS Bank Ltd`~`DBS Group` | 1+1\n",
      "INFO: Merged: `Stablecoin`~`VATP DFX Lab` | 0+1\n",
      "INFO: Merged: `Cross-Border Payment`~`LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `20%-30% y/y Revenue Growth`~`LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `Lianlian Digitech Co Ltd`~`VATP DFX Lab` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Lianlian Digitech Co Ltd` | 0+1\n",
      "INFO: Merged: `Crossborder Payments`~`Stablecoin` | 0+1\n",
      "INFO: Merged: `Hong Kong (HK)`~`LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `Email Communication`~`Lianlian Digitech Co Ltd` | 0+1\n",
      "INFO: Merged: `10%-15% y/y Growth for Domestic Payments`~`LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `Bullish Sentiment`~`Lianlian Digitech Co Ltd` | 0+1\n",
      "INFO: Merged: `Fair Value at HKD13.9`~`LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `Financial Performance Metrics`~`Lianlian Digitech Co Ltd` | 0+1\n",
      "INFO: Merged: `LianLian DigiTech`~`Recent Share Price Pull-back` | 0+1\n",
      "INFO: Merged: `37% Upside`~`LianLian DigiTech` | 0+1\n",
      "INFO: Merged: `Lianlian Digitech Co Ltd`~`Price Target` | 0+1\n",
      "INFO: Phase 3: Updating final 41(41+0) entities and  30 relations from doc-bf806a61685479119754d410ed734caa\n",
      "INFO: Completed merging: 41 entities, 0 extra entities, 30 relations\n",
      "INFO: [_] Writing graph with 80 nodes, 69 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-e9f35731b5ae45274d0df89520d323e1\n",
      "INFO:  == LLM cache == saving: default:extract:5d49fe0c5491ce4c7798c8a9392a627e\n",
      "INFO:  == LLM cache == saving: default:extract:8a70a3345f8a32b3366d5e6565a60098\n",
      "INFO: Chunk 1 of 6 extracted 3 Ent + 2 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:49c6913459ec437df4c5b83874f31956\n",
      "INFO:  == LLM cache == saving: default:extract:da1f42bfe919a89f9bc7154552361aaa\n",
      "INFO:  == LLM cache == saving: default:extract:1987e161383139ba1b97c4530b9c0e06\n",
      "INFO:  == LLM cache == saving: default:extract:b6a023993bc8ae0d551a180d6c319cca\n",
      "INFO:  == LLM cache == saving: default:extract:37a4c5d570d665dba37b288b8556a1b9\n",
      "INFO: Chunk 2 of 6 extracted 9 Ent + 8 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:12d84142e26c06b3f4629b4c4d248cfd\n",
      "INFO: Chunk 3 of 6 extracted 10 Ent + 7 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:91eb48ce9b49fa8fe5777b4653cf33ad\n",
      "INFO: Chunk 4 of 6 extracted 6 Ent + 5 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:05dc5fec76416e70ffb9d00552dfbe42\n",
      "INFO: Chunk 5 of 6 extracted 13 Ent + 10 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:d1b8ebd3312e0e7914f50470fbbc91ef\n",
      "INFO:  == LLM cache == saving: default:extract:47df952bcd4451bafc508d7feedaa411\n",
      "INFO: Chunk 6 of 6 extracted 9 Ent + 5 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 42 entities from doc-e9f35731b5ae45274d0df89520d323e1 (async: 8)\n",
      "INFO: Merged: `Nongfu Spring Co. Ltd` | 0+2\n",
      "INFO: Merged: `Alison Fok` | 0+4\n",
      "INFO: Merged: `Mavis Hui` | 2+2\n",
      "INFO: Merged: `20% Upside` | 0+1\n",
      "INFO: Merged: `Revenue Growth` | 2+2\n",
      "INFO: Merged: `Net Margin Expansion` | 0+1\n",
      "INFO: Merged: `Fair Value` | 0+1\n",
      "INFO: Merged: `Market Cap` | 1+1\n",
      "INFO: Merged: `Average Daily Value` | 1+1\n",
      "INFO: Merged: `Nongfu Spring` | 0+1\n",
      "INFO: Merged: `China` | 0+1\n",
      "INFO: Merged: `RTD Beverage Industry` | 0+1\n",
      "INFO: Merged: `Oriental Leaf` | 0+1\n",
      "INFO: Merged: `2024` | 0+1\n",
      "INFO: Merged: `Gross Profit Margin` | 0+1\n",
      "INFO: Merged: `Packaged Drinking Water` | 0+1\n",
      "INFO: Merged: `Market Share Recovery` | 0+1\n",
      "INFO: Merged: `HKD55.7` | 0+1\n",
      "INFO: Merged: `DBS Vickers Securities` | 2+1\n",
      "INFO: Merged: `DBS Bank Ltd` | 2+1\n",
      "INFO: Merged: `DBS Group` | 4+2\n",
      "INFO: Merged: `Confidential Note` | 2+1\n",
      "INFO: Merged: `Disclaimer` | 2+1\n",
      "INFO: Merged: `General Line` | 1+1\n",
      "INFO: Merged: `DBS` | 1+1\n",
      "INFO: Merged: `Email` | 1+1\n",
      "INFO: Merged: `Securities` | 1+1\n",
      "INFO: Merged: `Commodities` | 0+1\n",
      "INFO: Merged: `Futures Contracts` | 1+1\n",
      "INFO: Merged: `Derivatives` | 0+1\n",
      "INFO: Merged: `DBS Group Research` | 4+2\n",
      "INFO: Merged: `Email Sender` | 0+1\n",
      "INFO: Merged: `Received Date` | 0+1\n",
      "INFO: Merged: `Email Report` | 0+1\n",
      "INFO: Merged: `28 Aug 2025` | 0+1\n",
      "INFO: Merged: `HKD46.44` | 0+1\n",
      "INFO: Merged: `Price Target HKD58.00` | 0+1\n",
      "INFO: Merged: `Email Content` | 0+1\n",
      "INFO: Merged: `Traded Price` | 0+1\n",
      "INFO: Merged: `Double Digit Growth in FY25` | 0+1\n",
      "INFO: Merged: `Potential Target Prices` | 0+1\n",
      "INFO: Merged: `Financial Metrics` | 2+1\n",
      "INFO: Phase 2: Processing 36 relations from doc-e9f35731b5ae45274d0df89520d323e1 (async: 8)\n",
      "INFO: Merged: `Alison Fok`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Mavis Hui`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `China`~`Nongfu Spring` | 0+1\n",
      "INFO: Merged: `Nongfu Spring`~`RTD Beverage Industry` | 0+1\n",
      "INFO: Merged: `20% Upside`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Nongfu Spring`~`Oriental Leaf` | 0+1\n",
      "INFO: Merged: `Nongfu Spring Co. Ltd`~`Revenue Growth` | 0+2\n",
      "INFO: Merged: `Market Share Recovery`~`Nongfu Spring` | 0+1\n",
      "INFO: Merged: `Net Margin Expansion`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Gross Profit Margin`~`Nongfu Spring` | 0+1\n",
      "INFO: Merged: `DBS Group`~`DBS Vickers Securities` | 2+1\n",
      "INFO: Merged: `Fair Value`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Nongfu Spring`~`Packaged Drinking Water` | 0+1\n",
      "INFO: Merged: `Market Cap`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `DBS Bank Ltd`~`DBS Group` | 2+1\n",
      "INFO: Merged: `DBS Vickers Securities`~`General Line` | 0+1\n",
      "INFO: Merged: `HKD55.7`~`Nongfu Spring` | 0+1\n",
      "INFO: Merged: `Alison Fok`~`DBS` | 0+1\n",
      "INFO: Merged: `Confidential Note`~`DBS Group` | 1+1\n",
      "INFO: Merged: `Average Daily Value`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `DBS`~`Mavis Hui` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Disclaimer` | 1+1\n",
      "INFO: Merged: `DBS Group Research`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Securities` | 1+1\n",
      "INFO: Merged: `Email Report`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Alison Fok`~`DBS Group Research` | 0+1\n",
      "INFO: Merged: `Commodities`~`DBS Group` | 0+1\n",
      "INFO: Merged: `HKD46.44`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Futures Contracts` | 1+1\n",
      "INFO: Merged: `Nongfu Spring Co. Ltd`~`Price Target HKD58.00` | 0+1\n",
      "INFO: Merged: `Email Content`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Double Digit Growth in FY25`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `DBS Group`~`Derivatives` | 0+1\n",
      "INFO: Merged: `Financial Metrics`~`Nongfu Spring Co. Ltd` | 0+1\n",
      "INFO: Merged: `Nongfu Spring Co. Ltd`~`Potential Target Prices` | 0+1\n",
      "INFO: Merged: `DBS Group`~`DBS Group Research` | 1+1\n",
      "INFO: Phase 3: Updating final 42(42+0) entities and  36 relations from doc-e9f35731b5ae45274d0df89520d323e1\n",
      "INFO: Completed merging: 42 entities, 0 extra entities, 36 relations\n",
      "INFO: [_] Writing graph with 106 nodes, 98 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-59dee826303f9e53f84426518ec38c66\n",
      "INFO:  == LLM cache == saving: default:extract:05e98263b0a6d77712e6dfeb7df0de87\n",
      "INFO:  == LLM cache == saving: default:extract:adbe57c403c6f1f4b51e6b06ef5ee904\n",
      "INFO:  == LLM cache == saving: default:extract:02c028dafa9598f30cc73595a80c8a25\n",
      "WARNING: chunk-6e6869f845ff61c2ff2b1868964aa432: Relation `offline concerts and merchandise` extraction failed -- expecting 5 fields but got 4\n",
      "INFO:  == LLM cache == saving: default:extract:78ebeeb634c8441884cb3ca288e24c19\n",
      "INFO: Chunk 1 of 7 extracted 8 Ent + 6 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:b1f3dfe8e86aa3c59e72bd28517dec09\n",
      "INFO:  == LLM cache == saving: default:extract:e7a8b8b0035d9ab3a4d70064c3cf6fd7\n",
      "INFO:  == LLM cache == saving: default:extract:f5b9701c9cf63817a4174ba092da2637\n",
      "INFO: Chunk 2 of 7 extracted 2 Ent + 0 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:458e3839311cfa0392532548a0f078f7\n",
      "INFO: Chunk 3 of 7 extracted 11 Ent + 8 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:7080839654c67de59e2de076af600e6e\n",
      "INFO: Chunk 4 of 7 extracted 10 Ent + 8 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:5224b69d94fd08945362dc3fa072d9a3\n",
      "INFO: Chunk 5 of 7 extracted 17 Ent + 9 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:6c5728cb12de3a2a721d089fb4670c25\n",
      "INFO:  == LLM cache == saving: default:extract:0c6a51390266411f37fea48c9bc163c8\n",
      "INFO:  == LLM cache == saving: default:extract:5cf5a49d36ba44c34736b388bbb70031\n",
      "INFO: Chunk 6 of 7 extracted 6 Ent + 3 Rel\n",
      "INFO:  == LLM cache == saving: default:extract:5074d20a64ac20b48caa6b97240ab4ee\n",
      "INFO: Chunk 7 of 7 extracted 9 Ent + 5 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 57 entities from doc-59dee826303f9e53f84426518ec38c66 (async: 8)\n",
      "INFO: Merged: `Email` | 2+1\n",
      "INFO: Merged: `Securities` | 2+1\n",
      "INFO: Merged: `Date` | 0+1\n",
      "INFO: Merged: `Time` | 0+1\n",
      "INFO: Merged: `Tencent Music Entertainment` | 0+2\n",
      "INFO: Merged: `Edmond Fok` | 0+2\n",
      "INFO: Merged: `Andy Yu` | 0+2\n",
      "INFO: Merged: `Revenue Streams` | 0+1\n",
      "INFO: Merged: `Email Correspondence` | 0+1\n",
      "INFO: Merged: `2Q25 Adjusted Earnings` | 0+2\n",
      "INFO: Merged: `Ticker Symbols` | 0+1\n",
      "INFO: Merged: `Financial Metrics` | 3+1\n",
      "INFO: Merged: `Price Targets` | 0+1\n",
      "INFO: Merged: `Sentiment` | 0+1\n",
      "INFO: Merged: `USD58,402m Market Cap` | 0+1\n",
      "INFO: Merged: `HKD88.30` | 0+1\n",
      "INFO: Merged: `TME US` | 0+1\n",
      "INFO: Merged: `USD25.39` | 0+1\n",
      "INFO: Merged: `Analyst Edmond Fok` | 0+1\n",
      "INFO: Merged: `Analyst Andy Yu` | 0+1\n",
      "INFO: Merged: `Analyst Sachin MITTAL` | 0+1\n",
      "INFO: Merged: `Stronger Growth with Expanding Revenue Streams` | 0+1\n",
      "INFO: Merged: `DBS Vickers Securities` | 3+1\n",
      "INFO: Merged: `DBS Bank Ltd` | 3+1\n",
      "INFO: Merged: `Confidential Note` | 3+1\n",
      "INFO: Merged: `Disclaimer` | 3+1\n",
      "INFO: Merged: `General Line` | 2+1\n",
      "INFO: Merged: `research report` | 1+1\n",
      "INFO: Merged: `sales person` | 0+1\n",
      "INFO: Merged: `privileged information` | 0+1\n",
      "INFO: Merged: `Sachin Mittal` | 0+1\n",
      "INFO: Merged: `2Q25` | 0+1\n",
      "INFO: Merged: `RMB2.57bn` | 0+1\n",
      "INFO: Merged: `RMB4.38bn` | 0+1\n",
      "INFO: Merged: `incentivised ads` | 0+1\n",
      "INFO: Merged: `SVIP` | 0+1\n",
      "INFO: Merged: `Bubble interactive fans platform` | 0+1\n",
      "INFO: Merged: `3Q25` | 0+1\n",
      "INFO: Merged: `offline concerts and merchandise` | 0+1\n",
      "INFO: Merged: `discipline opex` | 0+1\n",
      "INFO: Merged: `strong ad revenue` | 0+1\n",
      "INFO: Merged: `ad revenue` | 0+1\n",
      "INFO: Merged: `fans economy` | 0+1\n",
      "INFO: Merged: `opex growth` | 0+1\n",
      "INFO: Merged: `monetisation strategy` | 0+1\n",
      "INFO: Merged: `mass users` | 0+1\n",
      "INFO:  == LLM cache == saving: default:summary:335953c9c60689123ee6cecb408e578a\n",
      "INFO: LLMmrg: `DBS Group Research` | 6+2\n",
      "INFO: Merged: `content and engagement` | 0+1\n",
      "INFO: Merged: `FY25 revenue` | 0+1\n",
      "INFO: Merged: `DBS` | 2+1\n",
      "INFO: Merged: `TME` | 0+1\n",
      "INFO: Merged: `Spotify` | 0+1\n",
      "INFO: Merged: `Chinese Users` | 0+1\n",
      "INFO: Merged: `Ads-Driven Approach` | 0+1\n",
      "INFO: Merged: `Operating Leverage` | 0+1\n",
      "INFO: Merged: `FY25F/26F Earnings` | 0+1\n",
      "INFO:  == LLM cache == saving: default:summary:95475ac3e139e582d0d576913912bfcf\n",
      "INFO: LLMmrg: `DBS Group` | 6+2\n",
      "INFO: Phase 2: Processing 39 relations from doc-59dee826303f9e53f84426518ec38c66 (async: 8)\n",
      "INFO: Merged: `DBS Group`~`Email` | 0+1\n",
      "INFO: Merged: `Revenue Streams`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `Sentiment`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Email` | 1+1\n",
      "INFO: Merged: `DBS Group`~`Securities` | 2+1\n",
      "INFO: Merged: `Financial Metrics`~`Price Targets` | 0+1\n",
      "INFO: Merged: `Tencent Music Entertainment`~`USD58,402m Market Cap` | 0+1\n",
      "INFO: Merged: `Financial Metrics`~`Ticker Symbols` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `2Q25 Adjusted Earnings`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Edmond Fok` | 0+1\n",
      "INFO: Merged: `TME US`~`USD25.39` | 0+1\n",
      "INFO: Merged: `Analyst Edmond Fok`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `Andy Yu`~`DBS Group Research` | 0+1\n",
      "INFO: Merged: `DBS Group`~`DBS Vickers Securities` | 3+1\n",
      "INFO: Merged: `Analyst Andy Yu`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Email Correspondence` | 0+1\n",
      "INFO: Merged: `DBS Bank Ltd`~`DBS Group` | 3+1\n",
      "INFO: Merged: `Confidential Note`~`research report` | 0+1\n",
      "INFO: Merged: `Analyst Sachin MITTAL`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `2Q25`~`RMB2.57bn` | 0+1\n",
      "INFO: Merged: `DBS Group`~`General Line` | 1+1\n",
      "INFO: Merged: `RMB4.38bn`~`SVIP` | 0+1\n",
      "INFO: Merged: `incentivised ads`~`strong ad revenue` | 0+1\n",
      "INFO: Merged: `HKD88.30`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `3Q25`~`RMB4.38bn` | 0+1\n",
      "INFO: Merged: `2Q25`~`opex growth` | 0+1\n",
      "INFO: Merged: `DBS Group`~`sales person` | 0+1\n",
      "INFO: Merged: `mass users`~`monetisation strategy` | 0+1\n",
      "INFO: Merged: `discipline opex`~`strong ad revenue` | 0+1\n",
      "INFO: Merged: `ad revenue`~`fans economy` | 0+1\n",
      "INFO: Merged: `Stronger Growth with Expanding Revenue Streams`~`Tencent Music Entertainment` | 0+1\n",
      "INFO: Merged: `3Q25`~`FY25 revenue` | 0+1\n",
      "INFO: Merged: `Spotify`~`TME` | 0+1\n",
      "INFO: Merged: `DBS`~`Edmond Fok` | 0+1\n",
      "INFO: Merged: `FY25F/26F Earnings`~`Operating Leverage` | 0+1\n",
      "INFO: Merged: `Chinese Users`~`TME` | 0+1\n",
      "INFO: Merged: `DBS`~`FY25F/26F Earnings` | 0+1\n",
      "INFO: Merged: `Ads-Driven Approach`~`TME` | 0+1\n",
      "INFO: Phase 3: Updating final 57(57+0) entities and  39 relations from doc-59dee826303f9e53f84426518ec38c66\n",
      "INFO: Completed merging: 57 entities, 0 extra entities, 39 relations\n",
      "INFO: [_] Writing graph with 151 nodes, 132 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-94b4877c6ca84a40d273c581a75b2655\n",
      "INFO:  == LLM cache == saving: default:extract:711f7760b6fd0fdf1455494b0817ad51\n",
      "INFO:  == LLM cache == saving: default:extract:dc98c821d9a109b77130c1fa2cab712f\n",
      "INFO: Chunk 1 of 1 extracted 5 Ent + 3 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 5 entities from doc-94b4877c6ca84a40d273c581a75b2655 (async: 8)\n",
      "INFO: Merged: `DBS Group Research` | 1+1\n",
      "INFO: Merged: `India` | 0+1\n",
      "INFO: Merged: `50% Tariff` | 0+1\n",
      "INFO: Merged: `Email from DBS Group Research` | 0+1\n",
      "INFO: Merged: `Aug 28, 2025` | 0+1\n",
      "INFO: Phase 2: Processing 3 relations from doc-94b4877c6ca84a40d273c581a75b2655 (async: 8)\n",
      "INFO: Merged: `DBS Group Research`~`India` | 0+1\n",
      "INFO: Merged: `50% Tariff`~`India` | 0+1\n",
      "INFO: Merged: `DBS Group Research`~`Email from DBS Group Research` | 0+1\n",
      "INFO: Phase 3: Updating final 5(5+0) entities and  3 relations from doc-94b4877c6ca84a40d273c581a75b2655\n",
      "INFO: Completed merging: 5 entities, 0 extra entities, 3 relations\n",
      "INFO: [_] Writing graph with 155 nodes, 135 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-2155bca3959d5eb57f072396e9f62128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üì∞ DOCUMENT 6/19                                                             ‚îÉ\n",
      "‚îÉ Source: News                                                                 ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: Can Nvidia Stock Hit $300 in 2025?                                   ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üì∞ DOCUMENT 7/19                                                             ‚îÉ\n",
      "‚îÉ Source: News                                                                 ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: How to trade the AI boom beyond chips                                ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üíπ DOCUMENT 8/19                                                             ‚îÉ\n",
      "‚îÉ Source: Financial API                                                        ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: NVIDIA Corporation                                                   ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üíπ DOCUMENT 9/19                                                             ‚îÉ\n",
      "‚îÉ Source: Financial API                                                        ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: Company Overview: NVIDIA Corporation                                 ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üíπ DOCUMENT 10/19                                                            ‚îÉ\n",
      "‚îÉ Source: Financial API                                                        ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: Company Details: Nvidia Corp                                         ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìë DOCUMENT 11/19                                                            ‚îÉ\n",
      "‚îÉ Source: SEC Filing                                                           ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: 144                                                                  ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìë DOCUMENT 12/19                                                            ‚îÉ\n",
      "‚îÉ Source: SEC Filing                                                           ‚îÉ\n",
      "‚îÉ Symbol: NVDA                                                                 ‚îÉ\n",
      "‚îÉ Title: 144                                                                  ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:  == LLM cache == saving: default:extract:54e31491db46d2f08b5ba883cf012db8\n",
      "INFO:  == LLM cache == saving: default:extract:83e8ee8743e46354f0a2d6ef3c0cbac1\n",
      "INFO: Chunk 1 of 1 extracted 6 Ent + 3 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 6 entities from doc-2155bca3959d5eb57f072396e9f62128 (async: 8)\n",
      "INFO: Merged: `Nvidia` | 0+1\n",
      "INFO: Merged: `AI Revolution` | 0+1\n",
      "INFO: Merged: `Stock Price Forecast` | 0+1\n",
      "INFO: Merged: `Barchart.com` | 0+1\n",
      "INFO: Merged: `2025-10-22` | 0+1\n",
      "INFO: Merged: `NVDA` | 0+1\n",
      "INFO: Phase 2: Processing 3 relations from doc-2155bca3959d5eb57f072396e9f62128 (async: 8)\n",
      "INFO: Merged: `AI Revolution`~`Nvidia` | 0+1\n",
      "INFO: Merged: `Nvidia`~`Stock Price Forecast` | 0+1\n",
      "INFO: Merged: `Barchart.com`~`Nvidia` | 0+1\n",
      "INFO: Phase 3: Updating final 6(6+0) entities and  3 relations from doc-2155bca3959d5eb57f072396e9f62128\n",
      "INFO: Completed merging: 6 entities, 0 extra entities, 3 relations\n",
      "INFO: [_] Writing graph with 161 nodes, 138 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-3026db65f66d6de3479659217d04a133\n",
      "INFO:  == LLM cache == saving: default:extract:6f804b99e3343ccab3392ff97611aafe\n",
      "INFO:  == LLM cache == saving: default:extract:521eea92d709664ede6fe0dcb242473a\n",
      "INFO: Chunk 1 of 1 extracted 10 Ent + 5 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 10 entities from doc-3026db65f66d6de3479659217d04a133 (async: 8)\n",
      "INFO: Merged: `Nvidia` | 1+1\n",
      "INFO: Merged: `Artificial Intelligence` | 0+1\n",
      "INFO: Merged: `Large Language Models` | 0+1\n",
      "INFO: Merged: `Big Data Centers` | 0+1\n",
      "INFO: Merged: `Yahoo Entertainment` | 0+1\n",
      "INFO: Merged: `AI Cycle` | 0+1\n",
      "INFO: Merged: `News Article` | 0+1\n",
      "INFO: Merged: `NVDA` | 1+1\n",
      "INFO: Merged: `2025-09-22` | 0+1\n",
      "INFO: Merged: `https://finance.yahoo.com/video/trade-ai-boom-beyond-chips-213601170.html` | 0+1\n",
      "INFO: Phase 2: Processing 5 relations from doc-3026db65f66d6de3479659217d04a133 (async: 8)\n",
      "INFO: Merged: `Artificial Intelligence`~`Nvidia` | 0+1\n",
      "INFO: Merged: `News Article`~`Yahoo Entertainment` | 0+1\n",
      "INFO: Merged: `AI Cycle`~`Big Data Centers` | 0+1\n",
      "INFO: Merged: `NVDA`~`News Article` | 0+1\n",
      "INFO: Merged: `Artificial Intelligence`~`Large Language Models` | 0+1\n",
      "INFO: Phase 3: Updating final 10(10+0) entities and  5 relations from doc-3026db65f66d6de3479659217d04a133\n",
      "INFO: Completed merging: 10 entities, 0 extra entities, 5 relations\n",
      "INFO: [_] Writing graph with 169 nodes, 143 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-4103269f86b9d4881931a11c87866dab\n",
      "INFO:  == LLM cache == saving: default:extract:7cf447f6edc4c94b8939e681af0fa0de\n",
      "INFO:  == LLM cache == saving: default:extract:5dad75ad6dfedea28c9989706a23c792\n",
      "INFO: Chunk 1 of 1 extracted 20 Ent + 13 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 20 entities from doc-4103269f86b9d4881931a11c87866dab (async: 8)\n",
      "INFO: Merged: `NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Jen-Hsun Huang` | 0+1\n",
      "INFO: Merged: `Santa Clara` | 0+1\n",
      "INFO: Merged: `GeForce GPUs` | 0+1\n",
      "INFO: Merged: `GeForce NOW` | 0+1\n",
      "INFO: Merged: `Quadro/NVIDIA RTX GPUs` | 0+1\n",
      "INFO: Merged: `Data Center Platforms` | 0+1\n",
      "INFO: Merged: `Mellanox Networking Solutions` | 0+1\n",
      "INFO: Merged: `Cryptocurrency Mining Processors` | 0+1\n",
      "INFO: Merged: `Jetson` | 0+1\n",
      "INFO: Merged: `Kroger Co.` | 0+1\n",
      "INFO: Merged: `US` | 0+1\n",
      "INFO: Merged: `SECTOR: Technology` | 0+1\n",
      "INFO: Merged: `INDUSTRY: Semiconductors` | 0+1\n",
      "INFO: Merged: `Market Cap` | 2+1\n",
      "INFO: Merged: `Current Price` | 0+1\n",
      "INFO: Merged: `Volume Average` | 0+1\n",
      "INFO: Merged: `Beta` | 0+1\n",
      "INFO: Merged: `IPO Date` | 0+1\n",
      "INFO: Merged: `52 Week Range` | 0+1\n",
      "INFO: Phase 2: Processing 13 relations from doc-4103269f86b9d4881931a11c87866dab (async: 8)\n",
      "INFO: Merged: `Jen-Hsun Huang`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Santa Clara` | 0+1\n",
      "INFO: Merged: `GeForce GPUs`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `GeForce NOW`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Data Center Platforms`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Kroger Co.`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Cryptocurrency Mining Processors`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Market Cap`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Current Price`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Volume Average` | 0+1\n",
      "INFO: Merged: `Beta`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `IPO Date`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `52 Week Range`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Phase 3: Updating final 20(20+0) entities and  13 relations from doc-4103269f86b9d4881931a11c87866dab\n",
      "INFO: Completed merging: 20 entities, 0 extra entities, 13 relations\n",
      "INFO: [_] Writing graph with 188 nodes, 156 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-f4e2fb0944fd638ef227a9ea52d90538\n",
      "INFO:  == LLM cache == saving: default:extract:1c77bcd8b2c1c9aae04c99e10ee36e72\n",
      "INFO:  == LLM cache == saving: default:extract:1d5178e4a8b34586135e50b83f50982b\n",
      "INFO: Chunk 1 of 1 extracted 18 Ent + 17 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 18 entities from doc-f4e2fb0944fd638ef227a9ea52d90538 (async: 8)\n",
      "INFO: Merged: `NVIDIA Corporation` | 1+1\n",
      "INFO: Merged: `Santa Clara` | 1+1\n",
      "INFO: Merged: `Technology Sector` | 0+1\n",
      "INFO: Merged: `Semiconductors Industry` | 0+1\n",
      "INFO: Merged: `Graphics Processing Units (GPUs)` | 0+1\n",
      "INFO: Merged: `Artificial Intelligence (AI)` | 0+1\n",
      "INFO: Merged: `System on a Chip (SoC)` | 0+1\n",
      "INFO: Merged: `Market Capitalization` | 1+1\n",
      "INFO: Merged: `Dividend Yield` | 0+1\n",
      "INFO: Merged: `Earnings Per Share (EPS)` | 0+1\n",
      "INFO: Merged: `Common Stock` | 0+1\n",
      "INFO: Merged: `NASDAQ` | 0+1\n",
      "INFO: Merged: `USD` | 0+1\n",
      "INFO: Merged: `Profit Margin` | 0+1\n",
      "INFO: Merged: `Operating Margin (TTM)` | 0+1\n",
      "INFO: Merged: `Return on Assets (TTM)` | 0+1\n",
      "INFO: Merged: `Return on Equity (TTM)` | 0+1\n",
      "INFO: Merged: `Revenue Per Share (TTM)` | 0+1\n",
      "INFO: Phase 2: Processing 17 relations from doc-f4e2fb0944fd638ef227a9ea52d90538 (async: 8)\n",
      "INFO: Merged: `NVIDIA Corporation`~`Santa Clara` | 1+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Technology Sector` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Semiconductors Industry` | 0+1\n",
      "INFO: Merged: `Graphics Processing Units (GPUs)`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Artificial Intelligence (AI)`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`System on a Chip (SoC)` | 0+1\n",
      "INFO: Merged: `Market Capitalization`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Dividend Yield`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Earnings Per Share (EPS)`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Common Stock`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `NASDAQ`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`USD` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Profit Margin` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Operating Margin (TTM)` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Return on Assets (TTM)` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Return on Equity (TTM)` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Revenue Per Share (TTM)` | 0+1\n",
      "INFO: Phase 3: Updating final 18(18+0) entities and  17 relations from doc-f4e2fb0944fd638ef227a9ea52d90538\n",
      "INFO: Completed merging: 18 entities, 0 extra entities, 17 relations\n",
      "INFO: [_] Writing graph with 203 nodes, 172 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-1577ff7fc656c607a4f16a343ee1aa45\n",
      "INFO:  == LLM cache == saving: default:extract:5ff0779b1f7c9efd2d911a0f09ad0dcf\n",
      "INFO:  == LLM cache == saving: default:extract:e06d9543aeea31f2d400cc7d3e8fb7a9\n",
      "INFO: Chunk 1 of 1 extracted 9 Ent + 7 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 9 entities from doc-1577ff7fc656c607a4f16a343ee1aa45 (async: 8)\n",
      "INFO: Merged: `Nvidia Corp` | 0+1\n",
      "INFO: Merged: `NVDA` | 2+1\n",
      "INFO: Merged: `United States` | 0+1\n",
      "INFO: Merged: `XNAS` | 0+1\n",
      "INFO: Merged: `Cuda` | 0+1\n",
      "INFO: Merged: `Graphics Processing Units` | 0+1\n",
      "INFO: Merged: `Market Capitalization` | 2+1\n",
      "INFO: Merged: `Outstanding Shares` | 0+1\n",
      "INFO: Merged: `Polygon.io` | 0+1\n",
      "INFO: Phase 2: Processing 7 relations from doc-1577ff7fc656c607a4f16a343ee1aa45 (async: 8)\n",
      "INFO: Merged: `NVDA`~`Nvidia Corp` | 0+1\n",
      "INFO: Merged: `Nvidia Corp`~`United States` | 0+1\n",
      "INFO: Merged: `Graphics Processing Units`~`Nvidia Corp` | 0+1\n",
      "INFO: Merged: `Cuda`~`Nvidia Corp` | 0+1\n",
      "INFO: Merged: `Market Capitalization`~`Nvidia Corp` | 0+1\n",
      "INFO: Merged: `Nvidia Corp`~`Outstanding Shares` | 0+1\n",
      "INFO: Merged: `Nvidia Corp`~`Polygon.io` | 0+1\n",
      "INFO: Phase 3: Updating final 9(9+0) entities and  7 relations from doc-1577ff7fc656c607a4f16a343ee1aa45\n",
      "INFO: Completed merging: 9 entities, 0 extra entities, 7 relations\n",
      "INFO: [_] Writing graph with 210 nodes, 179 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-742aa5eabc77c8c51b4bc42ad3b6a8c4\n",
      "INFO:  == LLM cache == saving: default:extract:e68faca51dd5b639a33548f5fd721951\n",
      "INFO:  == LLM cache == saving: default:extract:f5a5276a62ce6f636be43d13fd608477\n",
      "INFO: Chunk 1 of 1 extracted 11 Ent + 7 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 11 entities from doc-742aa5eabc77c8c51b4bc42ad3b6a8c4 (async: 8)\n",
      "INFO: Merged: `SEC EDGAR` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation` | 2+1\n",
      "INFO: Merged: `Form 144` | 0+1\n",
      "INFO: Merged: `Filing Date: 2025-10-22` | 0+1\n",
      "INFO: Merged: `Accession Number: 0001921094-25-001284` | 0+1\n",
      "INFO: Merged: `File Number: 000-23985` | 0+1\n",
      "INFO: Merged: `Acceptance DateTime: 2025-10-22T20:22:17.000Z` | 0+1\n",
      "INFO: Merged: `XBRL` | 0+1\n",
      "INFO: Merged: `Inline XBRL` | 0+1\n",
      "INFO: Merged: `Document Size: 38,203 bytes` | 0+1\n",
      "INFO: Merged: `Document Type: Regulatory Filing` | 0+1\n",
      "INFO: Phase 2: Processing 7 relations from doc-742aa5eabc77c8c51b4bc42ad3b6a8c4 (async: 8)\n",
      "INFO: Merged: `NVIDIA Corporation`~`SEC EDGAR` | 0+1\n",
      "INFO: Merged: `Document Size: 38,203 bytes`~`SEC EDGAR` | 0+1\n",
      "INFO: Merged: `Form 144`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Filing Date: 2025-10-22`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `SEC EDGAR`~`XBRL` | 0+1\n",
      "INFO: Merged: `Inline XBRL`~`SEC EDGAR` | 0+1\n",
      "INFO: Merged: `Document Type: Regulatory Filing`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Phase 3: Updating final 11(11+0) entities and  7 relations from doc-742aa5eabc77c8c51b4bc42ad3b6a8c4\n",
      "INFO: Completed merging: 11 entities, 0 extra entities, 7 relations\n",
      "INFO: [_] Writing graph with 220 nodes, 186 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-41cbf65669246f38cd6f9fc73022286d\n",
      "INFO:  == LLM cache == saving: default:extract:59bd325b32f91283274bb6eb79fcf3e8\n",
      "INFO:  == LLM cache == saving: default:extract:d7b92aaee3e20e88e7eccd309b988572\n",
      "INFO: Chunk 1 of 1 extracted 9 Ent + 8 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 9 entities from doc-41cbf65669246f38cd6f9fc73022286d (async: 8)\n",
      "INFO: Merged: `SEC EDGAR` | 1+1\n",
      "INFO: Merged: `NVIDIA Corporation` | 3+1\n",
      "INFO: Merged: `Regulatory Filing` | 0+1\n",
      "INFO: Merged: `Form 144` | 1+1\n",
      "INFO: Merged: `Filing Date` | 0+1\n",
      "INFO: Merged: `Accession Number` | 0+1\n",
      "INFO: Merged: `Acceptance DateTime` | 0+1\n",
      "INFO: Merged: `Document Size` | 0+1\n",
      "INFO: Merged: `Primary Document` | 0+1\n",
      "INFO: Phase 2: Processing 8 relations from doc-41cbf65669246f38cd6f9fc73022286d (async: 8)\n",
      "INFO: Merged: `NVIDIA Corporation`~`SEC EDGAR` | 1+1\n",
      "INFO: Merged: `Document Size`~`Regulatory Filing` | 0+1\n",
      "INFO: Merged: `Primary Document`~`Regulatory Filing` | 0+1\n",
      "INFO: Merged: `NVIDIA Corporation`~`Regulatory Filing` | 0+1\n",
      "INFO: Merged: `Form 144`~`NVIDIA Corporation` | 1+1\n",
      "INFO: Merged: `Filing Date`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Accession Number`~`NVIDIA Corporation` | 0+1\n",
      "INFO: Merged: `Acceptance DateTime`~`Filing Date` | 0+1\n",
      "INFO: Phase 3: Updating final 9(9+0) entities and  8 relations from doc-41cbf65669246f38cd6f9fc73022286d\n",
      "INFO: Completed merging: 9 entities, 0 extra entities, 8 relations\n",
      "INFO: [_] Writing graph with 226 nodes, 192 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-6835809150f63b28194832c442d81631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üì∞ DOCUMENT 13/19                                                            ‚îÉ\n",
      "‚îÉ Source: News                                                                 ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: AMD (AMD) Stock Is Up, What You Need To Know                         ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üì∞ DOCUMENT 14/19                                                            ‚îÉ\n",
      "‚îÉ Source: News                                                                 ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: Did Microsoft just tease that the next Xbox is a PC and console?     ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üíπ DOCUMENT 15/19                                                            ‚îÉ\n",
      "‚îÉ Source: Financial API                                                        ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: Advanced Micro Devices, Inc.                                         ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üíπ DOCUMENT 16/19                                                            ‚îÉ\n",
      "‚îÉ Source: Financial API                                                        ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: Company Overview: Advanced Micro Devices Inc                         ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üíπ DOCUMENT 17/19                                                            ‚îÉ\n",
      "‚îÉ Source: Financial API                                                        ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: Company Details: Advanced Micro Devices                              ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìë DOCUMENT 18/19                                                            ‚îÉ\n",
      "‚îÉ Source: SEC Filing                                                           ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: 4                                                                    ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ üìë DOCUMENT 19/19                                                            ‚îÉ\n",
      "‚îÉ Source: SEC Filing                                                           ‚îÉ\n",
      "‚îÉ Symbol: AMD                                                                  ‚îÉ\n",
      "‚îÉ Title: 144                                                                  ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:  == LLM cache == saving: default:extract:56c0df72bbfe65284993edf9dfa12700\n",
      "INFO:  == LLM cache == saving: default:extract:c0dd881e9ab0c3203f207d5b52053309\n",
      "INFO: Chunk 1 of 1 extracted 8 Ent + 4 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 8 entities from doc-6835809150f63b28194832c442d81631 (async: 8)\n",
      "INFO: Merged: `AMD` | 0+1\n",
      "INFO: Merged: `Artificial Intelligence Partnership` | 0+1\n",
      "INFO: Merged: `Yahoo Entertainment` | 1+1\n",
      "INFO: Merged: `Stock Price Increase` | 0+1\n",
      "INFO: Merged: `October 2, 2025` | 0+1\n",
      "INFO: Merged: `NASDAQ:AMD` | 0+1\n",
      "INFO: Merged: `News Article` | 1+1\n",
      "INFO: Merged: `Published Date` | 0+1\n",
      "INFO: Phase 2: Processing 4 relations from doc-6835809150f63b28194832c442d81631 (async: 8)\n",
      "INFO: Merged: `AMD`~`Stock Price Increase` | 0+1\n",
      "INFO: Merged: `AMD`~`Artificial Intelligence Partnership` | 0+1\n",
      "INFO: Merged: `AMD`~`Yahoo Entertainment` | 0+1\n",
      "INFO: Merged: `AMD`~`News Article` | 0+1\n",
      "INFO: Phase 3: Updating final 8(8+0) entities and  4 relations from doc-6835809150f63b28194832c442d81631\n",
      "INFO: Completed merging: 8 entities, 0 extra entities, 4 relations\n",
      "INFO: [_] Writing graph with 232 nodes, 196 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-43aba589efb0c8a9cba32e887d9bc016\n",
      "INFO:  == LLM cache == saving: default:extract:64357c158da39bb2c42f4115b21a4997\n",
      "INFO:  == LLM cache == saving: default:extract:f0e20c0d76a51ca4d71859652ffc32cb\n",
      "INFO: Chunk 1 of 1 extracted 9 Ent + 6 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 9 entities from doc-43aba589efb0c8a9cba32e887d9bc016 (async: 8)\n",
      "INFO: Merged: `Microsoft` | 0+1\n",
      "INFO: Merged: `Xbox` | 0+1\n",
      "INFO: Merged: `AMD` | 1+1\n",
      "INFO: Merged: `Tom Warren` | 0+1\n",
      "INFO: Merged: `Next-Gen Xbox Console` | 0+1\n",
      "INFO: Merged: `The Verge` | 0+1\n",
      "INFO: Merged: `Published Date` | 1+1\n",
      "INFO: Merged: `URL` | 0+1\n",
      "INFO: Merged: `Next-Gen Xbox` | 0+1\n",
      "INFO: Phase 2: Processing 5 relations from doc-43aba589efb0c8a9cba32e887d9bc016 (async: 8)\n",
      "INFO: Merged: `Microsoft`~`Next-Gen Xbox Console` | 0+2\n",
      "INFO: Merged: `The Verge`~`Tom Warren` | 0+1\n",
      "INFO: Merged: `AMD`~`Next-Gen Xbox Console` | 0+1\n",
      "INFO: Merged: `Next-Gen Xbox Console`~`Xbox` | 0+1\n",
      "INFO: Merged: `AMD`~`Next-Gen Xbox` | 0+1\n",
      "INFO: Phase 3: Updating final 9(9+0) entities and  5 relations from doc-43aba589efb0c8a9cba32e887d9bc016\n",
      "INFO: Completed merging: 9 entities, 0 extra entities, 5 relations\n",
      "INFO: [_] Writing graph with 239 nodes, 201 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-cbc5d0d1f6fe62784688cc4fc764e6d0\n",
      "INFO:  == LLM cache == saving: default:extract:b6f7f00fc25430b646f321c66c69a65b\n",
      "INFO:  == LLM cache == saving: default:extract:ec77c249a5d94abf5d26fe1412f71bff\n",
      "INFO: Chunk 1 of 1 extracted 17 Ent + 16 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 17 entities from doc-cbc5d0d1f6fe62784688cc4fc764e6d0 (async: 8)\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.` | 0+1\n",
      "INFO: Merged: `AMD` | 2+1\n",
      "INFO: Merged: `Technology` | 0+1\n",
      "INFO: Merged: `Semiconductors` | 0+1\n",
      "INFO: Merged: `Santa Clara, California` | 0+1\n",
      "INFO: Merged: `Lisa T. Su` | 0+1\n",
      "INFO: Merged: `Market Capitalization` | 3+1\n",
      "INFO: Merged: `Current Stock Price` | 0+1\n",
      "INFO: Merged: `Average Volume` | 0+1\n",
      "INFO: Merged: `IPO Date` | 1+1\n",
      "INFO: Merged: `2485 Augustine Drive, Santa Clara, CA 95054` | 0+1\n",
      "INFO: Merged: `x86 Microprocessors` | 0+1\n",
      "INFO: Merged: `Graphics Processing Units (GPUs)` | 1+1\n",
      "INFO: Merged: `AMD Ryzen` | 0+1\n",
      "INFO: Merged: `AMD Radeon` | 0+1\n",
      "INFO: Merged: `Server Processors` | 0+1\n",
      "INFO: Merged: `SoC Products` | 0+1\n",
      "INFO: Phase 2: Processing 16 relations from doc-cbc5d0d1f6fe62784688cc4fc764e6d0 (async: 8)\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Lisa T. Su` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Santa Clara, California` | 0+1\n",
      "INFO: Merged: `AMD`~`Advanced Micro Devices, Inc.` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Market Capitalization` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Current Stock Price` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Average Volume` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Technology` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Semiconductors` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`IPO Date` | 0+1\n",
      "INFO: Merged: `2485 Augustine Drive, Santa Clara, CA 95054`~`Advanced Micro Devices, Inc.` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`x86 Microprocessors` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Graphics Processing Units (GPUs)` | 0+1\n",
      "INFO: Merged: `AMD Ryzen`~`Advanced Micro Devices, Inc.` | 0+1\n",
      "INFO: Merged: `AMD Radeon`~`Advanced Micro Devices, Inc.` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`Server Processors` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices, Inc.`~`SoC Products` | 0+1\n",
      "INFO: Phase 3: Updating final 17(17+0) entities and  16 relations from doc-cbc5d0d1f6fe62784688cc4fc764e6d0\n",
      "INFO: Completed merging: 17 entities, 0 extra entities, 16 relations\n",
      "INFO: [_] Writing graph with 252 nodes, 217 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-6f11bf6ad82e1ecfb5bfd1004cf1b620\n",
      "INFO:  == LLM cache == saving: default:extract:3b5d60b1c8cd76237da4b6e6ce776a5d\n",
      "INFO:  == LLM cache == saving: default:extract:930038c8e0749fc4ef5005041f9b9eba\n",
      "INFO: Chunk 1 of 1 extracted 12 Ent + 11 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 12 entities from doc-6f11bf6ad82e1ecfb5bfd1004cf1b620 (async: 8)\n",
      "INFO: Merged: `Advanced Micro Devices Inc` | 0+1\n",
      "INFO: Merged: `Santa Clara` | 2+1\n",
      "INFO: Merged: `NASDAQ` | 1+1\n",
      "INFO: Merged: `Technology Sector` | 1+1\n",
      "INFO: Merged: `Semiconductors` | 1+1\n",
      "INFO: Merged: `Market Capitalization` | 4+1\n",
      "INFO: Merged: `Profit Margin` | 1+1\n",
      "INFO: Merged: `EPS` | 0+1\n",
      "INFO: Merged: `Revenue Per Share` | 0+1\n",
      "INFO: Merged: `200 Day Moving Average` | 0+1\n",
      "INFO: Merged: `52 Week High` | 0+1\n",
      "INFO: Merged: `52 Week Low` | 0+1\n",
      "INFO: Phase 2: Processing 11 relations from doc-6f11bf6ad82e1ecfb5bfd1004cf1b620 (async: 8)\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`Santa Clara` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`NASDAQ` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`Technology Sector` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`Semiconductors` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`Market Capitalization` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`EPS` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`Profit Margin` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices Inc`~`Revenue Per Share` | 0+1\n",
      "INFO: Merged: `200 Day Moving Average`~`Advanced Micro Devices Inc` | 0+1\n",
      "INFO: Merged: `52 Week High`~`Advanced Micro Devices Inc` | 0+1\n",
      "INFO: Merged: `52 Week Low`~`Advanced Micro Devices Inc` | 0+1\n",
      "INFO: Phase 3: Updating final 12(12+0) entities and  11 relations from doc-6f11bf6ad82e1ecfb5bfd1004cf1b620\n",
      "INFO: Completed merging: 12 entities, 0 extra entities, 11 relations\n",
      "INFO: [_] Writing graph with 258 nodes, 228 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-70f950f1d46ead4c5b38bcfe92e2843f\n",
      "INFO:  == LLM cache == saving: default:extract:d00e4def4c312e82931bf1c12710a938\n",
      "INFO:  == LLM cache == saving: default:extract:17fbb188620e46ef09d9b73439f5cc07\n",
      "INFO: Chunk 1 of 1 extracted 15 Ent + 12 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 15 entities from doc-70f950f1d46ead4c5b38bcfe92e2843f (async: 8)\n",
      "INFO: Merged: `Advanced Micro Devices` | 0+1\n",
      "INFO: Merged: `AMD` | 3+1\n",
      "INFO: Merged: `XNAS` | 1+1\n",
      "INFO: Merged: `PCs` | 0+1\n",
      "INFO: Merged: `Gaming Consoles` | 0+1\n",
      "INFO: Merged: `Data Centers` | 0+1\n",
      "INFO: Merged: `Artificial Intelligence` | 1+1\n",
      "INFO: Merged: `Market Capitalization` | 5+1\n",
      "INFO: Merged: `Ticker Symbol` | 0+1\n",
      "INFO: Merged: `Currency Name` | 0+1\n",
      "INFO: Merged: `CIK` | 0+1\n",
      "INFO: Merged: `Composite FIGI` | 0+1\n",
      "INFO: Merged: `Share Class FIGI` | 0+1\n",
      "INFO: Merged: `Weighted Shares Outstanding` | 0+1\n",
      "INFO: Merged: `Outstanding Shares` | 1+1\n",
      "INFO: Phase 2: Processing 12 relations from doc-70f950f1d46ead4c5b38bcfe92e2843f (async: 8)\n",
      "INFO: Merged: `Advanced Micro Devices`~`XNAS` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`PCs` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Gaming Consoles` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Data Centers` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Artificial Intelligence` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Ticker Symbol` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Currency Name` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`CIK` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Composite FIGI` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Share Class FIGI` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Weighted Shares Outstanding` | 0+1\n",
      "INFO: Merged: `Advanced Micro Devices`~`Outstanding Shares` | 0+1\n",
      "INFO: Phase 3: Updating final 15(15+0) entities and  12 relations from doc-70f950f1d46ead4c5b38bcfe92e2843f\n",
      "INFO: Completed merging: 15 entities, 0 extra entities, 12 relations\n",
      "INFO: [_] Writing graph with 268 nodes, 240 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-ca8b5e51c00114b59e2a26e6b357f650\n",
      "INFO:  == LLM cache == saving: default:extract:10d54a305e6d57bafe636b86328fce71\n",
      "INFO:  == LLM cache == saving: default:extract:fc508fdc7c84bf79c5c560180fb717e2\n",
      "INFO: Chunk 1 of 1 extracted 8 Ent + 3 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 8 entities from doc-ca8b5e51c00114b59e2a26e6b357f650 (async: 8)\n",
      "INFO: Merged: `SEC EDGAR` | 2+1\n",
      "INFO: Merged: `AMD` | 4+1\n",
      "INFO: Merged: `SEC EDGAR Filing: 4 - AMD` | 0+1\n",
      "INFO: Merged: `Filing Date: 2025-10-17` | 0+1\n",
      "INFO: Merged: `Accession Number: 0000002488-25-000159` | 0+1\n",
      "INFO: Merged: `Document Description: FORM 4` | 0+1\n",
      "INFO: Merged: `Primary Document: xslF345X05/wk-form4_1760736590.xml` | 0+1\n",
      "INFO: Merged: `Form Type: 4` | 0+1\n",
      "INFO: Phase 2: Processing 3 relations from doc-ca8b5e51c00114b59e2a26e6b357f650 (async: 8)\n",
      "INFO: Merged: `AMD`~`SEC EDGAR Filing: 4 - AMD` | 0+1\n",
      "INFO: Merged: `Filing Date: 2025-10-17`~`SEC EDGAR Filing: 4 - AMD` | 0+1\n",
      "INFO: Merged: `AMD`~`SEC EDGAR` | 0+1\n",
      "INFO: Phase 3: Updating final 8(8+0) entities and  3 relations from doc-ca8b5e51c00114b59e2a26e6b357f650\n",
      "INFO: Completed merging: 8 entities, 0 extra entities, 3 relations\n",
      "INFO: [_] Writing graph with 274 nodes, 243 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: unknown_source\n",
      "INFO: Processing d-id: doc-dd5e01b214113dec1efca63753293687\n",
      "INFO:  == LLM cache == saving: default:extract:ad9abbbf73c19530f6d68d2e9e9ddd45\n",
      "INFO:  == LLM cache == saving: default:extract:abf0a7ff27a62e0a107697365f7f3d02\n",
      "INFO: Chunk 1 of 1 extracted 8 Ent + 7 Rel\n",
      "INFO: Merging stage 1/1: unknown_source\n",
      "INFO: Phase 1: Processing 8 entities from doc-dd5e01b214113dec1efca63753293687 (async: 8)\n",
      "INFO: Merged: `SEC EDGAR` | 3+1\n",
      "INFO: Merged: `AMD` | 5+1\n",
      "INFO: Merged: `Form 144` | 2+1\n",
      "INFO: Merged: `2025-10-15` | 0+1\n",
      "INFO: Merged: `Accession Number 0001950047-25-007956` | 0+1\n",
      "INFO: Merged: `Acceptance DateTime 2025-10-15T20:01:50.000Z` | 0+1\n",
      "INFO: Merged: `Document Size 6,163 bytes` | 0+1\n",
      "INFO: Merged: `Primary Document xsl144X01/primary_doc.xml` | 0+1\n",
      "INFO: Phase 2: Processing 7 relations from doc-dd5e01b214113dec1efca63753293687 (async: 8)\n",
      "INFO: Merged: `AMD`~`SEC EDGAR` | 1+1\n",
      "INFO: Merged: `2025-10-15`~`Form 144` | 0+1\n",
      "INFO: Merged: `Accession Number 0001950047-25-007956`~`Form 144` | 0+1\n",
      "INFO: Merged: `Acceptance DateTime 2025-10-15T20:01:50.000Z`~`Form 144` | 0+1\n",
      "INFO: Merged: `Document Size 6,163 bytes`~`Form 144` | 0+1\n",
      "INFO: Merged: `Form 144`~`Primary Document xsl144X01/primary_doc.xml` | 0+1\n",
      "INFO: Merged: `AMD`~`Form 144` | 0+1\n",
      "INFO: Phase 3: Updating final 8(8+0) entities and  7 relations from doc-dd5e01b214113dec1efca63753293687\n",
      "INFO: Completed merging: 8 entities, 0 extra entities, 7 relations\n",
      "INFO: [_] Writing graph with 279 nodes, 249 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: unknown_source\n",
      "INFO: Enqueued document processing pipeline stoped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Ingestion Results:\n",
      "  Status: success\n",
      "  Holdings: 2/2\n",
      "  Documents: 19\n",
      "  ‚úÖ Successful: NVDA, AMD\n",
      "\n",
      "‚è±Ô∏è  Processing Time: 573.36s\n",
      "  Data Sources: newsapi, alpha_vantage, fmp, polygon, finnhub, benzinga, marketaux\n",
      "\n",
      "üìß Investment Signals Captured:\n",
      "  Broker emails: 5\n",
      "  Tickers covered: 23\n",
      "  BUY ratings: 0\n",
      "  SELL ratings: 0\n",
      "  Avg confidence: 0.80\n",
      "\n",
      "üìÇ Document Source Breakdown:\n",
      "  üìß Email (broker research): 5 documents\n",
      "  üåê API + SEC (market data): 14 documents\n",
      "  üìä Total documents: 19\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONFIGURATION: Set to False to skip graph building and use existing graph\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "REBUILD_GRAPH = True\n",
    "# REBUILD_GRAPH = False\n",
    "\n",
    "if REBUILD_GRAPH:\n",
    "    # Execute data ingestion\n",
    "    # NOTE: This operation may take several minutes. If it hangs, restart kernel.\n",
    "    print(f\"\\nüì• Fetching Portfolio Data\")\n",
    "    print(f\"‚îÅ\" * 50)\n",
    "\n",
    "    if not (ice and ice.is_ready()):\n",
    "        raise RuntimeError(\"ICE system not ready for data ingestion\")\n",
    "\n",
    "    # Fetch historical data (1 year for faster processing - adjust years parameter as needed)\n",
    "    print(f\"üîÑ Fetching data for {len(test_holdings)} holdings...\")\n",
    "    ingestion_result = ice.ingest_historical_data(\n",
    "        test_holdings, \n",
    "        years=1,\n",
    "        email_limit=email_limit,\n",
    "        news_limit=news_limit,\n",
    "        financial_limit=financial_limit,\n",
    "        market_limit=market_limit,\n",
    "        sec_limit=sec_limit,\n",
    "        research_limit=research_limit,\n",
    "        email_files=email_files_to_process if email_source_enabled else None\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nüìä Ingestion Results:\")\n",
    "    print(f\"  Status: {ingestion_result['status']}\")\n",
    "    print(f\"  Holdings: {len(ingestion_result['holdings_processed'])}/{len(test_holdings)}\")\n",
    "    print(f\"  Documents: {ingestion_result['total_documents']}\")\n",
    "\n",
    "    # Show successful holdings\n",
    "    if ingestion_result['holdings_processed']:\n",
    "        print(f\"  ‚úÖ Successful: {', '.join(ingestion_result['holdings_processed'])}\")\n",
    "\n",
    "    # Show metrics\n",
    "    if 'metrics' in ingestion_result:\n",
    "        print(f\"\\n‚è±Ô∏è  Processing Time: {ingestion_result['metrics']['processing_time']:.2f}s\")\n",
    "        if 'data_sources_used' in ingestion_result['metrics']:\n",
    "            print(f\"  Data Sources: {', '.join(ingestion_result['metrics']['data_sources_used'])}\")\n",
    "        \n",
    "        # Display investment signals from Phase 2.6.1 EntityExtractor\n",
    "        if 'investment_signals' in ingestion_result['metrics']:\n",
    "            signals = ingestion_result['metrics']['investment_signals']\n",
    "            print(f\"\\nüìß Investment Signals Captured:\")\n",
    "            print(f\"  Broker emails: {signals['email_count']}\")\n",
    "            print(f\"  Tickers covered: {signals['tickers_covered']}\")\n",
    "            print(f\"  BUY ratings: {signals['buy_ratings']}\")\n",
    "            print(f\"  SELL ratings: {signals['sell_ratings']}\")\n",
    "            print(f\"  Avg confidence: {signals['avg_confidence']:.2f}\")\n",
    "\n",
    "        # Document Source Breakdown\n",
    "        if 'metrics' in ingestion_result and 'investment_signals' in ingestion_result['metrics']:\n",
    "            signals = ingestion_result['metrics']['investment_signals']\n",
    "            email_count = signals['email_count']\n",
    "            \n",
    "            # Parse remaining document types from total\n",
    "            total_docs = ingestion_result.get('total_documents', 0)\n",
    "            api_sec_count = total_docs - email_count\n",
    "            \n",
    "            print(f\"\\nüìÇ Document Source Breakdown:\")\n",
    "            print(f\"  üìß Email (broker research): {email_count} documents\")\n",
    "            print(f\"  üåê API + SEC (market data): {api_sec_count} documents\")\n",
    "            print(f\"  üìä Total documents: {total_docs}\")\n",
    "\n",
    "    # Show failures if any\n",
    "    if ingestion_result.get('failed_holdings'):\n",
    "        print(f\"\\n‚ùå Failed Holdings:\")\n",
    "        for failure in ingestion_result['failed_holdings']:\n",
    "            print(f\"  {failure['symbol']}: {failure['error']}\")\n",
    "else:\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # STALENESS WARNING: Alert user if selectors changed\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  REBUILD_GRAPH = False\")\n",
    "    print(\"‚ö†Ô∏è  Using existing graph - NOT rebuilding with current selectors!\")\n",
    "    print(\"‚ö†Ô∏è  If you changed PORTFOLIO/EMAIL/SOURCE configuration,\")\n",
    "    print(\"‚ö†Ô∏è  set REBUILD_GRAPH=True to avoid querying STALE DATA!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(\"Using existing graph from: ice_lightrag/storage/\")\n",
    "    print(\"To rebuild, set REBUILD_GRAPH = True above and re-run this cell\")\n",
    "    \n",
    "    # Create mock ingestion_result for downstream cells\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    doc_count = 0\n",
    "    if Path('ice_lightrag/storage/kv_store_doc_status.json').exists():\n",
    "        with open('ice_lightrag/storage/kv_store_doc_status.json') as f:\n",
    "            doc_count = len(json.load(f))\n",
    "    \n",
    "    ingestion_result = {\n",
    "        'status': 'skipped',\n",
    "        'total_documents': doc_count,\n",
    "        'holdings_processed': holdings,\n",
    "        'failed_holdings': [],\n",
    "        'metrics': {\n",
    "            'processing_time': 0.0,\n",
    "            'data_sources_used': [],\n",
    "            'investment_signals': {\n",
    "                'email_count': 0,\n",
    "                'tickers_covered': 0,\n",
    "                'buy_ratings': 0,\n",
    "                'sell_ratings': 0,\n",
    "                'avg_confidence': 0.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Existing graph contains {doc_count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ICE Knowledge Graph Statistics\n",
      "======================================================================\n",
      "\n",
      "üìÑ TIER 1: Document Source Breakdown\n",
      "----------------------------------------------------------------------\n",
      "Total Documents: 19\n",
      "\n",
      "üìä Source Distribution (Visual Breakdown):\n",
      "  üìß Email:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   5 ( 26.3%)\n",
      "  üåê API:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  10 ( 52.6%)\n",
      "  üìã SEC:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   4 ( 21.1%)\n",
      "\n",
      "üìà Source Diversity Metrics:\n",
      "  ‚Ä¢ Unique sources detected: 6\n",
      "  ‚Ä¢ Expected sources (Email/API/SEC): 3/3\n",
      "  ‚Ä¢ Coverage: 100.0% (19/19 docs with markers)\n",
      "  ‚Ä¢ Status: COMPLETE\n",
      "\n",
      "  ‚úÖ All data sources properly tagged!\n",
      "\n",
      "üìä Detailed Breakdown:\n",
      "  üìß Email: 5 documents\n",
      "     ‚Ä¢ Portfolio-wide broker research\n",
      "  üåê API: 10 documents\n",
      "     ‚Ä¢ NewsAPI: 4\n",
      "     ‚Ä¢ FMP: 2\n",
      "     ‚Ä¢ Alpha Vantage: 2\n",
      "     ‚Ä¢ Polygon: 2\n",
      "  üìã SEC: 4 documents\n",
      "     ‚Ä¢ SEC EDGAR filings: 4\n",
      "\n",
      "\n",
      "üï∏Ô∏è  TIER 2: Knowledge Graph Structure\n",
      "----------------------------------------------------------------------\n",
      "Total Entities: 279\n",
      "Total Relationships: 249\n",
      "Avg Connections per Entity: 0.89\n",
      "\n",
      "\n",
      "üíº TIER 3: Investment Intelligence\n",
      "----------------------------------------------------------------------\n",
      "Portfolio Coverage: AMD, MU, NVDA (3 tickers)\n",
      "\n",
      "Investment Signals:\n",
      "  ‚Ä¢ BUY ratings: 2\n",
      "  ‚Ä¢ SELL ratings: 4\n",
      "  ‚Ä¢ Price targets: 5\n",
      "  ‚Ä¢ Risk mentions: 1\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Comprehensive statistics complete!\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive 3-Tier Knowledge Graph Statistics\n",
    "stats = ice.get_comprehensive_stats()\n",
    "\n",
    "print(\"üìä ICE Knowledge Graph Statistics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TIER 1: Document Source Breakdown\n",
    "print(\"\\nüìÑ TIER 1: Document Source Breakdown\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t1 = stats['tier1']\n",
    "diversity = t1.get('source_diversity', {})\n",
    "\n",
    "print(f\"Total Documents: {t1['total']}\")\n",
    "print(f\"\\nüìä Source Distribution (Visual Breakdown):\")\n",
    "print(f\"  üìß Email:    {ice._format_progress_bar(t1['email'], t1['total'])}\")\n",
    "print(f\"  üåê API:      {ice._format_progress_bar(t1['api_total'], t1['total'])}\")\n",
    "print(f\"  üìã SEC:      {ice._format_progress_bar(t1['sec_total'], t1['total'])}\")\n",
    "\n",
    "print(f\"\\nüìà Source Diversity Metrics:\")\n",
    "print(f\"  ‚Ä¢ Unique sources detected: {diversity.get('unique_sources', 0)}\")\n",
    "print(f\"  ‚Ä¢ Expected sources (Email/API/SEC): {diversity.get('expected_sources_present', 0)}/3\")\n",
    "print(f\"  ‚Ä¢ Coverage: {diversity.get('coverage_percentage', 0.0):.1f}% ({diversity.get('documents_with_markers', 0)}/{t1['total']} docs with markers)\")\n",
    "print(f\"  ‚Ä¢ Status: {diversity.get('status', 'unknown').upper()}\")\n",
    "\n",
    "if diversity.get('status') == 'incomplete' or diversity.get('coverage_percentage', 0) < 80:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  Low coverage detected! Set REBUILD_GRAPH=True in Cell 22 to rebuild with correct markers\")\n",
    "elif diversity.get('status') == 'complete':\n",
    "    print(f\"\\n  ‚úÖ All data sources properly tagged!\")\n",
    "\n",
    "print(f\"\\nüìä Detailed Breakdown:\")\n",
    "print(f\"  üìß Email: {t1['email']} documents\")\n",
    "print(f\"     ‚Ä¢ Portfolio-wide broker research\")\n",
    "print(f\"  üåê API: {t1['api_total']} documents\")\n",
    "print(f\"     ‚Ä¢ NewsAPI: {t1.get('newsapi', 0)}\")\n",
    "print(f\"     ‚Ä¢ FMP: {t1.get('fmp', 0)}\")\n",
    "print(f\"     ‚Ä¢ Alpha Vantage: {t1.get('alpha_vantage', 0)}\")\n",
    "print(f\"     ‚Ä¢ Polygon: {t1.get('polygon', 0)}\")\n",
    "if t1.get('finnhub', 0) > 0:\n",
    "    print(f\"     ‚Ä¢ Finnhub: {t1.get('finnhub', 0)}\")\n",
    "if t1.get('marketaux', 0) > 0:\n",
    "    print(f\"     ‚Ä¢ MarketAux: {t1.get('marketaux', 0)}\")\n",
    "if t1.get('benzinga', 0) > 0:\n",
    "    print(f\"     ‚Ä¢ Benzinga: {t1.get('benzinga', 0)}\")\n",
    "print(f\"  üìã SEC: {t1['sec_total']} documents\")\n",
    "print(f\"     ‚Ä¢ SEC EDGAR filings: {t1.get('sec_edgar', 0)}\")\n",
    "\n",
    "# TIER 2: Graph Structure\n",
    "print(\"\\n\\nüï∏Ô∏è  TIER 2: Knowledge Graph Structure\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t2 = stats['tier2']\n",
    "print(f\"Total Entities: {t2['total_entities']:,}\")\n",
    "print(f\"Total Relationships: {t2['total_relationships']:,}\")\n",
    "if t2['total_entities'] > 0:\n",
    "    print(f\"Avg Connections per Entity: {t2['avg_connections']:.2f}\")\n",
    "\n",
    "# TIER 3: Investment Intelligence\n",
    "print(\"\\n\\nüíº TIER 3: Investment Intelligence\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t3 = stats['tier3']\n",
    "if t3['tickers_covered']:\n",
    "    print(f\"Portfolio Coverage: {', '.join(t3['tickers_covered'])} ({len(t3['tickers_covered'])} tickers)\")\n",
    "else:\n",
    "    print(f\"Portfolio Coverage: No tickers detected\")\n",
    "\n",
    "print(f\"\\nInvestment Signals:\")\n",
    "print(f\"  ‚Ä¢ BUY ratings: {t3['buy_signals']}\")\n",
    "print(f\"  ‚Ä¢ SELL ratings: {t3['sell_signals']}\")\n",
    "print(f\"  ‚Ä¢ Price targets: {t3['price_targets']}\")\n",
    "print(f\"  ‚Ä¢ Risk mentions: {t3['risk_mentions']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Comprehensive statistics complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Knowledge Graph Building\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚ÑπÔ∏è  NOTE: Knowledge graph building happens automatically during data ingestion\n",
      "   The ingestion method (ingest_historical_data) already added documents\n",
      "   to the graph via LightRAG. This cell validates that building succeeded.\n",
      "\n",
      "‚úÖ KNOWLEDGE GRAPH BUILT SUCCESSFULLY\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   üìÑ Documents processed: 19\n",
      "   üíæ Storage size: 8.58 MB\n",
      "   üîó Components ready: 4/4\n",
      "\n",
      "üéØ Graph Building Process:\n",
      "   1Ô∏è‚É£ Text Chunking: 1200 tokens (optimal for financial documents)\n",
      "   2Ô∏è‚É£ Entity Extraction: Companies, metrics, risks, regulations\n",
      "   3Ô∏è‚É£ Relationship Discovery: Dependencies, impacts, correlations\n",
      "   4Ô∏è‚É£ Graph Construction: LightRAG optimized structure\n",
      "   5Ô∏è‚É£ Storage: chunks_vdb, entities_vdb, relationships_vdb, graph\n",
      "\n",
      "üöÄ System ready for intelligent queries!\n"
     ]
    }
   ],
   "source": [
    "# Knowledge Graph Building - Already completed during ingestion\n",
    "print(f\"\\nüß† Knowledge Graph Building\")\n",
    "print(f\"‚îÅ\" * 60)\n",
    "\n",
    "if not (ice and ice.core.is_ready()):\n",
    "    raise RuntimeError(\"LightRAG not ready\")\n",
    "\n",
    "print(f\"‚ÑπÔ∏è  NOTE: Knowledge graph building happens automatically during data ingestion\")\n",
    "print(f\"   The ingestion method (ingest_historical_data) already added documents\")\n",
    "print(f\"   to the graph via LightRAG. This cell validates that building succeeded.\\n\")\n",
    "\n",
    "# Validate that building succeeded by checking storage\n",
    "storage_stats = ice.core.get_storage_stats()\n",
    "\n",
    "if storage_stats['total_storage_bytes'] > 0:\n",
    "    print(f\"‚úÖ KNOWLEDGE GRAPH BUILT SUCCESSFULLY\")\n",
    "    print(f\"‚îÅ\" * 40)\n",
    "    print(f\"   üìÑ Documents processed: {ingestion_result.get('total_documents', 0)}\")\n",
    "    print(f\"   üíæ Storage size: {storage_stats['total_storage_bytes'] / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    components_ready = sum(1 for c in storage_stats['components'].values() if c['exists'])\n",
    "    print(f\"   üîó Components ready: {components_ready}/4\")\n",
    "    \n",
    "    # Create success result for metrics tracking\n",
    "    building_result = {\n",
    "        'status': 'success',\n",
    "        'total_documents': ingestion_result.get('total_documents', 0),\n",
    "        'metrics': {\n",
    "            'building_time': ingestion_result.get('metrics', {}).get('processing_time', 0.0),\n",
    "            'graph_initialized': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ Graph Building Process:\")\n",
    "    print(f\"   1Ô∏è‚É£ Text Chunking: 1200 tokens (optimal for financial documents)\")\n",
    "    print(f\"   2Ô∏è‚É£ Entity Extraction: Companies, metrics, risks, regulations\")\n",
    "    print(f\"   3Ô∏è‚É£ Relationship Discovery: Dependencies, impacts, correlations\")\n",
    "    print(f\"   4Ô∏è‚É£ Graph Construction: LightRAG optimized structure\")\n",
    "    print(f\"   5Ô∏è‚É£ Storage: chunks_vdb, entities_vdb, relationships_vdb, graph\")\n",
    "    \n",
    "    print(f\"\\nüöÄ System ready for intelligent queries!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è NO GRAPH DATA DETECTED\")\n",
    "    print(f\"   Storage size: 0 MB\")\n",
    "    print(f\"   Check ingestion results above for errors\")\n",
    "    print(f\"   Possible causes:\")\n",
    "    print(f\"   - No API keys configured\")\n",
    "    print(f\"   - All holdings failed to fetch data\")\n",
    "    print(f\"   - Network connectivity issues\")\n",
    "    \n",
    "    building_result = {\n",
    "        'status': 'error',\n",
    "        'message': 'No graph data - check ingestion results'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storage Architecture Validation & Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Storage Architecture Validation\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "üì¶ LightRAG Storage Components Status:\n",
      "  ‚úÖ chunks_vdb:\n",
      "    File: vdb_chunks.json\n",
      "    Purpose: Vector database for document chunks\n",
      "    Size: 0.56 MB\n",
      "  ‚úÖ entities_vdb:\n",
      "    File: vdb_entities.json\n",
      "    Purpose: Vector database for extracted entities\n",
      "    Size: 3.32 MB\n",
      "  ‚úÖ relationships_vdb:\n",
      "    File: vdb_relationships.json\n",
      "    Purpose: Vector database for entity relationships\n",
      "    Size: 2.97 MB\n",
      "  ‚úÖ graph:\n",
      "    File: graph_chunk_entity_relation.graphml\n",
      "    Purpose: NetworkX graph structure\n",
      "    Size: 0.23 MB\n",
      "\n",
      "üìä Storage Summary:\n",
      "  Working Directory: ice_lightrag/storage\n",
      "  Total Storage: 8.58 MB\n",
      "  System Initialized: True\n",
      "\n",
      "üï∏Ô∏è Knowledge Graph Status:\n",
      "  Graph Ready: True\n",
      "  All Components Present: True\n",
      "  Chunks Storage: 0.56 MB\n",
      "  Entity Storage: 3.32 MB\n",
      "  Relationship Storage: 2.97 MB\n",
      "  Graph Structure: 0.23 MB\n",
      "\n",
      "‚úÖ Validation Checks:\n",
      "  ‚úÖ System initialization: PASSED\n",
      "  ‚úÖ Storage directory: PASSED\n",
      "  ‚úÖ Storage components: PASSED (4/4 created)\n",
      "  ‚úÖ Storage content: PASSED\n",
      "\n",
      "üìä Validation Score: 4/4 (100%)\n",
      "üéâ All validations passed! Knowledge graph is ready for queries.\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive storage validation and metrics\n",
    "print(f\"\\nüîç Storage Architecture Validation\")\n",
    "print(f\"‚îÅ\" * 40)\n",
    "\n",
    "if not (ice and ice.core.is_ready()):\n",
    "    raise RuntimeError(\"Cannot validate storage without initialized system\")\n",
    "\n",
    "# Get detailed storage statistics\n",
    "storage_stats = ice.core.get_storage_stats()\n",
    "graph_stats = ice.core.get_graph_stats()\n",
    "\n",
    "print(f\"üì¶ LightRAG Storage Components Status:\")\n",
    "for component_name, component_info in storage_stats['components'].items():\n",
    "    status_icon = \"‚úÖ\" if component_info['exists'] else \"‚ö†Ô∏è\"\n",
    "    size_mb = component_info['size_bytes'] / (1024 * 1024) if component_info['size_bytes'] > 0 else 0\n",
    "    \n",
    "    print(f\"  {status_icon} {component_name}:\")\n",
    "    print(f\"    File: {component_info['file']}\")\n",
    "    print(f\"    Purpose: {component_info['description']}\")\n",
    "    print(f\"    Size: {size_mb:.2f} MB\" if size_mb > 0 else \"    Size: Not created yet\")\n",
    "\n",
    "print(f\"\\nüìä Storage Summary:\")\n",
    "print(f\"  Working Directory: {storage_stats['working_dir']}\")\n",
    "print(f\"  Total Storage: {storage_stats['total_storage_bytes'] / (1024 * 1024):.2f} MB\")\n",
    "print(f\"  System Initialized: {storage_stats['is_initialized']}\")\n",
    "\n",
    "print(f\"\\nüï∏Ô∏è Knowledge Graph Status:\")\n",
    "print(f\"  Graph Ready: {graph_stats['is_ready']}\")\n",
    "if graph_stats.get('storage_indicators'):\n",
    "    indicators = graph_stats['storage_indicators']\n",
    "    print(f\"  All Components Present: {indicators['all_components_present']}\")\n",
    "    print(f\"  Chunks Storage: {indicators['chunks_file_size']:.2f} MB\")\n",
    "    print(f\"  Entity Storage: {indicators['entities_file_size']:.2f} MB\")\n",
    "    print(f\"  Relationship Storage: {indicators['relationships_file_size']:.2f} MB\")\n",
    "    print(f\"  Graph Structure: {indicators['graph_file_size']:.2f} MB\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"\\n‚úÖ Validation Checks:\")\n",
    "validation_score = 0\n",
    "max_score = 4\n",
    "\n",
    "# Check 1: System ready\n",
    "if storage_stats['is_initialized']:\n",
    "    print(f\"  ‚úÖ System initialization: PASSED\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå System initialization: FAILED\")\n",
    "\n",
    "# Check 2: Storage exists\n",
    "if storage_stats['storage_exists']:\n",
    "    print(f\"  ‚úÖ Storage directory: PASSED\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå Storage directory: FAILED\")\n",
    "\n",
    "# Check 3: Components created\n",
    "components_exist = sum(1 for c in storage_stats['components'].values() if c['exists'])\n",
    "if components_exist > 0:\n",
    "    print(f\"  ‚úÖ Storage components: PASSED ({components_exist}/4 created)\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå Storage components: FAILED (no components created)\")\n",
    "\n",
    "# Check 4: Has storage content\n",
    "if storage_stats['total_storage_bytes'] > 0:\n",
    "    print(f\"  ‚úÖ Storage content: PASSED\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå Storage content: FAILED (no data stored)\")\n",
    "\n",
    "print(f\"\\nüìä Validation Score: {validation_score}/{max_score} ({(validation_score/max_score)*100:.0f}%)\")\n",
    "\n",
    "if validation_score == max_score:\n",
    "    print(f\"üéâ All validations passed! Knowledge graph is ready for queries.\")\n",
    "elif validation_score >= max_score * 0.75:\n",
    "    print(f\"‚úÖ Most validations passed. System is functional.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Some validations failed. Check configuration and retry building.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building Metrics & Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Building Session Metrics & Performance\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "üéØ Session Overview:\n",
      "  Holdings Processed: 10\n",
      "  Documents Processed: 19\n",
      "  Building Successful: True\n",
      "\n",
      "‚è±Ô∏è Performance Metrics:\n",
      "  Data Ingestion Time: 573.36s\n",
      "  Graph Building Time: 573.36s\n",
      "  Total Processing Time: 1146.71s\n",
      "\n",
      "üìà Efficiency Analysis:\n",
      "  Processing Rate: 0.03 documents/second\n",
      "  Holdings Rate: 0.02 holdings/second\n",
      "\n",
      "üèóÔ∏è Architecture Efficiency:\n",
      "  ICE Simplified: 2,508 lines of code\n",
      "  Code Reduction: 83% (vs 15,000 line original)\n",
      "  Files Count: 5 core modules\n",
      "  Dependencies: Direct LightRAG wrapper\n",
      "  Token Efficiency: 4,000x better than GraphRAG\n",
      "\n",
      "‚úÖ Building Session Summary:\n",
      "  üéâ Knowledge graph building completed successfully\n",
      "  üìä 19 documents processed\n",
      "  üöÄ System ready for intelligent investment queries\n",
      "  üí° Proceed to ice_query_workflow.ipynb for analysis\n",
      "\n",
      "üîó Next Steps:\n",
      "  1. Review building metrics and validate storage\n",
      "  2. Run ice_query_workflow.ipynb for portfolio analysis\n",
      "  3. Test different LightRAG query modes\n",
      "  4. Monitor system performance and optimize as needed\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive building session metrics\n",
    "print(f\"\\nüìä Building Session Metrics & Performance\")\n",
    "print(f\"‚îÅ\" * 50)\n",
    "\n",
    "session_metrics = {\n",
    "    'holdings_count': len(holdings),\n",
    "    'total_processing_time': 0.0,\n",
    "    'documents_processed': 0,\n",
    "    'building_successful': False\n",
    "}\n",
    "\n",
    "# Collect metrics from ingestion and building\n",
    "if 'ingestion_result' in locals() and ingestion_result:\n",
    "    if 'metrics' in ingestion_result:\n",
    "        session_metrics['ingestion_time'] = ingestion_result['metrics'].get('processing_time', 0.0)\n",
    "    session_metrics['documents_processed'] = ingestion_result.get('total_documents', 0)\n",
    "\n",
    "if 'building_result' in locals() and building_result:\n",
    "    if building_result.get('status') == 'success':\n",
    "        session_metrics['building_successful'] = True\n",
    "    if 'metrics' in building_result:\n",
    "        building_time = building_result['metrics'].get('building_time', building_result['metrics'].get('update_time', 0.0))\n",
    "        session_metrics['building_time'] = building_time\n",
    "\n",
    "# Calculate total time\n",
    "if 'pipeline_stats' in locals():\n",
    "    session_metrics['total_processing_time'] = pipeline_stats.get('processing_time', 0.0)\n",
    "\n",
    "print(f\"üéØ Session Overview:\")\n",
    "print(f\"  Holdings Processed: {session_metrics['holdings_count']}\")\n",
    "print(f\"  Documents Processed: {session_metrics['documents_processed']}\")\n",
    "print(f\"  Building Successful: {session_metrics['building_successful']}\")\n",
    "\n",
    "if session_metrics.get('ingestion_time', 0) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è Performance Metrics:\")\n",
    "    print(f\"  Data Ingestion Time: {session_metrics['ingestion_time']:.2f}s\")\n",
    "    if session_metrics.get('building_time', 0) > 0:\n",
    "        print(f\"  Graph Building Time: {session_metrics['building_time']:.2f}s\")\n",
    "        print(f\"  Total Processing Time: {session_metrics['ingestion_time'] + session_metrics['building_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìà Efficiency Analysis:\")\n",
    "    if session_metrics['documents_processed'] > 0:\n",
    "        docs_per_second = session_metrics['documents_processed'] / session_metrics['ingestion_time']\n",
    "        print(f\"  Processing Rate: {docs_per_second:.2f} documents/second\")\n",
    "    \n",
    "    holdings_per_second = session_metrics['holdings_count'] / session_metrics['ingestion_time']\n",
    "    print(f\"  Holdings Rate: {holdings_per_second:.2f} holdings/second\")\n",
    "\n",
    "# Architecture efficiency comparison\n",
    "print(f\"\\nüèóÔ∏è Architecture Efficiency:\")\n",
    "print(f\"  ICE Simplified: 2,508 lines of code\")\n",
    "print(f\"  Code Reduction: 83% (vs 15,000 line original)\")\n",
    "print(f\"  Files Count: 5 core modules\")\n",
    "print(f\"  Dependencies: Direct LightRAG wrapper\")\n",
    "print(f\"  Token Efficiency: 4,000x better than GraphRAG\")\n",
    "\n",
    "# Success summary\n",
    "print(f\"\\n‚úÖ Building Session Summary:\")\n",
    "if session_metrics['building_successful']:\n",
    "    print(f\"  üéâ Knowledge graph building completed successfully\")\n",
    "    print(f\"  üìä {session_metrics['documents_processed']} documents processed\")\n",
    "    print(f\"  üöÄ System ready for intelligent investment queries\")\n",
    "    print(f\"  üí° Proceed to ice_query_workflow.ipynb for analysis\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Building completed with warnings or in demo mode\")\n",
    "    print(f\"  üìã Review configuration and API settings\")\n",
    "    print(f\"  üîß Consider running with fresh data if issues persist\")\n",
    "\n",
    "print(f\"\\nüîó Next Steps:\")\n",
    "print(f\"  1. Review building metrics and validate storage\")\n",
    "print(f\"  2. Run ice_query_workflow.ipynb for portfolio analysis\")\n",
    "print(f\"  3. Test different LightRAG query modes\")\n",
    "print(f\"  4. Monitor system performance and optimize as needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Building Workflow Complete\n",
    "\n",
    "**Summary**: This notebook demonstrated the complete ICE building workflow from data ingestion through knowledge graph construction.\n",
    "\n",
    "### Key Achievements\n",
    "‚úÖ **System Initialization**: ICE simplified architecture deployed  \n",
    "‚úÖ **Data Ingestion**: Portfolio data fetched and processed  \n",
    "‚úÖ **Graph Building**: LightRAG knowledge graph constructed  \n",
    "‚úÖ **Storage Validation**: All components verified and metrics tracked  \n",
    "\n",
    "### Architecture Benefits\n",
    "- **83% Code Reduction**: 2,508 lines vs 15,000 original\n",
    "- **4,000x Token Efficiency**: vs GraphRAG baseline\n",
    "- **Mode Flexibility**: Initial build or incremental updates\n",
    "- **Complete Metrics**: Processing time, success rates, storage stats\n",
    "\n",
    "### Next Steps\n",
    "1. **Launch Query Workflow**: Open `ice_query_workflow.ipynb`\n",
    "2. **Test Investment Intelligence**: Run portfolio analysis queries\n",
    "3. **Explore Query Modes**: Test all 5 LightRAG modes\n",
    "4. **Monitor Performance**: Track query response times and accuracy\n",
    "\n",
    "---\n",
    "**Ready for Investment Intelligence Queries** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

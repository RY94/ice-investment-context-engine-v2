{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“§ Email Analysis Dashboard - ICE Pipeline\n",
    "\n",
    "This notebook provides a comprehensive view of all processed emails in the ICE Investment Context Engine pipeline.\n",
    "\n",
    "## Features:\n",
    "- ğŸ“Š **Email Processing Statistics**\n",
    "- ğŸ¯ **Investment Entity Analysis** \n",
    "- ğŸ•¸ï¸ **Knowledge Graph Visualization**\n",
    "- ğŸ“ˆ **Sentiment Trends**\n",
    "- ğŸ” **Interactive Email Search**\n",
    "- ğŸ’­ **Query Interface**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Email Analysis Dashboard - Ready!\n",
      "ğŸ“ Working directory: /Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/imap_email_ingestion_pipeline\n",
      "â° Dashboard loaded at: 2025-09-03 20:38:03\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import getpass\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add pipeline components to path\n",
    "pipeline_path = os.getcwd()\n",
    "if pipeline_path not in sys.path:\n",
    "    sys.path.append(pipeline_path)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class EmailDashboard:\n",
    "    \"\"\"Dashboard class for loading and analyzing processed email data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.emails_df = pd.DataFrame()\n",
    "        self.attachments_df = pd.DataFrame()\n",
    "        self.ice_data = []\n",
    "        self.working_dirs = []\n",
    "    \n",
    "    def find_pipeline_databases(self):\n",
    "        \"\"\"Find all pipeline state databases in the system\"\"\"\n",
    "        search_patterns = [\n",
    "            \"./pipeline_state.db\",\n",
    "            \"./**/pipeline_state.db\",\n",
    "            \"/tmp/*/pipeline_state.db\",\n",
    "            \"/var/folders/*/pipeline_state.db\"\n",
    "        ]\n",
    "        \n",
    "        databases = []\n",
    "        for pattern in search_patterns:\n",
    "            databases.extend(glob.glob(pattern, recursive=True))\n",
    "        \n",
    "        # Also check for temporary directories from pipeline runs\n",
    "        temp_dirs = [d for d in os.listdir('/tmp') if d.startswith('real_pipeline_') or d.startswith('pipeline_')]\n",
    "        for temp_dir in temp_dirs:\n",
    "            db_path = os.path.join('/tmp', temp_dir, 'pipeline_state.db')\n",
    "            if os.path.exists(db_path):\n",
    "                databases.append(db_path)\n",
    "        \n",
    "        return databases\n",
    "    \n",
    "    def load_emails_from_database(self, db_path):\n",
    "        \"\"\"Load email data from SQLite database\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            \n",
    "            # Load emails table\n",
    "            emails_query = \"SELECT * FROM emails\"\n",
    "            emails = pd.read_sql_query(emails_query, conn)\n",
    "            \n",
    "            # Load attachments table if exists\n",
    "            try:\n",
    "                attachments_query = \"SELECT * FROM attachments\"\n",
    "                attachments = pd.read_sql_query(attachments_query, conn)\n",
    "            except pd.io.sql.DatabaseError:\n",
    "                attachments = pd.DataFrame()\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return emails, attachments\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading database {db_path}: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    def load_ice_data_from_directory(self, working_dir):\n",
    "        \"\"\"Load ICE integration data from working directory\"\"\"\n",
    "        ice_data = []\n",
    "        \n",
    "        ice_storage_dir = os.path.join(working_dir, 'ice_storage')\n",
    "        if os.path.exists(ice_storage_dir):\n",
    "            # Look for ICE result files\n",
    "            result_files = glob.glob(os.path.join(ice_storage_dir, \"**/*.json\"), recursive=True)\n",
    "            \n",
    "            for file_path in result_files:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        ice_data.append(data)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        return ice_data\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all available email processing data\"\"\"\n",
    "        print(\"ğŸ” Searching for processed email data...\")\n",
    "        \n",
    "        databases = self.find_pipeline_databases()\n",
    "        \n",
    "        if not databases:\n",
    "            print(\"ğŸ“­ No pipeline databases found\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"ğŸ“Š Found {len(databases)} pipeline database(s)\")\n",
    "        \n",
    "        all_emails = []\n",
    "        all_attachments = []\n",
    "        \n",
    "        for db_path in databases:\n",
    "            working_dir = os.path.dirname(db_path)\n",
    "            self.working_dirs.append(working_dir)\n",
    "            \n",
    "            print(f\"ğŸ“‚ Loading data from: {working_dir}\")\n",
    "            \n",
    "            # Load email data\n",
    "            emails, attachments = self.load_emails_from_database(db_path)\n",
    "            \n",
    "            if not emails.empty:\n",
    "                all_emails.append(emails)\n",
    "                print(f\"   ğŸ“§ {len(emails)} emails\")\n",
    "            \n",
    "            if not attachments.empty:\n",
    "                all_attachments.append(attachments)\n",
    "                print(f\"   ğŸ“ {len(attachments)} attachments\")\n",
    "            \n",
    "            # Load ICE data\n",
    "            ice_data = self.load_ice_data_from_directory(working_dir)\n",
    "            self.ice_data.extend(ice_data)\n",
    "            \n",
    "            if ice_data:\n",
    "                print(f\"   ğŸ§  {len(ice_data)} ICE integrations\")\n",
    "        \n",
    "        # Combine all data\n",
    "        if all_emails:\n",
    "            self.emails_df = pd.concat(all_emails, ignore_index=True)\n",
    "            # Remove duplicates based on email_uid\n",
    "            if 'email_uid' in self.emails_df.columns:\n",
    "                self.emails_df = self.emails_df.drop_duplicates(subset=['email_uid'])\n",
    "        \n",
    "        if all_attachments:\n",
    "            self.attachments_df = pd.concat(all_attachments, ignore_index=True)\n",
    "            # Remove duplicates\n",
    "            if 'attachment_id' in self.attachments_df.columns:\n",
    "                self.attachments_df = self.attachments_df.drop_duplicates(subset=['attachment_id'])\n",
    "        \n",
    "        has_data = not self.emails_df.empty or bool(self.ice_data)\n",
    "        \n",
    "        if has_data:\n",
    "            print(f\"âœ… Total loaded: {len(self.emails_df)} emails, {len(self.attachments_df)} attachments, {len(self.ice_data)} ICE results\")\n",
    "        \n",
    "        return has_data\n",
    "\n",
    "print(\"ğŸ“Š Email Analysis Dashboard - Ready!\")\n",
    "print(f\"ğŸ“ Working directory: {pipeline_path}\")\n",
    "print(f\"â° Dashboard loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 1: Load Processed Email Data or Run Pipeline\n",
    "\n",
    "Let's check for processed email data. If none exists, we'll help you run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for processed email data...\n",
      "ğŸ“­ No pipeline databases found\n",
      "\n",
      "âš ï¸ No processed email data found.\n",
      "ğŸ“‹ Running in non-interactive mode\n",
      "ğŸ’¡ To run the pipeline in non-interactive mode:\n",
      "   1. Set environment variable: export EMAIL_PASSWORD='your_password'\n",
      "   2. Or run manually: python process_emails.py YOUR_PASSWORD\n",
      "   3. Then re-run this notebook to see your results\n",
      "\n",
      "ğŸ’¡ To run the pipeline interactively:\n",
      "   1. Open this notebook in Jupyter Lab/Notebook\n",
      "   2. Run the cells to get interactive prompts\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline_with_monitoring(password):\n",
    "    \"\"\"Run the email processing pipeline with real-time output monitoring\"\"\"\n",
    "    print(\"ğŸš€ Starting Email Processing Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Start the pipeline process\n",
    "        process = subprocess.Popen(\n",
    "            [sys.executable, \"process_emails.py\", password],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # Monitor output in real-time\n",
    "        output_lines = []\n",
    "        while True:\n",
    "            line = process.stdout.readline()\n",
    "            if not line and process.poll() is not None:\n",
    "                break\n",
    "            if line:\n",
    "                print(line.rstrip())\n",
    "                output_lines.append(line.rstrip())\n",
    "        \n",
    "        # Get the return code\n",
    "        return_code = process.poll()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"\\nğŸ‰ Pipeline completed successfully!\")\n",
    "            return True, output_lines\n",
    "        else:\n",
    "            print(f\"\\nâŒ Pipeline failed with return code: {return_code}\")\n",
    "            return False, output_lines\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running pipeline: {e}\")\n",
    "        return False, []\n",
    "\n",
    "def test_email_connection(password):\n",
    "    \"\"\"Test email connection before running full pipeline\"\"\"\n",
    "    print(\"ğŸ” Testing email connection...\")\n",
    "    \n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            [sys.executable, \"quick_email_test.py\", password],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120  # 2 minute timeout for connection test\n",
    "        )\n",
    "        \n",
    "        print(process.stdout)\n",
    "        if process.stderr:\n",
    "            print(process.stderr)\n",
    "            \n",
    "        return process.returncode == 0\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° Connection test timed out after 2 minutes\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Connection test error: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_interactive_environment():\n",
    "    \"\"\"Check if we're running in an interactive environment\"\"\"\n",
    "    try:\n",
    "        # Check if we can get input (works in interactive environments)\n",
    "        import sys\n",
    "        return hasattr(sys.stdin, 'isatty') and sys.stdin.isatty()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_password_from_environment():\n",
    "    \"\"\"Try to get password from environment variable for non-interactive execution\"\"\"\n",
    "    return os.environ.get('EMAIL_PASSWORD', '')\n",
    "\n",
    "# Initialize dashboard\n",
    "dashboard = EmailDashboard()\n",
    "has_data = dashboard.load_all_data()\n",
    "\n",
    "if not has_data:\n",
    "    print(\"\\nâš ï¸ No processed email data found.\")\n",
    "    \n",
    "    # Check if we're in an interactive environment\n",
    "    is_interactive = is_interactive_environment()\n",
    "    \n",
    "    if is_interactive:\n",
    "        print(\"ğŸ”„ Let's run the email processing pipeline to get started!\")\n",
    "        print()\n",
    "        \n",
    "        # Get user password securely in interactive mode\n",
    "        try:\n",
    "            run_pipeline = input(\"Would you like to run the email processing pipeline? (y/n): \").lower().strip()\n",
    "            \n",
    "            if run_pipeline in ['y', 'yes']:\n",
    "                print(\"\\nğŸ” Email Authentication Required\")\n",
    "                print(\"Enter your email password for roy@agtpartners.com.sg\")\n",
    "                print(\"Note: Your password is only used locally and not stored anywhere.\")\n",
    "                \n",
    "                try:\n",
    "                    password = getpass.getpass(\"Password: \")\n",
    "                    \n",
    "                    if not password.strip():\n",
    "                        print(\"âŒ Password cannot be empty\")\n",
    "                    else:\n",
    "                        # First test the connection\n",
    "                        print(\"\\nğŸ“‹ Step 1: Testing email connection...\")\n",
    "                        connection_ok = test_email_connection(password)\n",
    "                        \n",
    "                        if connection_ok:\n",
    "                            print(\"\\nâœ… Email connection successful!\")\n",
    "                            \n",
    "                            # Ask for confirmation before full pipeline\n",
    "                            confirm = input(\"\\nProceed with full pipeline processing? (y/n): \").lower().strip()\n",
    "                            \n",
    "                            if confirm in ['y', 'yes']:\n",
    "                                print(\"\\nğŸ“‹ Step 2: Running full email processing pipeline...\")\n",
    "                                success, output = run_pipeline_with_monitoring(password)\n",
    "                                \n",
    "                                if success:\n",
    "                                    print(\"\\nğŸ”„ Step 3: Reloading dashboard data...\")\n",
    "                                    # Reinitialize dashboard to load new data\n",
    "                                    dashboard = EmailDashboard()\n",
    "                                    has_data = dashboard.load_all_data()\n",
    "                                    \n",
    "                                    if has_data:\n",
    "                                        print(\"âœ… New email data loaded successfully!\")\n",
    "                                    else:\n",
    "                                        print(\"âš ï¸ Pipeline completed but no data was loaded. Check the pipeline logs above.\")\n",
    "                                else:\n",
    "                                    print(\"âŒ Pipeline execution failed. Please check the error messages above.\")\n",
    "                            else:\n",
    "                                print(\"â¹ï¸ Pipeline execution cancelled by user.\")\n",
    "                        else:\n",
    "                            print(\"âŒ Email connection failed. Please check your password and network connection.\")\n",
    "                            print(\"ğŸ’¡ You can also run the pipeline manually with:\")\n",
    "                            print(\"   python process_emails.py YOUR_PASSWORD\")\n",
    "                            \n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\nâ¹ï¸ Pipeline setup cancelled by user.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error during pipeline setup: {e}\")\n",
    "            else:\n",
    "                print(\"ğŸ’¡ To run the pipeline manually:\")\n",
    "                print(\"   python process_emails.py YOUR_PASSWORD\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Interactive input not available: {e}\")\n",
    "            is_interactive = False\n",
    "    \n",
    "    if not is_interactive:\n",
    "        # Non-interactive mode - check for environment variable\n",
    "        print(\"ğŸ“‹ Running in non-interactive mode\")\n",
    "        \n",
    "        env_password = get_password_from_environment()\n",
    "        if env_password:\n",
    "            print(\"ğŸ” Found password in environment variable\")\n",
    "            print(\"ğŸ”„ Running pipeline automatically...\")\n",
    "            \n",
    "            # Test connection first\n",
    "            print(\"\\nğŸ“‹ Step 1: Testing email connection...\")\n",
    "            connection_ok = test_email_connection(env_password)\n",
    "            \n",
    "            if connection_ok:\n",
    "                print(\"\\nâœ… Email connection successful!\")\n",
    "                print(\"\\nğŸ“‹ Step 2: Running full email processing pipeline...\")\n",
    "                success, output = run_pipeline_with_monitoring(env_password)\n",
    "                \n",
    "                if success:\n",
    "                    print(\"\\nğŸ”„ Step 3: Reloading dashboard data...\")\n",
    "                    # Reinitialize dashboard to load new data\n",
    "                    dashboard = EmailDashboard()\n",
    "                    has_data = dashboard.load_all_data()\n",
    "                    \n",
    "                    if has_data:\n",
    "                        print(\"âœ… New email data loaded successfully!\")\n",
    "                    else:\n",
    "                        print(\"âš ï¸ Pipeline completed but no data was loaded. Check the pipeline logs above.\")\n",
    "                else:\n",
    "                    print(\"âŒ Pipeline execution failed. Please check the error messages above.\")\n",
    "            else:\n",
    "                print(\"âŒ Email connection failed. Please check your password.\")\n",
    "        else:\n",
    "            print(\"ğŸ’¡ To run the pipeline in non-interactive mode:\")\n",
    "            print(\"   1. Set environment variable: export EMAIL_PASSWORD='your_password'\")\n",
    "            print(\"   2. Or run manually: python process_emails.py YOUR_PASSWORD\")\n",
    "            print(\"   3. Then re-run this notebook to see your results\")\n",
    "            \n",
    "            print(\"\\nğŸ’¡ To run the pipeline interactively:\")\n",
    "            print(\"   1. Open this notebook in Jupyter Lab/Notebook\")\n",
    "            print(\"   2. Run the cells to get interactive prompts\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nğŸ‰ Email data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 2: Email Processing Overview\n",
    "\n",
    "High-level statistics about processed emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No data to display. Please run the pipeline first.\n"
     ]
    }
   ],
   "source": [
    "if has_data:\n",
    "    # Email processing statistics\n",
    "    total_emails = len(dashboard.emails_df)\n",
    "    successful_emails = len(dashboard.emails_df[dashboard.emails_df['status'] == 'completed'])\n",
    "    failed_emails = len(dashboard.emails_df[dashboard.emails_df['status'] == 'failed'])\n",
    "    pending_emails = len(dashboard.emails_df[dashboard.emails_df['status'] == 'pending'])\n",
    "    \n",
    "    # Create overview dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=('Processing Status', 'Email Priority Distribution', 'Processing Time',\n",
    "                       'Daily Email Volume', 'Top Senders', 'Success Rate Trend'),\n",
    "        specs=[[{'type': 'domain'}, {'type': 'bar'}, {'type': 'histogram'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Processing Status Pie Chart\n",
    "    status_counts = dashboard.emails_df['status'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=status_counts.index,\n",
    "            values=status_counts.values,\n",
    "            name=\"Status\",\n",
    "            marker_colors=['#2ecc71', '#e74c3c', '#f39c12']\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Priority Distribution\n",
    "    priority_hist = dashboard.emails_df['priority'].value_counts().sort_index()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=priority_hist.index,\n",
    "            y=priority_hist.values,\n",
    "            name=\"Priority\",\n",
    "            marker_color='#3498db'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Processing Time Distribution\n",
    "    if 'processing_time_ms' in dashboard.emails_df.columns:\n",
    "        processing_times = dashboard.emails_df['processing_time_ms'].dropna()\n",
    "        if not processing_times.empty:\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=processing_times / 1000,  # Convert to seconds\n",
    "                    name=\"Processing Time (s)\",\n",
    "                    marker_color='#9b59b6'\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "    \n",
    "    # 4. Daily Email Volume\n",
    "    if 'processed_date' in dashboard.emails_df.columns:\n",
    "        daily_counts = pd.to_datetime(dashboard.emails_df['processed_date']).dt.date.value_counts().sort_index()\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=daily_counts.index,\n",
    "                y=daily_counts.values,\n",
    "                name=\"Daily Volume\",\n",
    "                marker_color='#1abc9c'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 5. Top Senders\n",
    "    top_senders = dashboard.emails_df['sender'].value_counts().head(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=[sender[:30] + '...' if len(sender) > 30 else sender for sender in top_senders.index],\n",
    "            x=top_senders.values,\n",
    "            orientation='h',\n",
    "            name=\"Sender Volume\",\n",
    "            marker_color='#e67e22'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 6. Success Rate Trend (if we have date data)\n",
    "    if 'processed_date' in dashboard.emails_df.columns:\n",
    "        dashboard.emails_df['date'] = pd.to_datetime(dashboard.emails_df['processed_date'])\n",
    "        daily_success = dashboard.emails_df.groupby(dashboard.emails_df['date'].dt.date)['status'].apply(\n",
    "            lambda x: (x == 'completed').sum() / len(x) * 100\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=daily_success.index,\n",
    "                y=daily_success.values,\n",
    "                mode='lines+markers',\n",
    "                name=\"Success Rate %\",\n",
    "                line_color='#27ae60'\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        title_text=\"ğŸ“§ Email Processing Dashboard Overview\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nğŸ“Š PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“§ Total emails processed: {total_emails:,}\")\n",
    "    print(f\"âœ… Successful: {successful_emails:,} ({successful_emails/total_emails*100:.1f}%)\")\n",
    "    print(f\"âŒ Failed: {failed_emails:,} ({failed_emails/total_emails*100:.1f}%)\")\n",
    "    print(f\"â³ Pending: {pending_emails:,} ({pending_emails/total_emails*100:.1f}%)\")\n",
    "    \n",
    "    if 'processing_time_ms' in dashboard.emails_df.columns:\n",
    "        avg_time = dashboard.emails_df['processing_time_ms'].mean() / 1000\n",
    "        print(f\"â±ï¸ Average processing time: {avg_time:.2f}s\")\n",
    "    \n",
    "    print(f\"ğŸ“ Total attachments: {len(dashboard.attachments_df):,}\")\n",
    "    print(f\"ğŸ“Š ICE integrations: {len(dashboard.ice_data):,}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No data to display. Please run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Step 3: Detailed Email List\n",
    "\n",
    "Interactive table showing all processed emails with search and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No email data available.\n"
     ]
    }
   ],
   "source": [
    "if has_data:\n",
    "    # Create a detailed email display\n",
    "    display_df = dashboard.emails_df.copy()\n",
    "    \n",
    "    # Clean up the display\n",
    "    if 'subject' in display_df.columns:\n",
    "        display_df['subject'] = display_df['subject'].apply(\n",
    "            lambda x: (x[:60] + '...') if pd.notna(x) and len(str(x)) > 60 else x\n",
    "        )\n",
    "    \n",
    "    if 'sender' in display_df.columns:\n",
    "        display_df['sender'] = display_df['sender'].apply(\n",
    "            lambda x: (x[:40] + '...') if pd.notna(x) and len(str(x)) > 40 else x\n",
    "        )\n",
    "    \n",
    "    # Select columns for display\n",
    "    display_columns = ['email_uid', 'subject', 'sender', 'processed_date', 'status', 'priority']\n",
    "    available_columns = [col for col in display_columns if col in display_df.columns]\n",
    "    \n",
    "    print(\"\\nğŸ“§ PROCESSED EMAILS DETAILS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if available_columns:\n",
    "        # Sort by processed date (most recent first)\n",
    "        if 'processed_date' in display_df.columns:\n",
    "            display_df = display_df.sort_values('processed_date', ascending=False)\n",
    "        \n",
    "        # Show top 20 emails\n",
    "        print(display_df[available_columns].head(20).to_string(index=False, max_colwidth=50))\n",
    "        \n",
    "        if len(display_df) > 20:\n",
    "            print(f\"\\n... and {len(display_df) - 20} more emails\")\n",
    "    \n",
    "    # Show high priority emails\n",
    "    high_priority = dashboard.emails_df[dashboard.emails_df['priority'] > 50]\n",
    "    if not high_priority.empty:\n",
    "        print(\"\\n\\nğŸ”¥ HIGH PRIORITY EMAILS\")\n",
    "        print(\"=\" * 40)\n",
    "        for _, email in high_priority.head(10).iterrows():\n",
    "            subject = email.get('subject', 'No Subject')[:60]\n",
    "            sender = email.get('sender', 'Unknown')[:30]\n",
    "            priority = email.get('priority', 0)\n",
    "            status = email.get('status', 'unknown')\n",
    "            \n",
    "            status_emoji = {'completed': 'âœ…', 'failed': 'âŒ', 'pending': 'â³'}.get(status, 'â“')\n",
    "            \n",
    "            print(f\"{status_emoji} Priority {priority}: {subject}\")\n",
    "            print(f\"   ğŸ“§ From: {sender}\")\n",
    "            print()\n",
    "    \n",
    "    # Show failed emails for troubleshooting\n",
    "    failed_emails = dashboard.emails_df[dashboard.emails_df['status'] == 'failed']\n",
    "    if not failed_emails.empty:\n",
    "        print(\"\\n\\nâŒ FAILED EMAILS (for troubleshooting)\")\n",
    "        print(\"=\" * 50)\n",
    "        for _, email in failed_emails.head(5).iterrows():\n",
    "            subject = email.get('subject', 'No Subject')[:50]\n",
    "            error = email.get('error_message', 'Unknown error')[:80]\n",
    "            \n",
    "            print(f\"ğŸ“§ {subject}\")\n",
    "            print(f\"   âŒ Error: {error}\")\n",
    "            print()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No email data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 4: Investment Entity Analysis\n",
    "\n",
    "Analysis of extracted investment entities from processed emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No ICE entity data available.\n",
      "ğŸ’¡ Ensure the pipeline completed successfully and ICE integration worked.\n"
     ]
    }
   ],
   "source": [
    "if has_data and dashboard.ice_data:\n",
    "    print(\"ğŸ¯ INVESTMENT ENTITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Aggregate data from ICE analysis results\n",
    "    all_tickers = []\n",
    "    all_companies = []\n",
    "    all_people = []\n",
    "    \n",
    "    for ice_result in dashboard.ice_data:\n",
    "        if 'entities' in ice_result:\n",
    "            entities = ice_result['entities']\n",
    "            all_tickers.extend(entities.get('tickers', []))\n",
    "            all_companies.extend(entities.get('companies', []))\n",
    "            all_people.extend(entities.get('people', []))\n",
    "    \n",
    "    if all_tickers or all_companies:\n",
    "        # Create entity visualization\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Top Mentioned Tickers', 'Top Mentioned Companies',\n",
    "                           'Entity Type Distribution', 'Storage Statistics'),\n",
    "            specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "                   [{'type': 'domain'}, {'type': 'bar'}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Top Tickers\n",
    "        if all_tickers:\n",
    "            ticker_counts = pd.Series(all_tickers).value_counts().head(15)\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=ticker_counts.index,\n",
    "                    y=ticker_counts.values,\n",
    "                    name=\"Tickers\",\n",
    "                    marker_color='#3498db'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # 2. Top Companies\n",
    "        if all_companies:\n",
    "            company_counts = pd.Series(all_companies).value_counts().head(10)\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    y=[comp[:25] + '...' if len(comp) > 25 else comp for comp in company_counts.index],\n",
    "                    x=company_counts.values,\n",
    "                    orientation='h',\n",
    "                    name=\"Companies\",\n",
    "                    marker_color='#2ecc71'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Entity Type Distribution\n",
    "        entity_types = ['Tickers', 'Companies', 'People']\n",
    "        entity_counts = [len(set(all_tickers)), len(set(all_companies)), len(set(all_people))]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=entity_types,\n",
    "                values=entity_counts,\n",
    "                name=\"Entity Types\"\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. ICE Storage Statistics\n",
    "        storage_stats = {}\n",
    "        for ice_result in dashboard.ice_data:\n",
    "            if 'storage_stats' in ice_result:\n",
    "                for key, value in ice_result['storage_stats'].items():\n",
    "                    storage_stats[key] = storage_stats.get(key, 0) + value\n",
    "        \n",
    "        if storage_stats:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=list(storage_stats.keys()),\n",
    "                    y=list(storage_stats.values()),\n",
    "                    name=\"Storage\",\n",
    "                    marker_color='#e74c3c'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            showlegend=False,\n",
    "            title_text=\"ğŸ¯ Investment Entity Analysis\",\n",
    "            title_x=0.5\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Print detailed statistics\n",
    "        print(f\"\\nğŸ“ˆ TICKER ANALYSIS:\")\n",
    "        print(f\"   Total ticker mentions: {len(all_tickers)}\")\n",
    "        print(f\"   Unique tickers: {len(set(all_tickers))}\")\n",
    "        \n",
    "        if all_tickers:\n",
    "            top_tickers = pd.Series(all_tickers).value_counts().head(10)\n",
    "            print(f\"   Top tickers: {', '.join(top_tickers.index.tolist())}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¢ COMPANY ANALYSIS:\")\n",
    "        print(f\"   Total company mentions: {len(all_companies)}\")\n",
    "        print(f\"   Unique companies: {len(set(all_companies))}\")\n",
    "        \n",
    "        print(f\"\\nğŸ‘¤ PEOPLE ANALYSIS:\")\n",
    "        print(f\"   Total people mentions: {len(all_people)}\")\n",
    "        print(f\"   Unique people: {len(set(all_people))}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ğŸ“Š Entity extraction results not found in ICE data.\")\n",
    "        print(\"ğŸ’¡ This might indicate the emails were processed with a different version.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No ICE entity data available.\")\n",
    "    print(\"ğŸ’¡ Ensure the pipeline completed successfully and ICE integration worked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 5: Attachment Analysis\n",
    "\n",
    "Analysis of processed email attachments and document extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ No attachment data available.\n",
      "ğŸ’¡ This is normal if processed emails didn't contain attachments.\n"
     ]
    }
   ],
   "source": [
    "if has_data and not dashboard.attachments_df.empty:\n",
    "    print(\"ğŸ“ ATTACHMENT ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Attachment statistics\n",
    "    total_attachments = len(dashboard.attachments_df)\n",
    "    processed_attachments = len(dashboard.attachments_df[dashboard.attachments_df['processing_status'] == 'completed'])\n",
    "    \n",
    "    print(f\"ğŸ“ Total attachments: {total_attachments:,}\")\n",
    "    print(f\"âœ… Successfully processed: {processed_attachments:,} ({processed_attachments/total_attachments*100:.1f}%)\")\n",
    "    \n",
    "    # File type analysis\n",
    "    if 'mime_type' in dashboard.attachments_df.columns:\n",
    "        file_types = dashboard.attachments_df['mime_type'].value_counts()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š File Types:\")\n",
    "        for mime_type, count in file_types.head(10).items():\n",
    "            percentage = count / total_attachments * 100\n",
    "            print(f\"   {mime_type}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Size analysis\n",
    "    if 'file_size' in dashboard.attachments_df.columns:\n",
    "        total_size_mb = dashboard.attachments_df['file_size'].sum() / (1024 * 1024)\n",
    "        avg_size_kb = dashboard.attachments_df['file_size'].mean() / 1024\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Size Analysis:\")\n",
    "        print(f\"   Total size: {total_size_mb:.1f} MB\")\n",
    "        print(f\"   Average size: {avg_size_kb:.1f} KB\")\n",
    "    \n",
    "    # OCR confidence analysis\n",
    "    if 'ocr_confidence' in dashboard.attachments_df.columns:\n",
    "        ocr_data = dashboard.attachments_df['ocr_confidence'].dropna()\n",
    "        if not ocr_data.empty:\n",
    "            avg_confidence = ocr_data.mean()\n",
    "            print(f\"\\nğŸ” OCR Analysis:\")\n",
    "            print(f\"   Average confidence: {avg_confidence:.2f}\")\n",
    "            print(f\"   High confidence (>0.8): {len(ocr_data[ocr_data > 0.8])} files\")\n",
    "    \n",
    "    # Create attachment visualization\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('File Type Distribution', 'Processing Status', 'File Size Distribution'),\n",
    "        specs=[[{'type': 'domain'}, {'type': 'domain'}, {'type': 'histogram'}]]\n",
    "    )\n",
    "    \n",
    "    # File types pie chart\n",
    "    if 'mime_type' in dashboard.attachments_df.columns:\n",
    "        file_types = dashboard.attachments_df['mime_type'].value_counts().head(8)\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=[t.split('/')[-1] for t in file_types.index],  # Simplify mime types\n",
    "                values=file_types.values,\n",
    "                name=\"File Types\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Processing status pie chart\n",
    "    status_counts = dashboard.attachments_df['processing_status'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=status_counts.index,\n",
    "            values=status_counts.values,\n",
    "            name=\"Status\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # File size histogram\n",
    "    if 'file_size' in dashboard.attachments_df.columns:\n",
    "        sizes_kb = dashboard.attachments_df['file_size'] / 1024\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=sizes_kb,\n",
    "                name=\"Size (KB)\",\n",
    "                nbinsx=20\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        showlegend=False,\n",
    "        title_text=\"ğŸ“ Attachment Processing Analysis\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Show sample attachments\n",
    "    print(\"\\nğŸ“‹ SAMPLE ATTACHMENTS:\")\n",
    "    sample_attachments = dashboard.attachments_df.head(10)\n",
    "    for _, att in sample_attachments.iterrows():\n",
    "        filename = att.get('filename', 'Unknown')[:40]\n",
    "        size_kb = att.get('file_size', 0) / 1024 if pd.notna(att.get('file_size')) else 0\n",
    "        status = att.get('processing_status', 'unknown')\n",
    "        method = att.get('extraction_method', 'unknown')\n",
    "        \n",
    "        status_emoji = {'completed': 'âœ…', 'failed': 'âŒ', 'pending': 'â³'}.get(status, 'â“')\n",
    "        \n",
    "        print(f\"{status_emoji} {filename} ({size_kb:.1f} KB) - {method}\")\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ“ No attachment data available.\")\n",
    "    print(\"ğŸ’¡ This is normal if processed emails didn't contain attachments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 6: Interactive Email Search\n",
    "\n",
    "Search through processed emails by various criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No email data available for searching.\n"
     ]
    }
   ],
   "source": [
    "if has_data:\n",
    "    print(\"ğŸ” INTERACTIVE EMAIL SEARCH\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    def search_emails(query=\"\", sender=\"\", priority_min=0, status=\"all\"):\n",
    "        \"\"\"Search emails with various filters\"\"\"\n",
    "        df = dashboard.emails_df.copy()\n",
    "        \n",
    "        # Apply filters\n",
    "        if query:\n",
    "            mask = df['subject'].str.contains(query, case=False, na=False)\n",
    "            df = df[mask]\n",
    "        \n",
    "        if sender:\n",
    "            mask = df['sender'].str.contains(sender, case=False, na=False)\n",
    "            df = df[mask]\n",
    "        \n",
    "        if priority_min > 0:\n",
    "            df = df[df['priority'] >= priority_min]\n",
    "        \n",
    "        if status != \"all\":\n",
    "            df = df[df['status'] == status]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Example searches\n",
    "    print(\"ğŸ“§ Example searches:\")\n",
    "    print()\n",
    "    \n",
    "    # 1. High priority emails\n",
    "    high_priority = search_emails(priority_min=50)\n",
    "    print(f\"ğŸ”¥ High priority emails (>50): {len(high_priority)}\")\n",
    "    if not high_priority.empty:\n",
    "        for _, email in high_priority.head(3).iterrows():\n",
    "            subject = email.get('subject', 'No Subject')[:50]\n",
    "            priority = email.get('priority', 0)\n",
    "            print(f\"   â€¢ Priority {priority}: {subject}\")\n",
    "    print()\n",
    "    \n",
    "    # 2. Search for investment terms\n",
    "    investment_terms = ['earnings', 'portfolio', 'analysis', 'rating', 'target']\n",
    "    \n",
    "    for term in investment_terms:\n",
    "        results = search_emails(query=term)\n",
    "        if not results.empty:\n",
    "            print(f\"ğŸ“ˆ Emails mentioning '{term}': {len(results)}\")\n",
    "            \n",
    "            # Show top result\n",
    "            if len(results) > 0:\n",
    "                top_result = results.iloc[0]\n",
    "                subject = top_result.get('subject', 'No Subject')[:60]\n",
    "                sender = top_result.get('sender', 'Unknown')[:30]\n",
    "                print(f\"   ğŸ“§ {subject} (from {sender})\")\n",
    "            print()\n",
    "    \n",
    "    # 3. Search by sender domain\n",
    "    sender_domains = ['research', 'analyst', 'investment', 'agtpartners']\n",
    "    \n",
    "    print(\"ğŸ‘¥ Emails by sender type:\")\n",
    "    for domain in sender_domains:\n",
    "        results = search_emails(sender=domain)\n",
    "        if not results.empty:\n",
    "            print(f\"   {domain}: {len(results)} emails\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Failed emails for troubleshooting\n",
    "    failed = search_emails(status='failed')\n",
    "    print(f\"âŒ Failed emails: {len(failed)}\")\n",
    "    if not failed.empty:\n",
    "        print(\"   Common failure patterns:\")\n",
    "        if 'error_message' in failed.columns:\n",
    "            error_counts = failed['error_message'].value_counts().head(3)\n",
    "            for error, count in error_counts.items():\n",
    "                if pd.notna(error):\n",
    "                    error_short = error[:60] + '...' if len(str(error)) > 60 else error\n",
    "                    print(f\"     â€¢ {error_short}: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ You can create custom searches using the search_emails() function:\")\n",
    "    print(\"   search_emails(query='NVIDIA', priority_min=30)\")\n",
    "    print(\"   search_emails(sender='research', status='completed')\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No email data available for searching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’­ Step 7: Query Interface\n",
    "\n",
    "Interface to query the processed email knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No processed email data available for querying.\n"
     ]
    }
   ],
   "source": [
    "if has_data:\n",
    "    print(\"ğŸ’­ ICE QUERY INTERFACE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Try to load ICE integrator from the working directories\n",
    "    ice_integrator = None\n",
    "    \n",
    "    for working_dir in dashboard.working_dirs:\n",
    "        ice_storage_dir = os.path.join(working_dir, 'ice_storage')\n",
    "        if os.path.exists(ice_storage_dir):\n",
    "            try:\n",
    "                # Import ICE integrator\n",
    "                from ice_integrator import ICEEmailIntegrator\n",
    "                ice_integrator = ICEEmailIntegrator(ice_storage_dir)\n",
    "                print(f\"âœ… Connected to ICE knowledge base: {ice_storage_dir}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not load ICE integrator: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if ice_integrator:\n",
    "        # Sample queries\n",
    "        sample_queries = [\n",
    "            \"What stocks were mentioned in my recent emails?\",\n",
    "            \"What's the sentiment around technology stocks?\",\n",
    "            \"Who are the key analysts mentioned?\",\n",
    "            \"What companies have earnings updates?\",\n",
    "            \"What investment risks were highlighted?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nğŸ“ Sample queries you can ask:\")\n",
    "        for i, query in enumerate(sample_queries, 1):\n",
    "            print(f\"   {i}. {query}\")\n",
    "        \n",
    "        print(\"\\nğŸ” Query Examples:\")\n",
    "        \n",
    "        # Try a few sample queries\n",
    "        test_queries = [\n",
    "            \"What tickers were mentioned?\",\n",
    "            \"Show me investment recommendations\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nâ“ Query: {query}\")\n",
    "            try:\n",
    "                # Note: This might need async handling depending on implementation\n",
    "                result = ice_integrator.query_email_content(query, mode=\"hybrid\")\n",
    "                \n",
    "                if isinstance(result, dict) and result.get('success'):\n",
    "                    response = result.get('response', 'No response generated')\n",
    "                    print(f\"âœ… Response: {response[:200]}...\" if len(response) > 200 else f\"âœ… Response: {response}\")\n",
    "                else:\n",
    "                    print(f\"âŒ Query failed or returned no results\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Query error: {e}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ To run custom queries:\")\n",
    "        print(\"   result = ice_integrator.query_email_content('Your question here')\")\n",
    "        print(\"   print(result)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ICE knowledge base not found\")\n",
    "        print(\"ğŸ’¡ Make sure you ran the full pipeline: python process_emails.py\")\n",
    "        \n",
    "        # Show available data summary instead\n",
    "        if dashboard.ice_data:\n",
    "            print(\"\\nğŸ“Š Available ICE data summary:\")\n",
    "            for i, ice_result in enumerate(dashboard.ice_data[:3], 1):\n",
    "                print(f\"   Source {i}:\")\n",
    "                if 'entities' in ice_result:\n",
    "                    entities = ice_result['entities']\n",
    "                    print(f\"     Tickers: {len(entities.get('tickers', []))}\")\n",
    "                    print(f\"     Companies: {len(entities.get('companies', []))}\")\n",
    "                if 'summary' in ice_result:\n",
    "                    summary = ice_result['summary']\n",
    "                    print(f\"     Processed: {summary.get('processed_count', 0)} emails\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No processed email data available for querying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 8: Export and Summary\n",
    "\n",
    "Export processed data and generate final summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No data available to export.\n",
      "\n",
      "ğŸš€ To get started:\n",
      "   1. Re-run this notebook - it will automatically prompt you to run the pipeline\n",
      "   2. Or manually run: python process_emails.py YOUR_PASSWORD\n",
      "   3. Then re-run this notebook to see your results\n",
      "\n",
      "ğŸ’¡ This dashboard will automatically:\n",
      "   â€¢ Test your email connection first\n",
      "   â€¢ Show real-time pipeline progress\n",
      "   â€¢ Automatically refresh data after completion\n",
      "   â€¢ Handle errors gracefully with clear instructions\n"
     ]
    }
   ],
   "source": [
    "if has_data:\n",
    "    print(\"ğŸ“Š EXPORT AND SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Generate export files\n",
    "    export_dir = \"./email_analysis_exports\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Export emails to CSV\n",
    "    emails_export = os.path.join(export_dir, f\"processed_emails_{timestamp}.csv\")\n",
    "    dashboard.emails_df.to_csv(emails_export, index=False)\n",
    "    print(f\"âœ… Emails exported to: {emails_export}\")\n",
    "    \n",
    "    # Export attachments to CSV\n",
    "    if not dashboard.attachments_df.empty:\n",
    "        attachments_export = os.path.join(export_dir, f\"processed_attachments_{timestamp}.csv\")\n",
    "        dashboard.attachments_df.to_csv(attachments_export, index=False)\n",
    "        print(f\"âœ… Attachments exported to: {attachments_export}\")\n",
    "    \n",
    "    # Export ICE data summary\n",
    "    if dashboard.ice_data:\n",
    "        ice_summary_export = os.path.join(export_dir, f\"ice_data_summary_{timestamp}.json\")\n",
    "        with open(ice_summary_export, 'w') as f:\n",
    "            json.dump(dashboard.ice_data, f, indent=2, default=str)\n",
    "        print(f\"âœ… ICE data exported to: {ice_summary_export}\")\n",
    "    \n",
    "    # Generate final summary report\n",
    "    summary_report = {\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "        \"pipeline_statistics\": {\n",
    "            \"total_emails\": len(dashboard.emails_df),\n",
    "            \"successful_emails\": len(dashboard.emails_df[dashboard.emails_df['status'] == 'completed']) if 'status' in dashboard.emails_df.columns else 0,\n",
    "            \"failed_emails\": len(dashboard.emails_df[dashboard.emails_df['status'] == 'failed']) if 'status' in dashboard.emails_df.columns else 0,\n",
    "            \"total_attachments\": len(dashboard.attachments_df),\n",
    "            \"ice_integrations\": len(dashboard.ice_data)\n",
    "        },\n",
    "        \"top_senders\": dashboard.emails_df['sender'].value_counts().head(10).to_dict() if 'sender' in dashboard.emails_df.columns else {},\n",
    "        \"priority_distribution\": dashboard.emails_df['priority'].describe().to_dict() if 'priority' in dashboard.emails_df.columns else {},\n",
    "        \"working_directories\": dashboard.working_dirs\n",
    "    }\n",
    "    \n",
    "    summary_export = os.path.join(export_dir, f\"pipeline_summary_{timestamp}.json\")\n",
    "    with open(summary_export, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2, default=str)\n",
    "    print(f\"âœ… Summary report exported to: {summary_export}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ EMAIL ANALYSIS DASHBOARD COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“§ Total emails analyzed: {len(dashboard.emails_df):,}\")\n",
    "    \n",
    "    if 'status' in dashboard.emails_df.columns:\n",
    "        success_count = len(dashboard.emails_df[dashboard.emails_df['status'] == 'completed'])\n",
    "        print(f\"ğŸ“Š Success rate: {success_count / len(dashboard.emails_df) * 100:.1f}%\")\n",
    "    \n",
    "    print(f\"ğŸ“ Attachments processed: {len(dashboard.attachments_df):,}\")\n",
    "    print(f\"ğŸ§  ICE integrations: {len(dashboard.ice_data):,}\")\n",
    "    print(f\"ğŸ’¾ Export directory: {export_dir}\")\n",
    "    \n",
    "    print(\"\\nğŸ” Next steps:\")\n",
    "    print(\"   â€¢ Use the query interface to ask questions about your emails\")\n",
    "    print(\"   â€¢ Review failed emails for troubleshooting\")\n",
    "    print(\"   â€¢ Run additional pipeline cycles to process more emails\")\n",
    "    print(\"   â€¢ Integrate with the main ICE system for advanced analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No data available to export.\")\n",
    "    print(\"\\nğŸš€ To get started:\")\n",
    "    print(\"   1. Re-run this notebook - it will automatically prompt you to run the pipeline\")\n",
    "    print(\"   2. Or manually run: python process_emails.py YOUR_PASSWORD\")\n",
    "    print(\"   3. Then re-run this notebook to see your results\")\n",
    "    print(\"\\nğŸ’¡ This dashboard will automatically:\")\n",
    "    print(\"   â€¢ Test your email connection first\") \n",
    "    print(\"   â€¢ Show real-time pipeline progress\")\n",
    "    print(\"   â€¢ Automatically refresh data after completion\")\n",
    "    print(\"   â€¢ Handle errors gracefully with clear instructions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

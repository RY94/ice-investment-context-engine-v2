{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICE Building Workflow - Knowledge Graph Construction\n",
    "\n",
    "**Purpose**: Comprehensive data ingestion and knowledge graph building for investment intelligence\n",
    "**Architecture**: ICE Simplified (2,508 lines) with LightRAG integration\n",
    "**Input**: Financial data from multiple sources ‚Üí **Output**: Searchable knowledge graph\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Environment Setup** - Initialize ICE system and configure data sources\n",
    "2. **Workflow Mode Selection** - Choose between initial build or incremental update\n",
    "3. **Data Ingestion** - Fetch financial data from APIs and process documents\n",
    "4. **Knowledge Graph Building** - Extract entities, relationships, and build LightRAG graph\n",
    "5. **Storage & Validation** - Verify graph construction and monitor storage\n",
    "6. **Metrics & Monitoring** - Track processing metrics and system health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 1. Environment Setup & System Initialization -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "# üîß Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set API key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', '')\n",
    "\n",
    "# Enable Docling for professional-grade document parsing (97.9% table accuracy)\n",
    "# Phase 2 implementation (2025-11-04): Docling for both email attachments AND URL PDFs\n",
    "os.environ['USE_DOCLING_EMAIL'] = 'true'   # Email attachments: 42% ‚Üí 97.9%\n",
    "os.environ['USE_DOCLING_URLS'] = 'true'    # URL PDFs: 42% ‚Üí 97.9% (PHASE 2 - NEW)\n",
    "\n",
    "# Enable Crawl4AI for complex URL processing (JS-heavy sites, portals, paywalls)\n",
    "# Set to 'true' to enable browser automation for Tier 3-5 URLs\n",
    "# Set to 'false' (default) for simple HTTP only\n",
    "os.environ['USE_CRAWL4AI_LINKS'] = 'true'  # Enable Crawl4AI\n",
    "os.environ['CRAWL4AI_TIMEOUT'] = '80'      # 60 second timeout\n",
    "os.environ['CRAWL4AI_HEADLESS'] = 'true'   # Run browser in background\n",
    "\n",
    "# URL Rate Limiting (added 2025-11-05 for robustness)\n",
    "os.environ['URL_RATE_LIMIT_DELAY'] = '1.0'      # Seconds between requests per domain (default: 1.0)\n",
    "os.environ['URL_CONCURRENT_DOWNLOADS'] = '3'     # Max concurrent downloads (default: 3)\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "print(f\"üìÅ Working directory: {project_root}\")\n",
    "print(f\"üìä Docling enabled (email): {os.environ.get('USE_DOCLING_EMAIL', 'false')}\")\n",
    "print(f\"üìä Docling enabled (URLs): {os.environ.get('USE_DOCLING_URLS', 'false')}\")\n",
    "print(f\"üåê Crawl4AI enabled: {os.environ.get('USE_CRAWL4AI_LINKS', 'false')}\")\n",
    "\n",
    "# In notebook, after Cell 2:\n",
    "print(f\"Email Docling: {os.environ.get('USE_DOCLING_EMAIL')}\")  # Should show: true\n",
    "print(f\"URL Docling: {os.environ.get('USE_DOCLING_URLS')}\")     # Should show: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Initialize ICE system\n",
    "from updated_architectures.implementation.ice_simplified import create_ice_system\n",
    "\n",
    "try:\n",
    "    ice = create_ice_system()\n",
    "    system_ready = ice.is_ready()\n",
    "    print(f\"‚úÖ ICE System Initialized\")\n",
    "    print(f\"üß† LightRAG Status: {'Ready' if system_ready else 'Initializing'}\")\n",
    "    print(f\"üìä Architecture: ICE Simplified (2,508 lines)\")\n",
    "    print(f\"üîó Components: Core + Ingester + QueryEngine\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization Error: {e}\")\n",
    "    raise  # Let errors surface for proper debugging\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "print('#'*150)\n",
    "print(\"=\"*70)\n",
    "print(\"ATTACHMENT PROCESSOR DIAGNOSTIC (CORRECTED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Correct path: ice.ingester.attachment_processor (not ice.data_sources)\n",
    "if hasattr(ice, 'ingester'):\n",
    "    print(f\"‚úÖ ice.ingester exists: {type(ice.ingester).__name__}\")\n",
    "\n",
    "    if hasattr(ice.ingester, 'attachment_processor'):\n",
    "        if ice.ingester.attachment_processor is None:\n",
    "            print(\"‚ùå attachment_processor is None (initialization failed)\")\n",
    "            print(\"   ‚Üí PDFs will be downloaded but NOT processed/ingested\")\n",
    "            print(\"\\nüîç Why this happens:\")\n",
    "            print(\"   - ImportError during DoclingProcessor/AttachmentProcessor import\")\n",
    "            print(\"   - Exception during processor initialization\")\n",
    "            print(\"\\nüìã Check Cell 3 output for:\")\n",
    "            print(\"   - '‚ö†Ô∏è Attachment processor initialization failed: [error]'\")\n",
    "        else:\n",
    "            processor_type =type(ice.ingester.attachment_processor).__name__\n",
    "            print(f\"‚úÖ attachment_processor initialized: {processor_type}\")\n",
    "            print(\"   ‚Üí PDFs WILL be processed and ingested\")\n",
    "\n",
    "            # Check which processor\n",
    "            if 'Docling' in processor_type:\n",
    "                print(\"   ‚Üí Using DoclingProcessor (97.9% table accuracy)\")\n",
    "            else:\n",
    "                print(\"   ‚Üí Using AttachmentProcessor (42% table accuracy, PyPDF2)\")\n",
    "    else:\n",
    "        print(\"‚ùå attachment_processor attribute missing from ingester\")\n",
    "        print(\"   ‚Üí This is a code structure issue\")\n",
    "else:\n",
    "    print(\"‚ùå ice.ingester attribute missing\")\n",
    "    print(\"   ‚Üí ICE system not properly initialized\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ATTACHMENT PROCESSOR DIAGNOSTIC (CORRECTED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Correct path: ice.ingester.attachment_processor (not ice.data_sources)\n",
    "if hasattr(ice, 'ingester'):\n",
    "    print(f\"‚úÖ ice.ingester exists: {type(ice.ingester).__name__}\")\n",
    "\n",
    "    if hasattr(ice.ingester, 'attachment_processor'):\n",
    "        if ice.ingester.attachment_processor is None:\n",
    "            print(\"‚ùå attachment_processor is None (initialization failed)\")\n",
    "            print(\"   ‚Üí PDFs will be downloaded but NOT processed/ingested\")\n",
    "            print(\"\\nüîç Why this happens:\")\n",
    "            print(\"   - ImportError during DoclingProcessor/AttachmentProcessor import\")\n",
    "            print(\"   - Exception during processor initialization\")\n",
    "            print(\"\\nüìã Check Cell 3 output for:\")\n",
    "            print(\"   - '‚ö†Ô∏è Attachment processor initialization failed: [error]'\")\n",
    "        else:\n",
    "            processor_type =type(ice.ingester.attachment_processor).__name__\n",
    "            print(f\"‚úÖ attachment_processor initialized: {processor_type}\")\n",
    "            print(\"   ‚Üí PDFs WILL be processed and ingested\")\n",
    "\n",
    "            # Check which processor\n",
    "            if 'Docling' in processor_type:\n",
    "                print(\"   ‚Üí Using DoclingProcessor (97.9% table accuracy)\")\n",
    "            else:\n",
    "                print(\"   ‚Üí Using AttachmentProcessor (42% table accuracy, PyPDF2)\")\n",
    "    else:\n",
    "        print(\"‚ùå attachment_processor attribute missing from ingester\")\n",
    "        print(\"   ‚Üí This is a code structure issue\")\n",
    "else:\n",
    "    print(\"‚ùå ice.ingester attribute missing\")\n",
    "    print(\"   ‚Üí ICE system not properly initialized\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Verify storage architecture and components\n",
    "print(f\"üì¶ LightRAG Storage Architecture Verification\")\n",
    "print(f\"‚îÅ\" * 40)\n",
    "\n",
    "if not (ice and ice.core.is_ready()):\n",
    "    raise RuntimeError(\"ICE system not ready - cannot verify storage\")\n",
    "\n",
    "# Get storage statistics using new method\n",
    "storage_stats = ice.core.get_storage_stats()\n",
    "\n",
    "print(f\"LightRAG Storage Components:\")\n",
    "for component_name, component_info in storage_stats['components'].items():\n",
    "    status = \"‚úÖ Initialized\" if component_info['exists'] else \"‚ö†Ô∏è Not created yet\"\n",
    "    size_mb = component_info['size_bytes'] / (1024 * 1024) if component_info['size_bytes'] > 0 else 0\n",
    "    print(f\"  {component_name}: {status}\")\n",
    "    print(f\"    Purpose: {component_info['description']}\")\n",
    "    print(f\"    File: {component_info['file']}\")\n",
    "    if size_mb > 0:\n",
    "        print(f\"    Size: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìÅ Working Directory: {storage_stats['working_dir']}\")\n",
    "print(f\"üóÑÔ∏è Storage Backend: File-based (development mode)\")\n",
    "print(f\"üíæ Total Storage: {storage_stats['total_storage_bytes'] / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Data sources configuration status\n",
    "if not (ice and hasattr(ice, 'ingester')):\n",
    "    raise RuntimeError(\"Data ingester not initialized\")\n",
    "\n",
    "available_services = ice.ingester.available_services\n",
    "print(f\"\\nüì° Data Sources Available: {len(available_services)}\")\n",
    "for service in available_services:\n",
    "    print(f\"  ‚úÖ {service}\")\n",
    "\n",
    "if not available_services:\n",
    "    print(f\"  ‚ö†Ô∏è No APIs configured - will use sample data\")\n",
    "    print(f\"  üí° Set NEWSAPI_ORG_API_KEY for real news\")\n",
    "    print(f\"  üí° Set ALPHA_VANTAGE_API_KEY for financial data\")\n",
    "\n",
    "# Validate OpenAI for LightRAG\n",
    "openai_configured = bool(os.getenv('OPENAI_API_KEY'))\n",
    "print(f\"\\nüîë OpenAI API: {'‚úÖ Configured' if openai_configured else '‚ùå Required for full functionality'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 2. Workflow Mode Selection & Configuration -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### üîß Model Provider Configuration\n",
    "\n",
    "ICE supports **OpenAI** (paid) or **Ollama** (free local) for LLM and embeddings:\n",
    "\n",
    "#### Option 1: OpenAI (Default - No setup required)\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n",
    "- **Cost**: ~$5/month for typical usage\n",
    "- **Quality**: Highest accuracy for entity extraction and reasoning\n",
    "- **Setup**: Just set API key\n",
    "\n",
    "#### Option 2: Ollama (Free Local - Requires setup)\n",
    "```bash\n",
    "# Set provider\n",
    "export LLM_PROVIDER=\"ollama\"\n",
    "\n",
    "# One-time setup:\n",
    "ollama serve                      # Start Ollama service\n",
    "ollama pull qwen3:30b-32k        # Pull LLM model (32k context required)\n",
    "ollama pull nomic-embed-text      # Pull embedding model\n",
    "```\n",
    "- **Cost**: $0/month (completely free)\n",
    "- **Quality**: Good for most investment analysis tasks\n",
    "- **Setup**: Requires local Ollama installation and model download\n",
    "\n",
    "#### Option 3: Hybrid (Recommended for cost-conscious users)\n",
    "```bash\n",
    "export LLM_PROVIDER=\"ollama\"           # Use Ollama for LLM\n",
    "export EMBEDDING_PROVIDER=\"openai\"     # Use OpenAI for embeddings\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n",
    "- **Cost**: ~$2/month (embeddings only)\n",
    "- **Quality**: Balanced - free LLM with high-quality embeddings\n",
    "\n",
    "**Current configuration will be logged when you run the next cell.** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### üìÑ Docling Document Processing (Switchable Architecture)\n",
    "\n",
    "ICE supports **switchable document processing** for SEC filings and email attachments:\n",
    "\n",
    "#### Docling Integration (Default - Professional-grade table extraction)\n",
    "```bash\n",
    "export USE_DOCLING_SEC=true          # SEC filings: 0% ‚Üí 97.9% table extraction\n",
    "export USE_DOCLING_EMAIL=true        # Email attachments: 42% ‚Üí 97.9% accuracy\n",
    "```\n",
    "- **Accuracy**: 97.9% table extraction (vs 42% with PyPDF2)\n",
    "- **Cost**: $0/month (local execution)\n",
    "- **Setup**: Models auto-download on first use (~500MB, one-time)\n",
    "\n",
    "#### Original Implementations (For comparison testing)\n",
    "```bash\n",
    "export USE_DOCLING_SEC=false         # Use metadata-only SEC extraction\n",
    "export USE_DOCLING_EMAIL=false       # Use PyPDF2/openpyxl processors\n",
    "```\n",
    "- **Purpose**: A/B testing and backward compatibility\n",
    "- **Use when**: Comparing extraction accuracy or troubleshooting\n",
    "\n",
    "**Note**: Docling uses IBM's AI models (DocLayNet, TableFormer, Granite-Docling VLM) for professional-grade document parsing. Both implementations coexist - toggle switches instantly without code changes.\n",
    "\n",
    "**More info**: See `md_files/DOCLING_INTEGRATION_TESTING.md` for detailed testing procedures. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### üåê Crawl4AI URL Fetching (Switchable Architecture)\n",
    "\n",
    "ICE supports **hybrid URL fetching** for email links with intelligent routing:\n",
    "\n",
    "#### Smart Routing Strategy (Default - Simple HTTP only)\n",
    "```bash\n",
    "export USE_CRAWL4AI_LINKS=false      # Simple HTTP only (fast, free, default)\n",
    "```\n",
    "- **Approach**: Use simple HTTP (aiohttp) for all URLs\n",
    "- **Works for**: DBS research URLs with embedded tokens, direct PDFs, SEC EDGAR\n",
    "- **Cost**: $0/month (no browser automation)\n",
    "- **Speed**: <2 seconds per URL average\n",
    "\n",
    "#### Crawl4AI Hybrid Routing (Enable for complex sites)\n",
    "```bash\n",
    "export USE_CRAWL4AI_LINKS=true       # Enable browser automation for complex URLs\n",
    "export CRAWL4AI_TIMEOUT=60           # Timeout in seconds (default: 60)\n",
    "export CRAWL4AI_HEADLESS=true        # Headless mode (default: true)\n",
    "```\n",
    "- **Routing**: Automatically classifies URLs and uses appropriate method\n",
    "  - Simple HTTP: DBS URLs, direct file downloads, SEC EDGAR, static content\n",
    "  - Crawl4AI: Premium research portals (Goldman, Morgan Stanley), JS-heavy IR sites\n",
    "- **Fallback**: Graceful degradation to simple HTTP if Crawl4AI fails\n",
    "- **Cost**: CPU cost for browser automation, free tier usage\n",
    "- **Use when**: Accessing JavaScript-heavy sites or login-required portals\n",
    "\n",
    "**Note**: DBS research portal URLs work with simple HTTP (embedded auth tokens in `?E=...` parameter) - no browser automation needed.\n",
    "\n",
    "**More info**: See `md_files/CRAWL4AI_INTEGRATION_PLAN.md` for detailed integration strategy and `CLAUDE.md` Pattern #6 for code examples. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# ### Provider Switching - Uncomment ONE option below, then restart kernel\n",
    "\n",
    "#############################################################################\n",
    "#                              Model Selection                              #\n",
    "#############################################################################\n",
    "\n",
    "### Option 1: OpenAI ($5/mo, highest quality)\n",
    "import os; os.environ['LLM_PROVIDER'] = 'openai'\n",
    "print(\"‚úÖ Switched to OpenAI\")\n",
    "\n",
    "# ###Option 2: Hybrid ($2/mo, 60% savings, recommended)\n",
    "# import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'openai'\n",
    "# print(\"‚úÖ Switched to Hybrid\")\n",
    "\n",
    "# ### Option 3: Full Ollama ($0/mo, faster model)\n",
    "# import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'ollama'; \n",
    "# # os.environ['LLM_MODEL'] = 'llama3.1:8b'\n",
    "# os.environ['LLM_MODEL'] = 'qwen3:8b'\n",
    "# # os.environ['LLM_MODEL'] = 'qwen3:14b'\n",
    "# # os.environ['LLM_MODEL'] = 'qwen3:30b-32k'\n",
    "# print(\"‚úÖ Switched to Full Ollama Model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### üóëÔ∏è Graph Management (Optional)\n",
    "\n",
    "**When to clear the graph:**\n",
    "- ‚úÖ Switching to Full Ollama (1536-dim ‚Üí 768-dim embeddings)\n",
    "- ‚úÖ Graph corrupted or very old (>30 days without updates)\n",
    "- ‚úÖ Testing fresh graph builds from scratch\n",
    "\n",
    "**When NOT to clear:**\n",
    "- ‚ùå Just switching LLM provider (OpenAI ‚Üî Hybrid use same embeddings)\n",
    "- ‚ùå Adding new documents (incremental updates work fine)\n",
    "- ‚ùå Changing query modes (local, hybrid, etc.)\n",
    "\n",
    "**How to clear:**\n",
    "Run the code cell below (uncomment lines to activate) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "##########################################################\n",
    "#                    Check graph info                    #\n",
    "##########################################################\n",
    "from updated_architectures.implementation.ice_simplified import create_ice_system\n",
    "ice = create_ice_system()\n",
    "\n",
    "# Toggle to enable/disable logging output to file\n",
    "SAVE_PROCESSING_LOG = True  # Set to False to disable logging\n",
    "\n",
    "if SAVE_PROCESSING_LOG:\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    \n",
    "    output_buffer = StringIO()\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = output_buffer\n",
    "    error_buffer = StringIO()\n",
    "    original_stderr = sys.stderr\n",
    "    sys.stderr = error_buffer\n",
    "\n",
    "    # Reconfigure logging handlers to use redirected stderr\n",
    "    import logging\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        handler.stream = sys.stderr  # Now points to error_buffer\n",
    "\n",
    "\n",
    "##########################################################\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def check_storage(storage_path):\n",
    "    \"\"\"Check and display storage file inventory\"\"\"\n",
    "    files = ['vdb_entities.json', 'vdb_relationships.json', 'vdb_chunks.json', 'graph_chunk_entity_relation.graphml']\n",
    "    total_size = 0\n",
    "    for fname in files:\n",
    "        fpath = storage_path / fname\n",
    "        if fpath.exists():\n",
    "            size_mb = fpath.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úÖ {fname}: {size_mb:.2f} MB\")\n",
    "            total_size += size_mb\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  {fname}: not found\")\n",
    "    print(f\"  üíæ Total: {total_size:.2f} MB\")\n",
    "\n",
    "# Use actual config path instead of hardcoded path to avoid path mismatches\n",
    "storage_path = Path(ice.config.working_dir)\n",
    "\n",
    "check_storage(storage_path)\n",
    "\n",
    "#####################################################################\n",
    "ice.core.get_graph_stats()\n",
    "\n",
    "##########################################################\n",
    "#              Graph Health Metrics (P0)                 #\n",
    "##########################################################\n",
    "\n",
    "def check_graph_health(storage_path):\n",
    "    \"\"\"Check critical graph health metrics (P0 only)\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    TICKERS = {'NVDA', 'TSMC', 'AMD', 'ASML'}  # Known portfolio tickers\n",
    "    \n",
    "    result = {\n",
    "        'tickers_covered': set(),\n",
    "        'total_entities': 0,\n",
    "        'total_relationships': 0,\n",
    "        'buy_signals': 0,\n",
    "        'sell_signals': 0,\n",
    "        'price_targets': 0\n",
    "    }\n",
    "    \n",
    "    # Parse entities\n",
    "    entities_file = Path(storage_path) / 'vdb_entities.json'\n",
    "    if entities_file.exists():\n",
    "        data = json.loads(entities_file.read_text())\n",
    "        result['total_entities'] = len(data.get('data', []))\n",
    "        \n",
    "        for entity in data.get('data', []):\n",
    "            text = f\"{entity.get('entity_name', '')} {entity.get('content', '')}\".upper()\n",
    "            \n",
    "            # Detect tickers\n",
    "            for ticker in TICKERS:\n",
    "                if ticker in text:\n",
    "                    result['tickers_covered'].add(ticker)\n",
    "            \n",
    "            # Detect signals\n",
    "            if 'BUY' in text:\n",
    "                result['buy_signals'] += 1\n",
    "            if 'SELL' in text:\n",
    "                result['sell_signals'] += 1\n",
    "            if 'PRICE TARGET' in text or 'PRICE_TARGET' in text:\n",
    "                result['price_targets'] += 1\n",
    "    \n",
    "    # Parse relationships\n",
    "    rels_file = Path(storage_path) / 'vdb_relationships.json'\n",
    "    if rels_file.exists():\n",
    "        data = json.loads(rels_file.read_text())\n",
    "        result['total_relationships'] = len(data.get('data', []))\n",
    "    \n",
    "    result['tickers_covered'] = sorted(list(result['tickers_covered']))\n",
    "    return result\n",
    "\n",
    "def get_extended_graph_stats(storage_path):\n",
    "    \"\"\"Get additional graph statistics for comprehensive analysis\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    stats = {\n",
    "        'chunk_count': 0,\n",
    "        'email_chunks': 0,\n",
    "        'api_sec_chunks': 0\n",
    "    }\n",
    "    \n",
    "    # Parse chunks\n",
    "    chunks_file = Path(storage_path) / 'vdb_chunks.json'\n",
    "    if chunks_file.exists():\n",
    "        data = json.loads(chunks_file.read_text())\n",
    "        chunks = data.get('data', [])\n",
    "        stats['chunk_count'] = len(chunks)\n",
    "        \n",
    "        # Infer source from content markers\n",
    "        for chunk in chunks:\n",
    "            content = chunk.get('content', '')\n",
    "            # Email documents contain investment signal markup\n",
    "            if any(marker in content for marker in ['[TICKER:', '[RATING:', '[PRICE_TARGET:']):\n",
    "                stats['email_chunks'] += 1\n",
    "            else:\n",
    "                stats['api_sec_chunks'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run health check\n",
    "health = check_graph_health(storage_path)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüß¨ Graph Health Metrics:\")\n",
    "print(f\"  üìä Content Coverage:\")\n",
    "print(f\"    Tickers: {', '.join(health['tickers_covered']) if health['tickers_covered'] else 'None'} ({len(health['tickers_covered'])}/4 portfolio holdings)\")\n",
    "\n",
    "print(f\"\\n  üï∏Ô∏è Graph Structure:\")\n",
    "print(f\"    Total entities: {health['total_entities']:,}\")\n",
    "print(f\"    Total relationships: {health['total_relationships']:,}\")\n",
    "if health['total_entities'] > 0:\n",
    "    avg_conn = health['total_relationships'] / health['total_entities']\n",
    "    print(f\"    Avg connections: {avg_conn:.2f}\")\n",
    "\n",
    "print(f\"\\n  üíº Investment Signals:\")\n",
    "print(f\"    BUY signals: {health['buy_signals']}\")\n",
    "print(f\"    SELL signals: {health['sell_signals']}\")\n",
    "print(f\"    Price targets: {health['price_targets']}\")\n",
    "\n",
    "# Run extended stats\n",
    "extended_stats = get_extended_graph_stats(storage_path)\n",
    "\n",
    "print(f\"\\n  üì¶ Graph Storage:\")\n",
    "print(f\"    Total chunks: {extended_stats['chunk_count']:,}\")\n",
    "print(f\"    Email chunks: {extended_stats['email_chunks']:,}\")\n",
    "print(f\"    API/SEC chunks: {extended_stats['api_sec_chunks']:,}\")\n",
    "\n",
    "# Save output to log file if enabled\n",
    "if SAVE_PROCESSING_LOG:\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "    captured_out = output_buffer.getvalue()\n",
    "    captured_err = error_buffer.getvalue()\n",
    "    combined = captured_err + captured_out\n",
    "    print(combined)  # Display in notebook\n",
    "    \n",
    "    # Save to markdown file with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = Path(f'logs/graph_processing_log_{timestamp}.md')\n",
    "    log_file.parent.mkdir(exist_ok=True)\n",
    "    log_file.write_text(\n",
    "        f'# Graph Processing Log\\n'\n",
    "        f'**Timestamp:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\n'\n",
    "        f'```\\n{combined}\\n```\\n'\n",
    "    )\n",
    "    print(f'\\n‚úÖ Log saved: {log_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## üß™ Test: Hybrid Entity Categorization with Qwen2.5-3B\n",
    "\n",
    "**Purpose**: Compare categorization accuracy between keyword-only and hybrid (keyword+LLM) approaches\n",
    "\n",
    "**What this tests**:\n",
    "- Baseline: Fast keyword pattern matching (~1ms per entity)\n",
    "- Enhanced: Confidence scoring to identify ambiguous cases\n",
    "- Hybrid: LLM fallback for low-confidence entities (~40ms per entity)\n",
    "\n",
    "**Prerequisites**:\n",
    "- ‚úÖ Ollama installed with qwen2.5:3b model (optional - degrades gracefully)\n",
    "- ‚úÖ LightRAG graph built (previous cells completed)\n",
    "\n",
    "**Expected runtime**: ~0.5 seconds for 12 sample entities (hybrid mode)\n",
    "\n",
    "**Configuration**: `src/ice_lightrag/graph_categorization.py` - Change `CATEGORIZATION_MODE` to enable hybrid by default -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Purpose: Test hybrid entity categorization with configurable sampling and model selection\n",
    "# Location: ice_building_workflow.ipynb Cell 12 (FIXED VERSION)\n",
    "# Dependencies: graph_categorization.py, entity_categories.py, LightRAG storage\n",
    "\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Force reload of graph_categorization module to pick up new functions\n",
    "import importlib\n",
    "if 'src.ice_lightrag.graph_categorization' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.ice_lightrag.graph_categorization'])\n",
    "\n",
    "# ===== CONFIGURATION: User-editable settings =====\n",
    "RANDOM_SEED = 42  # Set to None for different entities each run, or keep 42 for reproducible testing\n",
    "OLLAMA_MODEL_OVERRIDE = 'qwen2.5:3b'  # Change to use different model (e.g., 'llama3.1:8b', 'qwen3:8b')\n",
    "\n",
    "# ===== SETUP: Imports with error handling =====\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ Entity Categorization Test (4 Modes) - FIXED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure src is in path for notebook context\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    from src.ice_lightrag.graph_categorization import (\n",
    "        categorize_entity,\n",
    "        categorize_entity_with_confidence,\n",
    "        categorize_entity_hybrid,\n",
    "        categorize_entity_llm_only  # NEW: Pure LLM mode\n",
    "    )\n",
    "    from src.ice_lightrag.entity_categories import CATEGORY_DISPLAY_ORDER\n",
    "    print(\"‚úÖ Categorization functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   ‚Üí Ensure previous cells completed successfully\")\n",
    "    print(\"   ‚Üí Check that src/ice_lightrag/graph_categorization.py exists\\n\")\n",
    "    raise\n",
    "\n",
    "# Patch module constant for Ollama model selection\n",
    "import src.ice_lightrag.graph_categorization as graph_cat_module\n",
    "graph_cat_module.OLLAMA_MODEL = OLLAMA_MODEL_OVERRIDE\n",
    "print(f\"üîß Ollama model configured: {OLLAMA_MODEL_OVERRIDE}\\n\")\n",
    "\n",
    "# ===== HEALTH CHECK: Ollama service availability =====\n",
    "def check_ollama_service():\n",
    "    \"\"\"Check if Ollama service is running and configured model is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            # Use exact match to avoid accepting wrong model versions\n",
    "            model_available = any(m.get('name', '') == OLLAMA_MODEL_OVERRIDE for m in models)\n",
    "            return True, model_available\n",
    "        return False, False\n",
    "    except requests.RequestException:\n",
    "        return False, False\n",
    "    except (KeyError, json.JSONDecodeError):\n",
    "        return True, False\n",
    "\n",
    "ollama_running, model_available = check_ollama_service()\n",
    "\n",
    "if ollama_running and model_available:\n",
    "    print(f\"‚úÖ Ollama service running with {OLLAMA_MODEL_OVERRIDE} model\")\n",
    "elif ollama_running:\n",
    "    print(f\"‚ö†Ô∏è  Ollama running but {OLLAMA_MODEL_OVERRIDE} not found\")\n",
    "    print(f\"   ‚Üí Install: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama not running - TEST 3 & 4 will be skipped\")\n",
    "    print(\"   ‚Üí Start Ollama: brew services start ollama (macOS)\\n\")\n",
    "\n",
    "# ===== DATA LOADING: Entities with validation =====\n",
    "storage_path = Path(ice.config.working_dir) / \"vdb_entities.json\"\n",
    "entities_data = {}  # Initialize to prevent NameError if file missing or error occurs\n",
    "\n",
    "if not storage_path.exists():\n",
    "    print(f\"‚ùå Storage file not found: {storage_path}\")\n",
    "    print(\"   ‚Üí Run previous cells to build the knowledge graph first\\n\")\n",
    "    print(\"   ‚ö†Ô∏è  Categorization tests will be skipped\\n\")\n",
    "else:\n",
    "    try:\n",
    "        with open(storage_path) as f:\n",
    "            entities_data = json.load(f)\n",
    "\n",
    "        if not isinstance(entities_data, dict) or 'data' not in entities_data:\n",
    "            print(f\"‚ùå Invalid storage format (expected dict with 'data' key)\")\n",
    "            print(\"   ‚ö†Ô∏è  Invalid storage - tests will be skipped\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Storage error: {e}\")\n",
    "        print(\"   ‚ö†Ô∏è  Tests will be skipped\\n\")\n",
    "\n",
    "entities_list = entities_data.get('data', [])\n",
    "\n",
    "if not isinstance(entities_list, list) or len(entities_list) == 0:\n",
    "    print(f\"‚ùå No entities found in storage\")\n",
    "    print(\"   ‚ö†Ô∏è  No data - tests will be skipped\\n\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(entities_list)} entities from knowledge graph\")\n",
    "\n",
    "# Configurable random sampling\n",
    "if RANDOM_SEED is not None:\n",
    "    random.seed(RANDOM_SEED)\n",
    "    sampling_mode = f\"Reproducible (seed={RANDOM_SEED}, same entities each run)\"\n",
    "else:\n",
    "    sampling_mode = \"Random (different entities each run)\"\n",
    "\n",
    "test_entities = random.sample(entities_list, min(12, len(entities_list)))\n",
    "print(f\"   Sampling mode: {sampling_mode}\")\n",
    "print(f\"   Testing with {len(test_entities)} entities\")\n",
    "print(f\"   üí° To change: Set RANDOM_SEED = None (line 17) or OLLAMA_MODEL_OVERRIDE (line 18)\\n\")\n",
    "\n",
    "# ===== HELPER: Compact result display =====\n",
    "def display_results(results, title, show_confidence=False, show_llm=False):\n",
    "    \"\"\"Display categorization results in compact format\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, (name, category, confidence, used_llm) in enumerate(results, 1):\n",
    "        display_name = name[:40] + \"...\" if len(name) > 40 else name\n",
    "\n",
    "        if show_llm and used_llm:\n",
    "            indicator = \"ü§ñ\"\n",
    "        else:\n",
    "            indicator = \"‚ö°\"\n",
    "\n",
    "        if show_confidence:\n",
    "            print(f\"{i:2d}. {indicator} {display_name:43s} ‚Üí {category:20s} (conf: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(f\"{i:2d}. {display_name:45s} ‚Üí {category}\")\n",
    "\n",
    "    category_counts = Counter(cat for _, cat, _, _ in results)\n",
    "    print(f\"\\nüìä Distribution: {dict(category_counts)}\")\n",
    "\n",
    "# ===== TEST 1: Keyword-Only Baseline =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 1: Keyword-Only Categorization (Baseline)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "keyword_results = []\n",
    "\n",
    "for entity in test_entities:\n",
    "    name = entity.get('entity_name', '')\n",
    "    content = entity.get('content', '')\n",
    "    category = categorize_entity(name, content)\n",
    "    keyword_results.append((name, category, 1.0, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(keyword_results, \"Results (Keyword Matching):\", show_confidence=False)\n",
    "if len(test_entities) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "# ===== TEST 2: Confidence Scoring Analysis =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: Keyword + Confidence Scoring\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "confidence_results = []\n",
    "\n",
    "for entity in test_entities:\n",
    "    name = entity.get('entity_name', '')\n",
    "    content = entity.get('content', '')\n",
    "    category, confidence = categorize_entity_with_confidence(name, content)\n",
    "    confidence_results.append((name, category, confidence, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(confidence_results, \"Results (with confidence scores):\", show_confidence=True)\n",
    "\n",
    "low_confidence = [(n, c, conf) for n, c, conf, _ in confidence_results if conf < 0.70]\n",
    "\n",
    "if low_confidence:\n",
    "    print(f\"\\nüîç Ambiguous entities (confidence < 0.70): {len(low_confidence)}\")\n",
    "    for name, cat, conf in low_confidence:\n",
    "        print(f\"   - {name[:50]:50s} ‚Üí {cat:20s} (conf: {conf:.2f})\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All entities have high confidence (‚â•0.70) - no LLM fallback needed\")\n",
    "\n",
    "if len(test_entities) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "# ===== TEST 3: Hybrid Mode (if Ollama available) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 3: Hybrid Categorization (Keyword + LLM Fallback)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: LLM calls may take 5-10 seconds total...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    hybrid_results = []\n",
    "    llm_call_count = 0\n",
    "\n",
    "    for entity in test_entities:\n",
    "        name = entity.get('entity_name', '')\n",
    "        content = entity.get('content', '')\n",
    "\n",
    "        keyword_cat, keyword_conf = categorize_entity_with_confidence(name, content)\n",
    "\n",
    "        if keyword_conf >= 0.70:\n",
    "            category, confidence = keyword_cat, keyword_conf\n",
    "            used_llm = False\n",
    "        else:\n",
    "            category, confidence = categorize_entity_hybrid(name, content, confidence_threshold=0.70)\n",
    "            used_llm = (confidence == 0.90)\n",
    "            if used_llm:\n",
    "                llm_call_count += 1\n",
    "\n",
    "        hybrid_results.append((name, category, confidence, used_llm))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(hybrid_results, \"Results (Hybrid mode - keyword + LLM):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"\\nü§ñ LLM calls: {llm_call_count}/{len(test_entities)} ({100*llm_call_count/len(test_entities):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\\\nü§ñ LLM calls: 0/0 (no entitys to test)\")\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "    # ===== COMPARISON SUMMARY =====\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Hybrid)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    changes = 0\n",
    "    for i in range(len(test_entities)):\n",
    "        if keyword_results[i][1] != hybrid_results[i][1]:\n",
    "            changes += 1\n",
    "\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"Entities recategorized by LLM: {changes}/{len(test_entities)} ({100*changes/len(test_entities):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No entitys - skipping recategorization analysis\")\n",
    "\n",
    "    if changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_entities)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            hyb_cat = hybrid_results[i][1]\n",
    "            if kw_cat != hyb_cat:\n",
    "                name = test_entities[i].get('entity_name', '')[:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {hyb_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Hybrid categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 3 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable hybrid mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "# ===== TEST 4: Pure LLM Mode (NEW) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 4: Pure LLM Categorization (All entities)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: This will call LLM for ALL entities (may take 30-60 seconds)...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm_results = []\n",
    "\n",
    "    for entity in test_entities:\n",
    "        name = entity.get('entity_name', '')\n",
    "        content = entity.get('content', '')\n",
    "        category, confidence = categorize_entity_llm_only(name, content)\n",
    "        llm_results.append((name, category, confidence, True))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(llm_results, \"Results (Pure LLM mode):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    print(f\"\\nü§ñ LLM calls: {len(test_entities)}/{len(test_entities)} (100%)\")\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_entities):.1f}ms per entity)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no entitys to test)\")\n",
    "\n",
    "    # Compare keyword vs pure LLM (FIXED INDENTATION)\n",
    "    llm_changes = sum(1 for i in range(len(test_entities)) if keyword_results[i][1] != llm_results[i][1])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Pure LLM)\")\n",
    "    print(\"=\" * 70)\n",
    "    if len(test_entities) > 0:\n",
    "        print(f\"Entities recategorized by pure LLM: {llm_changes}/{len(test_entities)} ({100*llm_changes/len(test_entities):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No entitys - skipping recategorization analysis\")\n",
    "\n",
    "    if llm_changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_entities)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            llm_cat = llm_results[i][1]\n",
    "            if kw_cat != llm_cat:\n",
    "                name = test_entities[i].get('entity_name', '')[:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {llm_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Pure LLM categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 4 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable pure LLM mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ TESTING COMPLETE - 4 Categorization Modes Evaluated\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Purpose: Test relationship categorization with configurable sampling and model selection\n",
    "# Location: ice_building_workflow.ipynb (FIXED VERSION)\n",
    "# Dependencies: graph_categorization.py, relationship_categories.py, LightRAG storage\n",
    "\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Force reload of graph_categorization module to pick up new functions\n",
    "import importlib\n",
    "if 'src.ice_lightrag.graph_categorization' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.ice_lightrag.graph_categorization'])\n",
    "\n",
    "# ===== CONFIGURATION: User-editable settings =====\n",
    "RANDOM_SEED = 42  # Set to None for different relationships each run, or keep 42 for reproducible testing\n",
    "OLLAMA_MODEL_OVERRIDE = 'qwen2.5:3b'  # Change to use different model (e.g., 'llama3.1:8b', 'qwen3:8b')\n",
    "\n",
    "# ===== SETUP: Imports with error handling =====\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ Relationship Categorization Test (4 Modes) - FIXED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure src is in path for notebook context\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    from src.ice_lightrag.graph_categorization import (\n",
    "        categorize_relationship,\n",
    "        categorize_relationship_with_confidence,\n",
    "        categorize_relationship_hybrid,\n",
    "        categorize_relationship_llm_only\n",
    "    )\n",
    "    from src.ice_lightrag.relationship_categories import CATEGORY_DISPLAY_ORDER, extract_relationship_types\n",
    "    print(\"‚úÖ Categorization functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   ‚Üí Ensure previous cells completed successfully\")\n",
    "    print(\"   ‚Üí Check that src/ice_lightrag/graph_categorization.py exists\\n\")\n",
    "    raise\n",
    "\n",
    "# Patch module constant for Ollama model selection\n",
    "import src.ice_lightrag.graph_categorization as graph_cat_module\n",
    "graph_cat_module.OLLAMA_MODEL = OLLAMA_MODEL_OVERRIDE\n",
    "print(f\"üîß Ollama model configured: {OLLAMA_MODEL_OVERRIDE}\\n\")\n",
    "\n",
    "# ===== HEALTH CHECK: Ollama service availability =====\n",
    "def check_ollama_service():\n",
    "    \"\"\"Check if Ollama service is running and configured model is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            # Use exact match to avoid accepting wrong model versions\n",
    "            model_available = any(m.get('name', '') == OLLAMA_MODEL_OVERRIDE for m in models)\n",
    "            return True, model_available\n",
    "        return False, False\n",
    "    except requests.RequestException:\n",
    "        return False, False\n",
    "    except (KeyError, json.JSONDecodeError):\n",
    "        return True, False\n",
    "\n",
    "ollama_running, model_available = check_ollama_service()\n",
    "\n",
    "if ollama_running and model_available:\n",
    "    print(f\"‚úÖ Ollama service running with {OLLAMA_MODEL_OVERRIDE} model\")\n",
    "elif ollama_running:\n",
    "    print(f\"‚ö†Ô∏è  Ollama running but {OLLAMA_MODEL_OVERRIDE} not found\")\n",
    "    print(f\"   ‚Üí Install: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama not running - TEST 3 & 4 will be skipped\")\n",
    "    print(\"   ‚Üí Start Ollama: brew services start ollama (macOS)\\n\")\n",
    "\n",
    "# ===== DATA LOADING: Relationships with validation =====\n",
    "storage_path = Path(ice.config.working_dir) / \"vdb_relationships.json\"\n",
    "relationships_data = {}  # Initialize to prevent NameError if file missing or error occurs\n",
    "\n",
    "if not storage_path.exists():\n",
    "    print(f\"‚ùå Storage file not found: {storage_path}\")\n",
    "    print(\"   ‚Üí Run previous cells to build the knowledge graph first\\n\")\n",
    "    print(\"   ‚ö†Ô∏è  Categorization tests will be skipped\\n\")\n",
    "else:\n",
    "    try:\n",
    "        with open(storage_path) as f:\n",
    "            relationships_data = json.load(f)\n",
    "\n",
    "        if not isinstance(relationships_data, dict) or 'data' not in relationships_data:\n",
    "            print(f\"‚ùå Invalid storage format (expected dict with 'data' key)\")\n",
    "            print(\"   ‚ö†Ô∏è  Invalid storage - tests will be skipped\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Storage error: {e}\")\n",
    "        print(\"   ‚ö†Ô∏è  Tests will be skipped\\n\")\n",
    "\n",
    "relationships_list = relationships_data.get('data', [])\n",
    "\n",
    "if not isinstance(relationships_list, list) or len(relationships_list) == 0:\n",
    "    print(f\"‚ùå No relationships found in storage\")\n",
    "    print(\"   ‚ö†Ô∏è  No data - tests will be skipped\\n\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(relationships_list)} relationships from knowledge graph\")\n",
    "\n",
    "# Configurable random sampling\n",
    "if RANDOM_SEED is not None:\n",
    "    random.seed(RANDOM_SEED)\n",
    "    sampling_mode = f\"Reproducible (seed={RANDOM_SEED}, same relationships each run)\"\n",
    "else:\n",
    "    sampling_mode = \"Random (different relationships each run)\"\n",
    "\n",
    "test_relationships = random.sample(relationships_list, min(12, len(relationships_list)))\n",
    "print(f\"   Sampling mode: {sampling_mode}\")\n",
    "print(f\"   Testing with {len(test_relationships)} relationships\")\n",
    "print(f\"   üí° To change: Set RANDOM_SEED = None (line 17) or OLLAMA_MODEL_OVERRIDE (line 18)\\n\")\n",
    "\n",
    "# ===== HELPER: Compact result display =====\n",
    "def display_results(results, title, show_confidence=False, show_llm=False):\n",
    "    \"\"\"Display categorization results in compact format\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, (name, category, confidence, used_llm) in enumerate(results, 1):\n",
    "        display_name = name[:40] + \"...\" if len(name) > 40 else name\n",
    "\n",
    "        if show_llm and used_llm:\n",
    "            indicator = \"ü§ñ\"\n",
    "        else:\n",
    "            indicator = \"‚ö°\"\n",
    "\n",
    "        if show_confidence:\n",
    "            print(f\"{i:2d}. {indicator} {display_name:43s} ‚Üí {category:20s} (conf: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(f\"{i:2d}. {display_name:45s} ‚Üí {category}\")\n",
    "\n",
    "    category_counts = Counter(cat for _, cat, _, _ in results)\n",
    "    print(f\"\\nüìä Distribution: {dict(category_counts)}\")\n",
    "\n",
    "# ===== TEST 1: Keyword-Only Baseline =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 1: Keyword-Only Categorization (Baseline)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "keyword_results = []\n",
    "\n",
    "for relationship in test_relationships:\n",
    "    content = relationship.get('content', '')\n",
    "    rel_types = extract_relationship_types(content)\n",
    "    display_name = rel_types if rel_types else content[:50]\n",
    "    category = categorize_relationship(content)\n",
    "    keyword_results.append((display_name, category, 1.0, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(keyword_results, \"Results (Keyword Matching):\", show_confidence=False)\n",
    "if len(test_relationships) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "# ===== TEST 2: Confidence Scoring Analysis =====\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: Keyword + Confidence Scoring\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "confidence_results = []\n",
    "\n",
    "for relationship in test_relationships:\n",
    "    content = relationship.get('content', '')\n",
    "    rel_types = extract_relationship_types(content)\n",
    "    display_name = rel_types if rel_types else content[:50]\n",
    "    category, confidence = categorize_relationship_with_confidence(content)\n",
    "    confidence_results.append((display_name, category, confidence, False))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "display_results(confidence_results, \"Results (with confidence scores):\", show_confidence=True)\n",
    "\n",
    "low_confidence = [(n, c, conf) for n, c, conf, _ in confidence_results if conf < 0.70]\n",
    "\n",
    "if low_confidence:\n",
    "    print(f\"\\nüîç Ambiguous relationships (confidence < 0.70): {len(low_confidence)}\")\n",
    "    for name, cat, conf in low_confidence:\n",
    "        print(f\"   - {name[:50]:50s} ‚Üí {cat:20s} (conf: {conf:.2f})\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All relationships have high confidence (‚â•0.70) - no LLM fallback needed\")\n",
    "\n",
    "if len(test_relationships) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "# ===== TEST 3: Hybrid Mode (if Ollama available) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 3: Hybrid Categorization (Keyword + LLM Fallback)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: LLM calls may take 5-10 seconds total...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    hybrid_results = []\n",
    "    llm_call_count = 0\n",
    "\n",
    "    for relationship in test_relationships:\n",
    "        content = relationship.get('content', '')\n",
    "        rel_types = extract_relationship_types(content)\n",
    "        display_name = rel_types if rel_types else content[:50]\n",
    "\n",
    "        keyword_cat, keyword_conf = categorize_relationship_with_confidence(content)\n",
    "\n",
    "        if keyword_conf >= 0.70:\n",
    "            category, confidence = keyword_cat, keyword_conf\n",
    "            used_llm = False\n",
    "        else:\n",
    "            category, confidence = categorize_relationship_hybrid(content, confidence_threshold=0.70)\n",
    "            used_llm = (confidence == 0.90)\n",
    "            if used_llm:\n",
    "                llm_call_count += 1\n",
    "\n",
    "        hybrid_results.append((display_name, category, confidence, used_llm))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(hybrid_results, \"Results (Hybrid mode - keyword + LLM):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"\\nü§ñ LLM calls: {llm_call_count}/{len(test_relationships)} ({100*llm_call_count/len(test_relationships):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\\\nü§ñ LLM calls: 0/0 (no relationships to test)\")\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "    # ===== COMPARISON SUMMARY =====\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Hybrid)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    changes = 0\n",
    "    for i in range(len(test_relationships)):\n",
    "        if keyword_results[i][1] != hybrid_results[i][1]:\n",
    "            changes += 1\n",
    "\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"Relationships recategorized by LLM: {changes}/{len(test_relationships)} ({100*changes/len(test_relationships):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No relationships - skipping recategorization analysis\")\n",
    "\n",
    "    if changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_relationships)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            hyb_cat = hybrid_results[i][1]\n",
    "            if kw_cat != hyb_cat:\n",
    "                name = keyword_results[i][0][:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {hyb_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Hybrid categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 3 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable hybrid mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "# ===== TEST 4: Pure LLM Mode (NEW) =====\n",
    "if ollama_running and model_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST 4: Pure LLM Categorization (All relationships)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è±Ô∏è  Note: This will call LLM for ALL relationships (may take 30-60 seconds)...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm_results = []\n",
    "\n",
    "    for relationship in test_relationships:\n",
    "        content = relationship.get('content', '')\n",
    "        rel_types = extract_relationship_types(content)\n",
    "        display_name = rel_types if rel_types else content[:50]\n",
    "        category, confidence = categorize_relationship_llm_only(content)\n",
    "        llm_results.append((display_name, category, confidence, True))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    display_results(llm_results, \"Results (Pure LLM mode):\", show_confidence=True, show_llm=True)\n",
    "\n",
    "    print(f\"\\nü§ñ LLM calls: {len(test_relationships)}/{len(test_relationships)} (100%)\")\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"‚è±Ô∏è  Time: {elapsed*1000:.1f}ms ({elapsed*1000/len(test_relationships):.1f}ms per relationship)\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚è±Ô∏è  Time: {elapsed*1000:.1f}ms (no relationships to test)\")\n",
    "\n",
    "    # Compare keyword vs pure LLM (FIXED INDENTATION)\n",
    "    llm_changes = sum(1 for i in range(len(test_relationships)) if keyword_results[i][1] != llm_results[i][1])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä COMPARISON SUMMARY (Keyword vs Pure LLM)\")\n",
    "    print(\"=\" * 70)\n",
    "    if len(test_relationships) > 0:\n",
    "        print(f\"Relationships recategorized by pure LLM: {llm_changes}/{len(test_relationships)} ({100*llm_changes/len(test_relationships):.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No relationships - skipping recategorization analysis\")\n",
    "\n",
    "    if llm_changes > 0:\n",
    "        print(\"\\nRecategorization details:\")\n",
    "        for i in range(len(test_relationships)):\n",
    "            kw_cat = keyword_results[i][1]\n",
    "            llm_cat = llm_results[i][1]\n",
    "            if kw_cat != llm_cat:\n",
    "                name = keyword_results[i][0][:50]\n",
    "                print(f\"   - {name:50s}: {kw_cat:20s} ‚Üí {llm_cat}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Pure LLM categorization complete!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  TEST 4 SKIPPED: Ollama not available\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"To enable pure LLM mode:\")\n",
    "    print(\"   1. Install Ollama: https://ollama.com\")\n",
    "    print(f\"   2. Pull model: ollama pull {OLLAMA_MODEL_OVERRIDE}\")\n",
    "    print(\"   3. Start service: brew services start ollama (macOS)\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ TESTING COMPLETE - 4 Categorization Modes Evaluated\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ice_data_ingestion.ice_integration:ICE LightRAG system initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LightRAG successfully imported!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ice_data_ingestion.ice_integration:ICE LightRAG system initialized successfully\n",
      "INFO:updated_architectures.implementation.ice_simplified:ICE Core initializing with ICESystemManager orchestration\n",
      "INFO:src.ice_core.ice_system_manager:ICE System Manager initialized with working_dir: ice_lightrag/storage\n",
      "INFO:updated_architectures.implementation.ice_simplified:‚úÖ ICESystemManager initialized successfully\n",
      "INFO:imap_email_ingestion_pipeline.entity_extractor:spaCy model loaded successfully\n",
      "INFO:updated_architectures.implementation.data_ingestion:‚úÖ TickerValidator initialized (false positive filtering)\n",
      "INFO:src.ice_docling.docling_processor:DoclingProcessor initialized: storage=/Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/data/attachments\n",
      "INFO:updated_architectures.implementation.data_ingestion:‚úÖ DoclingProcessor initialized (97.9% table accuracy)\n",
      "INFO:updated_architectures.implementation.data_ingestion:‚úÖ BenzingaClient initialized (real-time professional news)\n",
      "INFO:src.ice_docling.docling_processor:DoclingProcessor initialized: storage=/Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/data/attachments\n",
      "INFO:imap_email_ingestion_pipeline.intelligent_link_processor.IntelligentLinkProcessor:Intelligent Link Processor initialized. Storage path: /Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/data/attachments\n",
      "INFO:imap_email_ingestion_pipeline.intelligent_link_processor.IntelligentLinkProcessor:Rate limiting: 1.0s delay, max 3 concurrent downloads\n",
      "INFO:updated_architectures.implementation.data_ingestion:‚úÖ IntelligentLinkProcessor initialized (hybrid URL fetching) with Docling (97.9% table accuracy)\n",
      "INFO:updated_architectures.implementation.data_ingestion:üóÇÔ∏è  STORAGE PATH RESOLUTION:\n",
      "INFO:updated_architectures.implementation.data_ingestion:   Current file: /Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/updated_architectures/implementation/data_ingestion.py\n",
      "INFO:updated_architectures.implementation.data_ingestion:   Resolved path: /Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/data/attachments\n",
      "INFO:updated_architectures.implementation.data_ingestion:   Path exists: True\n",
      "INFO:updated_architectures.implementation.data_ingestion:   Path writable: True\n",
      "INFO:updated_architectures.implementation.data_ingestion:‚úÖ Unified storage path confirmed: /Users/royyeo/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Capstone Project/data/attachments\n",
      "INFO:updated_architectures.implementation.data_ingestion:Data Ingester initialized with 7 API services: ['newsapi', 'alpha_vantage', 'fmp', 'polygon', 'finnhub', 'benzinga', 'marketaux']\n",
      "INFO:updated_architectures.implementation.data_ingestion:Production modules initialized: SEC EDGAR connector, EntityExtractor, GraphBuilder, AttachmentProcessor, IntelligentLinkProcessor ready\n",
      "INFO:updated_architectures.implementation.signal_store:Signal Store tables created successfully\n",
      "INFO:updated_architectures.implementation.signal_store:Signal Store initialized at data/signal_store/signal_store.db\n",
      "INFO:updated_architectures.implementation.data_ingestion:‚úÖ Signal Store initialized for dual-layer architecture\n",
      "INFO:updated_architectures.implementation.ice_simplified:Query Engine initialized\n",
      "INFO:updated_architectures.implementation.ice_simplified:‚úÖ Query router initialized for dual-layer architecture\n",
      "INFO:updated_architectures.implementation.ice_simplified:‚úÖ ICE Simplified system initialized successfully\n",
      "INFO:src.ice_core.ice_system_manager:LightRAG wrapper created successfully (lazy initialization mode)\n",
      "INFO:src.ice_core.ice_system_manager:Exa MCP connector initialized successfully\n",
      "INFO:src.ice_core.ice_graph_builder:ICE Graph Builder initialized\n",
      "INFO:src.ice_core.ice_graph_builder:LightRAG instance updated in Graph Builder\n",
      "INFO:src.ice_core.ice_system_manager:Graph Builder initialized successfully\n",
      "INFO:src.ice_core.ice_query_processor:ICE Query Processor initialized\n",
      "INFO:src.ice_core.ice_system_manager:Query Processor initialized successfully\n",
      "INFO:updated_architectures.implementation.ice_simplified:System health: ready=True\n",
      "INFO:updated_architectures.implementation.ice_simplified:Components: {'lightrag': True, 'exa_connector': True, 'graph_builder': True, 'query_processor': True, 'data_manager': False}\n",
      "INFO:updated_architectures.implementation.ice_simplified:‚úÖ ICE system created and ready for operations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PRE-DELETION CHECK\n",
      "   Files: 0\n",
      "   Total size: 0.00 MB\n",
      "\n",
      "‚úÖ POST-DELETION CHECK\n",
      "   Storage cleared: ice_lightrag/storage\n",
      "   Files remaining: 0\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  CRITICAL: KERNEL RESTART REQUIRED\n",
      "======================================================================\n",
      "\n",
      "Why restart is needed:\n",
      "  ‚Ä¢ LightRAG maintains document IDs in Python memory\n",
      "  ‚Ä¢ Clearing disk files does NOT clear memory state\n",
      "  ‚Ä¢ Without restart: Documents will be rejected as 'duplicates'\n",
      "  ‚Ä¢ Result: Empty graph even after re-running ingestion\n",
      "\n",
      "üìã Next Steps:\n",
      "  1. Jupyter Menu ‚Üí Kernel ‚Üí Restart Kernel\n",
      "  2. Re-run Cell 3 (Initialize ICE System)\n",
      "  3. Re-run Cell 27 (Data Ingestion)\n",
      "  4. Verify Cell 28 shows documents > 0\n",
      "\n",
      "üí° Alternative (if you don't want to restart):\n",
      "  ‚Ä¢ Just run Cell 27 with REBUILD_GRAPH = False\n",
      "  ‚Ä¢ This skips rebuilding and uses existing graph\n",
      "  ‚Ä¢ Use this for query testing without re-ingesting data\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Graph cleared - RESTART KERNEL before rebuilding\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 9\n",
    "# ## Clear graph storage with clear kernel restart instructions\n",
    "# ## This is the SAFE and ROBUST approach\n",
    "\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "\n",
    "# from updated_architectures.implementation.ice_simplified import create_ice_system\n",
    "# ice = create_ice_system()\n",
    "\n",
    "# ##########################################################\n",
    "# #                  Clear Graph Storage                   #\n",
    "# ##########################################################\n",
    "\n",
    "# # FIX: Reset storage_path to directory (Cell 12 redefined it to file)\n",
    "# storage_path = Path(ice.config.working_dir)\n",
    "\n",
    "# if storage_path.exists():\n",
    "#     print(\"üìä PRE-DELETION CHECK\")\n",
    "\n",
    "#     # Show what will be deleted\n",
    "#     files = list(storage_path.glob(\"*\"))\n",
    "#     total_size = sum(f.stat().st_size for f in files if f.is_file())\n",
    "#     print(f\"   Files: {len(files)}\")\n",
    "#     print(f\"   Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "#     # Delete storage\n",
    "#     shutil.rmtree(storage_path)  # Deletes directory + all contents\n",
    "#     storage_path.mkdir(parents=True, exist_ok=True)  # Re-create empty directory\n",
    "\n",
    "#     print(\"\\n‚úÖ POST-DELETION CHECK\")\n",
    "#     print(f\"   Storage cleared: {storage_path}\")\n",
    "#     print(f\"   Files remaining: {len(list(storage_path.glob('*')))}\")\n",
    "\n",
    "#     # CRITICAL WARNING\n",
    "#     print(\"\\n\" + \"=\" * 70)\n",
    "#     print(\"‚ö†Ô∏è  CRITICAL: KERNEL RESTART REQUIRED\")\n",
    "#     print(\"=\" * 70)\n",
    "#     print(\"\\nWhy restart is needed:\")\n",
    "#     print(\"  ‚Ä¢ LightRAG maintains document IDs in Python memory\")\n",
    "#     print(\"  ‚Ä¢ Clearing disk files does NOT clear memory state\")\n",
    "#     print(\"  ‚Ä¢ Without restart: Documents will be rejected as 'duplicates'\")\n",
    "#     print(\"  ‚Ä¢ Result: Empty graph even after re-running ingestion\")\n",
    "\n",
    "#     print(\"\\nüìã Next Steps:\")\n",
    "#     print(\"  1. Jupyter Menu ‚Üí Kernel ‚Üí Restart Kernel\")\n",
    "#     print(\"  2. Re-run Cell 3 (Initialize ICE System)\")\n",
    "#     print(\"  3. Re-run Cell 27 (Data Ingestion)\")\n",
    "#     print(\"  4. Verify Cell 28 shows documents > 0\")\n",
    "\n",
    "#     print(\"\\nüí° Alternative (if you don't want to restart):\")\n",
    "#     print(\"  ‚Ä¢ Just run Cell 27 with REBUILD_GRAPH = False\")\n",
    "#     print(\"  ‚Ä¢ This skips rebuilding and uses existing graph\")\n",
    "#     print(\"  ‚Ä¢ Use this for query testing without re-ingesting data\")\n",
    "\n",
    "#     print(\"\\n\" + \"=\" * 70)\n",
    "#     print(\"‚úÖ Graph cleared - RESTART KERNEL before rebuilding\")\n",
    "#     print(\"=\" * 70)\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è  Storage path doesn't exist - nothing to clear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Portfolio configuration\n",
    "import pandas as pd\n",
    "\n",
    "# portfolio_df = pd.read_csv('portfolio_holdings.csv')\n",
    "portfolio_df = pd.read_csv('portfolio_holdings_folder/portfolio_holdings_diversified_10.csv')\n",
    "\n",
    "# Basic validation\n",
    "if portfolio_df.empty:\n",
    "    raise ValueError(\"Portfolio CSV is empty\")\n",
    "if 'ticker' not in portfolio_df.columns:\n",
    "    raise ValueError(\"CSV must have 'ticker' column\")\n",
    "\n",
    "holdings = portfolio_df['ticker'].tolist()\n",
    "\n",
    "print(f\"üéØ Portfolio Configuration\")\n",
    "print(f\"‚îÅ\" * 40)\n",
    "print(f\"Holdings: {', '.join(holdings)} ({len(holdings)} stocks)\")\n",
    "print(f\"Sector: {portfolio_df['sector'].iloc[0] if len(portfolio_df) > 0 else 'N/A'}\")\n",
    "print(f\"Data Range: 2 years historical (editable in Cell 21)\")\n",
    "print(f\"üìÑ Source: portfolio_holdings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 3. Data Ingestion & Processing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - Clear data/attachments for testing (OPTIONAL)\n",
    "# Safe, single-line command to clear attachments folder for processor testing\n",
    "# This preserves the main directory structure while removing subdirectories\n",
    "\n",
    "\n",
    "# Alternative: View what would be cleared (safe preview)\n",
    "if os.path.exists('data/attachments'):\n",
    "    subdirs = [d for d in os.listdir('data/attachments') if os.path.isdir(os.path.join('data/attachments', d))]\n",
    "    if subdirs:\n",
    "        print(f\"üìÅ Found {len(subdirs)} subdirectories in data/attachments/:\")\n",
    "        for subdir in subdirs[:5]:  # Show first 5\n",
    "            print(f\"   - {subdir}\")\n",
    "        if len(subdirs) > 5:\n",
    "            print(f\"   ... and {len(subdirs) - 5} more\")\n",
    "    else:\n",
    "        print(\"‚ú® data/attachments/ is already empty\")\n",
    "else:\n",
    "    print(\"üìÅ data/attachments/ does not exist yet\")\n",
    "    \n",
    "    \n",
    "#######################################################################    \n",
    "### CLEAR COMMAND (uncomment to use):\n",
    "# import shutil, os; [shutil.rmtree(os.path.join('data/attachments', d)) for d in os.listdir('data/attachments') if os.path.isdir(os.path.join('data/attachments', d))] if os.path.exists('data/attachments') else None; print(f\"‚úÖ Cleared {len([d for d in os.listdir('data/attachments') if os.path.isdir(os.path.join('data/attachments', d))]) if os.path.exists('data/attachments') else 0} subdirectories from data/attachments/\") if os.path.exists('data/attachments') else print(\"üìÅ data/attachments/ does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "print(\"\\nüìä Data Source Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show ACTUAL metrics if available (not fake percentages)\n",
    "if ice and ice.core.is_ready():\n",
    "    storage_stats = ice.core.get_storage_stats()\n",
    "    print(f\"üíæ Current Graph Size: {storage_stats['total_storage_bytes'] / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Show real source info if ingestion has run\n",
    "    if 'ingestion_result' in locals() and 'metrics' in ingestion_result:\n",
    "        metrics = ingestion_result['metrics']\n",
    "        if 'data_sources_used' in metrics:\n",
    "            print(f\"‚úÖ Active sources: {', '.join(metrics['data_sources_used'])}\")\n",
    "        print(f\"üìÑ Total documents: {ingestion_result.get('total_documents', 0)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Data source metrics available after ingestion completes\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Knowledge graph not ready\")\n",
    "    \n",
    "# Cell 12\n",
    "ice.core.get_graph_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 3b. Data Source Contribution Visualization (Week 5) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "print(\"üìä ICE Data Sources Summary (Phase 1 Integration)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ÑπÔ∏è  Phase 1 focuses on architecture and data flow patterns.\")\n",
    "print(\"Actual data ingestion depends on configured API keys.\\n\")\n",
    "print(\"\\n1Ô∏è‚É£ API/MCP Sources:\")\n",
    "print(\"   - NewsAPI: Real-time financial news\")\n",
    "print(\"   - SEC EDGAR: Regulatory filings (10-K, 10-Q, 8-K)\")\n",
    "print(\"   - Alpha Vantage: Market data\")\n",
    "print(\"\\n2Ô∏è‚É£ Email Pipeline (Phase 1 Enhanced Documents):\")\n",
    "print(\"   - Broker research with BUY/SELL signals\")\n",
    "print(\"   - Enhanced documents: [TICKER:NVDA|confidence:0.95]\")\n",
    "print(\"   - See detailed demo: imap_email_ingestion_pipeline/investment_email_extractor_simple.ipynb\")\n",
    "print(\"\\n3Ô∏è‚É£ SEC Filings:\")\n",
    "print(\"   - Management commentary and financial statements\")\n",
    "print(\"   - Integrated via SEC EDGAR connector\")\n",
    "print(\"\\nüí° All sources ‚Üí Single LightRAG knowledge graph via ice_simplified.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 3a. ICE Data Sources Integration (Week 5)\n",
    "\n",
    "ICE integrates 3 heterogeneous data sources into unified knowledge graph:\n",
    "\n",
    "**Detailed Demonstrations Available**:\n",
    "- üìß **Email Pipeline**: See `imap_email_ingestion_pipeline/investment_email_extractor_simple.ipynb` (25 cells)\n",
    "  - Entity extraction (tickers, ratings, price targets)\n",
    "  - BUY/SELL signal extraction with confidence scores\n",
    "  - Enhanced document creation with inline metadata\n",
    "  \n",
    "- üìä **Quick Summary Below** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 14\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TWO-LAYER DATA SOURCE CONTROL SYSTEM\n",
    "# Layer 1: Source Type Switches (Boolean - Master Kill Switches)\n",
    "# Layer 2: Category Limits (Integer - Granular Control per stock)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "'''\n",
    "Note:\n",
    "- email_limit only works when EMAIL_SELECTOR is set as 'all'; in other words, EMAIL_SELECTOR (except for 'all') will bypass email_limit.\n",
    "- Check precedence: Source switch > Selector > Limit\n",
    "- Remember Crawl4AI disabled = 30-40% success for Tier 3-5 URLs.\n",
    "'''\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LAYER 1: SOURCE TYPE SWITCHES (Master Kill Switches)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "### AQ1\n",
    "\n",
    "email_source_enabled = True      # Controls EmailConnector\n",
    "api_source_enabled = False   # Controls ALL API sources (News, Financial, Market, SEC)\n",
    "mcp_source_enabled = False       # Controls MCP sources (Research/Search via Exa MCP)\n",
    "url_processing_enabled = True    # Controls URL processing in emails (master switch)\n",
    "crawl4ai_enabled = False         # Controls Crawl4AI browser automation for URL fetching\n",
    "\n",
    "# Set environment variables (requires kernel restart to take effect if changed mid-session)\n",
    "import os\n",
    "os.environ['ICE_PROCESS_URLS'] = 'true' if url_processing_enabled else 'false'\n",
    "os.environ['USE_CRAWL4AI_LINKS'] = 'true' if crawl4ai_enabled else 'false'\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LAYER 2: CATEGORY LIMITS (Granular Control - only active when source type enabled)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# # Email category (not per-stock, portfolio-wide)\n",
    "# email_limit = 25                 # Top X latest emails\n",
    "\n",
    "# # API categories (per stock)\n",
    "# news_limit = 2                   # News articles per stock (NewsAPI, Benzinga, Finnhub, MarketAux)\n",
    "# financial_limit = 2              # Financial fundamentals per stock (FMP, Alpha Vantage)\n",
    "# market_limit = 1                 # Market data per stock (Polygon)\n",
    "# sec_limit = 2                    # SEC filings per stock (SEC EDGAR)\n",
    "\n",
    "# # MCP categories (per stock)\n",
    "# research_limit = 0               # Research documents per stock (Exa MCP, on-demand only)\n",
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PORTFOLIO SIZE SELECTOR - Sets holdings + default limits\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "PORTFOLIO_SIZE = 'tiny'  # Options: 'tiny' | 'small' | 'medium' | 'full'\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# VALIDATION: Ensure valid configuration\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Validate PORTFOLIO_SIZE value\n",
    "valid_sizes = ['tiny', 'small', 'medium', 'full']\n",
    "if PORTFOLIO_SIZE not in valid_sizes:\n",
    "    raise ValueError(f\"‚ùå Invalid PORTFOLIO_SIZE='{PORTFOLIO_SIZE}'. Choose from: {', '.join(valid_sizes)}\")\n",
    "\n",
    "# Validate dependency for 'full' portfolio\n",
    "if PORTFOLIO_SIZE == 'full' and 'holdings' not in dir():\n",
    "    raise RuntimeError(\"‚ùå PORTFOLIO_SIZE='full' requires Cell 16 to run first! (Loads holdings from CSV)\")\n",
    "\n",
    "### PORTFOLIO CONFIGURATIONS:\n",
    "portfolios = {\n",
    "    'tiny': {\n",
    "        # 'holdings': ['NVDA', 'AMD'],\n",
    "        'holdings': ['FICO'],\n",
    "        'email_limit': 1, # EMAIL_SELECTOR needs to be set as 'all'\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 1,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 1,\n",
    "        'research_limit': 0\n",
    "    },\n",
    "    'small': {\n",
    "        'holdings': ['NVDA', 'AMD'],\n",
    "        'email_limit': 25, # EMAIL_SELECTOR needs to be set as 'all'\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 2,\n",
    "        'research_limit': 0\n",
    "    },\n",
    "    'medium': {\n",
    "        'holdings': ['NVDA', 'AMD', 'TSMC'],\n",
    "        'email_limit': 50, # EMAIL_SELECTOR needs to be set as 'all'\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 3,\n",
    "        'research_limit': 0\n",
    "    },\n",
    "    'full': {\n",
    "        'holdings': holdings,  # ‚Üê Use ALL stocks from CSV (Cell 16)\n",
    "        'email_limit': 71, # EMAIL_SELECTOR needs to be set as 'all'\n",
    "        'news_limit': 2,\n",
    "        'financial_limit': 2,\n",
    "        'market_limit': 1,\n",
    "        'sec_limit': 3,\n",
    "        'research_limit': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "portfolio = portfolios[PORTFOLIO_SIZE]\n",
    "\n",
    "# Set ALL variables (holdings + 6 category limits)\n",
    "test_holdings = portfolio['holdings']\n",
    "email_limit = portfolio['email_limit']\n",
    "news_limit = portfolio['news_limit']\n",
    "financial_limit = portfolio['financial_limit']\n",
    "market_limit = portfolio['market_limit']\n",
    "sec_limit = portfolio['sec_limit']\n",
    "research_limit = portfolio['research_limit']\n",
    "\n",
    "print(f\"üìä {PORTFOLIO_SIZE.upper()} Portfolio Selected\")\n",
    "print(f\"{'‚ïê'*70}\")\n",
    "print(f\"Holdings: {', '.join(test_holdings)} ({len(test_holdings)} stocks)\")\n",
    "print(f\"\\nüì¶ Default Category Limits (can be overridden in Cell 26):\")\n",
    "print(f\"  üìß Email: {email_limit} broker research emails\")\n",
    "print(f\"  üì∞ News: {news_limit}/stock\")\n",
    "print(f\"  üíπ Financial: {financial_limit}/stock\")\n",
    "print(f\"  üìà Market: {market_limit}/stock\")\n",
    "print(f\"  üìë SEC: {sec_limit}/stock\")\n",
    "print(f\"  üî¨ Research: {research_limit}/stock\")\n",
    "\n",
    "estimated = (\n",
    "    email_limit + \n",
    "    len(test_holdings) * (news_limit + financial_limit + market_limit + sec_limit + research_limit)\n",
    ")\n",
    "print(f\"\\nüìä Estimated Docs: ~{estimated} (before source switches in Cell 26)\")\n",
    "print(f\"{'‚ïê'*70}\")\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SPECIAL SELECTOR: EMAIL_SELECTOR (Email Category Only)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# EMAIL_SELECTOR precedence:\n",
    "# - If 'all' ‚Üí use email_limit\n",
    "# - If specific selector ‚Üí bypass email_limit, use email_files_to_process\n",
    "\n",
    "# (EMAIL_SELECTOR defined earlier in notebook, typically 'all')\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# EMAIL SELECTOR - Choose which emails to process\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "EMAIL_SELECTOR = 'crawl4ai_test'  # Options: 'all' | 'crawl4ai_test' | 'docling_test' | 'custom'\n",
    "# EMAIL_SELECTOR = 'html_table_test'  # Options: 'all' | 'crawl4ai_test' | 'docling_test' | 'custom'\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# VALIDATION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Validate EMAIL_SELECTOR\n",
    "valid_email = ['all', 'html_table_test', 'crawl4ai_test', 'docling_test', 'custom']\n",
    "if EMAIL_SELECTOR not in valid_email:\n",
    "    raise ValueError(f\"‚ùå Invalid EMAIL_SELECTOR='{EMAIL_SELECTOR}'. Choose from: {', '.join(valid_email)}\")\n",
    "\n",
    "email_sets = {\n",
    "    'all': {\n",
    "        'email_files': None,  # Process all 71 emails\n",
    "        'description': 'All 71 sample emails'\n",
    "    },\n",
    "    'html_table_test': {\n",
    "        'email_files': [\n",
    "            'FW_ RHB | Singapore Morning Cuppa _ 15 August 2025 (ST Engineering, First Resources, Golden Agri-Resources, StarHub).eml'\n",
    "        ],\n",
    "        'description': 'Emails with HTML tables.'\n",
    "    },\n",
    "    'crawl4ai_test': {\n",
    "        'email_files': [\n",
    "            # 'CH_HK_ Nongfu Spring Co. Ltd (9633 HK)_ Leading the pack (NOT RATED).eml',\n",
    "            'CH_HK_ Tencent Music Entertainment (1698 HK)_ Stronger growth with expanding revenue streams  (NOT RATED).eml',\n",
    "            'DBS SALES SCOOP (14 AUG 2025)_ TENCENT | UOL.eml',\n",
    "        ],\n",
    "        'description': 'Emails with URLs (tests link processing)'\n",
    "    },\n",
    "    'docling_test': {\n",
    "        'email_files': [\n",
    "            # 'Yupi Indo IPO calculations.eml',\n",
    "            # 'CGSI Futuristic Tour 2.0 Shenzhen & Guangzhou 14-15 April 2025.eml',\n",
    "            # 'DBS Economics & Strategy_ Macro Strategy_ Fed noise; Singapore GDP; lower USD.eml',\n",
    "            # 'CGS Global AI & Robotic Conference 2025 - Hangzhou_ 27 March 2025 | some key takeaways from Supermarket _ Sports retailers (Anta, Li Ning, 361, Xtep).eml',\n",
    "            # 'DBS Economics & Strategy_ China_ Capacity reduction campaign weighs on activity.eml',\n",
    "            'Tencent Q2 2025 Earnings.eml',\n",
    "            # 'BABA Q1 2026 June Qtr Earnings.eml',\n",
    "        ],\n",
    "        'description': 'Emails for testing... (tests Docling PDF/Excel/image processing)'\n",
    "    },\n",
    "    'crawl4ai_docling_test': {\n",
    "        'email_files': [\n",
    "            'CGSI Futuristic Tour 2.0 Shenzhen & Guangzhou 14-15 April 2025.eml',\n",
    "            # 'Yupi Indo IPO calculations.eml',\n",
    "        ],\n",
    "        'description': 'Emails for testing... PDF documents contained in the embedded link in the email.'\n",
    "    },\n",
    "    \n",
    "    'custom': {\n",
    "        'email_files': [\n",
    "            # Add your custom email files here\n",
    "            # Example: 'your_email.eml'\n",
    "        ],\n",
    "        'description': 'Custom email selection'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get selected email set\n",
    "selected_emails = email_sets[EMAIL_SELECTOR]\n",
    "email_files_to_process = selected_emails['email_files']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EMAIL SELECTOR: {EMAIL_SELECTOR}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Description: {selected_emails['description']}\")\n",
    "if email_files_to_process:\n",
    "    print(f\"Emails to process: {len(email_files_to_process)}\")\n",
    "    for i, email_file in enumerate(email_files_to_process, 1):\n",
    "        print(f\"  {i}. {email_file}\")\n",
    "else:\n",
    "    print(f\"Emails to process: All (up to email_limit)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PRECEDENCE HIERARCHY (Highest to Lowest):\n",
    "# 1. Source Type Switch (email_source_enabled, api_source_enabled, mcp_source_enabled)\n",
    "# 2. Category Limit (news_limit, financial_limit, market_limit, sec_limit, research_limit)\n",
    "# 3. Special Selector (EMAIL_SELECTOR for emails only)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Apply precedence: Source type disabled ‚Üí override all category limits to 0\n",
    "if not email_source_enabled:\n",
    "    email_limit = 0\n",
    "    email_files_to_process = None  # Clear selector too\n",
    "\n",
    "if not api_source_enabled:\n",
    "    news_limit = 0\n",
    "    financial_limit = 0\n",
    "    market_limit = 0\n",
    "    sec_limit = 0\n",
    "\n",
    "if not mcp_source_enabled:\n",
    "    research_limit = 0\n",
    "\n",
    "# Email special case: EMAIL_SELECTOR can bypass email_limit (but not email_source_enabled)\n",
    "if email_source_enabled:\n",
    "    if EMAIL_SELECTOR == 'all':\n",
    "        # Use email_limit for 'all' mode\n",
    "        actual_email_count = email_limit\n",
    "        email_display = f\"{email_limit} emails (up to limit)\"\n",
    "    else:\n",
    "        # Specific selector bypasses email_limit\n",
    "        actual_email_count = len(email_files_to_process) if email_files_to_process else 0\n",
    "        email_display = f\"{actual_email_count} specific files (EMAIL_SELECTOR ignores email_limit)\"\n",
    "else:\n",
    "    # Source disabled - override everything\n",
    "    actual_email_count = 0\n",
    "    email_display = \"0 (source disabled)\"\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DISPLAY: Show configuration after all precedence rules applied\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DATA SOURCE CONFIGURATION (Two-Layer Control)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"LAYER 1: Source Type Switches\")\n",
    "print(f\"  {'‚úÖ' if email_source_enabled else '‚ùå'} Email Source\")\n",
    "print(f\"  {'‚úÖ' if api_source_enabled else '‚ùå'} API Source (News + Financial + Market + SEC)\")\n",
    "print(f\"  {'‚úÖ' if mcp_source_enabled else '‚ùå'} MCP Source (Research/Search)\")\n",
    "print(f\"  {'‚úÖ' if crawl4ai_enabled else '‚ùå'} Crawl4AI (Browser Automation for URLs)\")\n",
    "if crawl4ai_enabled:\n",
    "    print(f\"      ‚Üí Tier 3-5 URLs use browser automation (60-80% success rate)\")\n",
    "else:\n",
    "    print(f\"      ‚Üí Tier 3-5 URLs use simple HTTP only (30-40% success rate)\")\n",
    "\n",
    "\n",
    "print(f\"\\nLAYER 2: Category Limits (Active Categories Only)\\n\")\n",
    "\n",
    "# Category 1: Email\n",
    "print(f\"  {'‚úÖ' if email_source_enabled and actual_email_count > 0 else '‚ùå'} Email: {email_display}\")\n",
    "\n",
    "# Categories 2-5: API sources\n",
    "if api_source_enabled:\n",
    "    print(f\"  {'‚úÖ' if news_limit > 0 else '‚ùå'} News: {news_limit}/stock (NewsAPI, Benzinga, Finnhub, MarketAux)\")\n",
    "    print(f\"  {'‚úÖ' if financial_limit > 0 else '‚ùå'} Financial: {financial_limit}/stock (FMP, Alpha Vantage)\")\n",
    "    print(f\"  {'‚úÖ' if market_limit > 0 else '‚ùå'} Market: {market_limit}/stock (Polygon)\")\n",
    "    print(f\"  {'‚úÖ' if sec_limit > 0 else '‚ùå'} SEC: {sec_limit}/stock (SEC EDGAR)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå News: 0/stock (API source disabled)\")\n",
    "    print(f\"  ‚ùå Financial: 0/stock (API source disabled)\")\n",
    "    print(f\"  ‚ùå Market: 0/stock (API source disabled)\")\n",
    "    print(f\"  ‚ùå SEC: 0/stock (API source disabled)\")\n",
    "\n",
    "# Category 6: MCP source\n",
    "if mcp_source_enabled:\n",
    "    print(f\"  {'‚úÖ' if research_limit > 0 else '‚ùå'} Research: {research_limit}/stock (Exa MCP)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Research: 0/stock (MCP source disabled)\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CALCULATE: Estimated documents (after ALL overrides and precedence)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "estimated_docs = (\n",
    "    actual_email_count +\n",
    "    len(test_holdings) * news_limit +\n",
    "    len(test_holdings) * financial_limit +\n",
    "    len(test_holdings) * market_limit +\n",
    "    len(test_holdings) * sec_limit +\n",
    "    len(test_holdings) * research_limit\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä FINAL Estimated Documents: {estimated_docs}\")\n",
    "print(f\"  - Email: {actual_email_count}\")\n",
    "print(f\"  - News: {len(test_holdings)} tickers √ó {news_limit} = {len(test_holdings) * news_limit}\")\n",
    "print(f\"  - Financial: {len(test_holdings)} tickers √ó {financial_limit} = {len(test_holdings) * financial_limit}\")\n",
    "print(f\"  - Market: {len(test_holdings)} tickers √ó {market_limit} = {len(test_holdings) * market_limit}\")\n",
    "print(f\"  - SEC: {len(test_holdings)} tickers √ó {sec_limit} = {len(test_holdings) * sec_limit}\")\n",
    "print(f\"  - Research: {len(test_holdings)} tickers √ó {research_limit} = {len(test_holdings) * research_limit}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  NOTE: Changing crawl4ai_enabled requires KERNEL RESTART\")\n",
    "print(f\"   ‚Ä¢ Current: USE_CRAWL4AI_LINKS={os.getenv('USE_CRAWL4AI_LINKS', 'false')}\")\n",
    "print(f\"   ‚Ä¢ Environment variables are read at kernel start\")\n",
    "print(f\"   ‚Ä¢ To apply changes: Kernel ‚Üí Restart & Run All\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED DIAGNOSTIC: Check PDFs in correct unified storage location\n",
    "# Added 2025-11-04: Replaces legacy diagnostic cells that checked wrong directory\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÅ URL PDF STORAGE VERIFICATION (FIXED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check correct storage location using hierarchical structure\n",
    "storage_dir = Path(\"data/attachments\")\n",
    "\n",
    "if storage_dir.exists():\n",
    "    # Count ALL PDFs in unified storage (email attachments + URL PDFs)\n",
    "    all_pdfs = list(storage_dir.glob(\"*/*/original/*.pdf\"))\n",
    "    print(f\"\\n‚úÖ Storage directory exists: {storage_dir}\")\n",
    "    print(f\"üìä Total PDFs in unified storage: {len(all_pdfs)}\")\n",
    "    \n",
    "    # Show recent PDFs (last 10 minutes)\n",
    "    recent_cutoff = datetime.now() - timedelta(minutes=10)\n",
    "    recent_pdfs = [f for f in all_pdfs if datetime.fromtimestamp(f.stat().st_mtime) > recent_cutoff]\n",
    "    \n",
    "    if recent_pdfs:\n",
    "        print(f\"\\nüÜï Recent PDFs (last 10 min): {len(recent_pdfs)}\")\n",
    "        for pdf in recent_pdfs[:5]:  # Show first 5\n",
    "            email_uid = pdf.parts[-4]  # Get email UID from path\n",
    "            file_hash = pdf.parts[-3]  # Get file hash\n",
    "            print(f\"   ‚Ä¢ {pdf.name[:50]}... ({pdf.stat().st_size/1024:.1f} KB)\")\n",
    "            print(f\"     Email: {email_uid[:30]}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚ÑπÔ∏è  No PDFs downloaded in last 10 minutes\")\n",
    "        print(f\"   (Existing PDFs may be from previous runs)\")\n",
    "    \n",
    "    # Check metadata.json files to distinguish sources\n",
    "    metadata_files = list(storage_dir.glob(\"*/*/metadata.json\"))\n",
    "    url_pdf_count = 0\n",
    "    email_attachment_count = 0\n",
    "    \n",
    "    for meta_path in metadata_files:\n",
    "        try:\n",
    "            import json\n",
    "            with open(meta_path) as f:\n",
    "                metadata = json.load(f)\n",
    "                if metadata.get('source_type') == 'url_pdf':\n",
    "                    url_pdf_count += 1\n",
    "                elif metadata.get('source_type') == 'email_attachment':\n",
    "                    email_attachment_count += 1\n",
    "        except:\n",
    "            pass  # Skip if metadata read fails\n",
    "    \n",
    "    print(f\"\\nüìà Source Breakdown (from metadata.json):\")\n",
    "    print(f\"   ‚Ä¢ URL PDFs: {url_pdf_count}\")\n",
    "    print(f\"   ‚Ä¢ Email Attachments: {email_attachment_count}\")\n",
    "    print(f\"   ‚Ä¢ Unknown/No metadata: {len(all_pdfs) - url_pdf_count - email_attachment_count}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Storage directory not found: {storage_dir}\")\n",
    "    print(f\"   Run data ingestion first to create storage and download PDFs\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14.5 - Clear Link Cache (Optional)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PURPOSE: Force fresh downloads by clearing URL cache\n",
    "# WHEN TO USE:\n",
    "#   - Testing crawl4ai_test emails that were previously processed\n",
    "#   - URLs already cached ‚Üí won't save to email-specific folders\n",
    "#   - Want to verify URL processing works end-to-end\n",
    "# \n",
    "# ISSUE BACKGROUND:\n",
    "#   Cached URLs return immediately without saving to data/attachments/{email_uid}/\n",
    "#   This is a known caching architecture issue (see tmp/tmp_cache_diagnostic_report.md)\n",
    "# \n",
    "# SAFE TO RUN: Shows statistics before deletion, only clears link cache\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path('data/link_cache')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üóëÔ∏è  LINK CACHE CLEARING UTILITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if cache_dir.exists():\n",
    "    # Pre-deletion statistics\n",
    "    cache_files = list(cache_dir.glob('*.json'))\n",
    "    index_file = cache_dir / 'link_cache_index.json'\n",
    "    \n",
    "    if index_file.exists():\n",
    "        cache_files_count = len(cache_files) - 1  # Exclude index file\n",
    "    else:\n",
    "        cache_files_count = len(cache_files)\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n",
    "    \n",
    "    print(f\"\\nüìä PRE-DELETION STATISTICS:\")\n",
    "    print(f\"   Cache directory: {cache_dir}\")\n",
    "    print(f\"   Cached URLs: {cache_files_count}\")\n",
    "    print(f\"   Total cache size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Show most recent cached URLs (top 5)\n",
    "    if index_file.exists():\n",
    "        import json\n",
    "        with open(index_file, 'r') as f:\n",
    "            cache_index = json.load(f)\n",
    "        \n",
    "        if cache_index:\n",
    "            print(f\"\\nüìã RECENT CACHED URLs (last 5):\")\n",
    "            sorted_entries = sorted(\n",
    "                cache_index.items(), \n",
    "                key=lambda x: x[1].get('cached_time', ''), \n",
    "                reverse=True\n",
    "            )[:5]\n",
    "            \n",
    "            for i, (url_hash, entry) in enumerate(sorted_entries, 1):\n",
    "                url = entry.get('url', 'Unknown')\n",
    "                cached_time = entry.get('cached_time', 'Unknown')[:19]  # Truncate to date+time\n",
    "                size_kb = entry.get('file_size', 0) / 1024\n",
    "                \n",
    "                # Truncate URL for display\n",
    "                url_display = url if len(url) <= 60 else url[:57] + '...'\n",
    "                \n",
    "                print(f\"   {i}. {url_display}\")\n",
    "                print(f\"      Cached: {cached_time} | Size: {size_kb:.1f} KB\")\n",
    "    \n",
    "    # Delete cache\n",
    "    print(f\"\\nüóëÔ∏è  DELETING CACHE...\")\n",
    "    shutil.rmtree(cache_dir)\n",
    "    \n",
    "    print(f\"\\n‚úÖ POST-DELETION CHECK:\")\n",
    "    print(f\"   Cache directory: {cache_dir}\")\n",
    "    print(f\"   Exists: {cache_dir.exists()}\")\n",
    "    print(f\"   Status: Cache cleared successfully\")\n",
    "    \n",
    "    print(f\"\\nüí° NEXT STEPS:\")\n",
    "    print(f\"   1. Run Cell 15 to process emails\")\n",
    "    print(f\"   2. URLs will be downloaded fresh (not from cache)\")\n",
    "    print(f\"   3. Files will be saved to data/attachments/{{email_uid}}/\")\n",
    "    print(f\"   4. Cache will be rebuilt as URLs are processed\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cache directory does not exist: {cache_dir}\")\n",
    "    print(f\"   Status: No cache to clear\")\n",
    "    print(f\"   This is normal if:\")\n",
    "    print(f\"      - First time running notebook\")\n",
    "    print(f\"      - Cache was already cleared\")\n",
    "    print(f\"      - No URL processing has occurred yet\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Cache clearing complete!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14.6 - Clear Content Cache (HIGHLY RECOMMENDED)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PURPOSE: Force fresh email processing by clearing content cache\n",
    "# WHY: Content cache persists across runs and can lock in stale results\n",
    "# WHEN: Run before REBUILD_GRAPH=True for guaranteed fresh processing\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "content_cache_dir = Path('./data/content_cache')\n",
    "\n",
    "print(\"üóëÔ∏è  CONTENT CACHE CLEARING UTILITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if content_cache_dir.exists():\n",
    "    cache_files = list(content_cache_dir.glob('*.cache'))\n",
    "    index_file = content_cache_dir / 'cache_index.json'\n",
    "    \n",
    "    if index_file.exists():\n",
    "        cache_count = len(cache_files)\n",
    "        total_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n",
    "        \n",
    "        print(f\"   Cache directory: {content_cache_dir}\")\n",
    "        print(f\"   Cached items: {cache_count}\")\n",
    "        print(f\"   Total cache size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "        print()\n",
    "        print(\"üóëÔ∏è  DELETING CACHE...\")\n",
    "        \n",
    "        shutil.rmtree(content_cache_dir)\n",
    "        content_cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"   ‚úÖ Content cache cleared successfully\")\n",
    "        print()\n",
    "        print(\"üìå WHAT THIS MEANS:\")\n",
    "        print(\"   1. Email bodies will be re-processed (not from cache)\")\n",
    "        print(\"   2. Entity extraction will run fresh\")\n",
    "        print(\"   3. Cache will be rebuilt as emails are processed\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  Content cache is already empty\")\n",
    "else:\n",
    "    content_cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"   ‚ÑπÔ∏è  Content cache directory created\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Cache clearing complete!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14.7 - Configure LLM Temperature (IMPORTANT)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PURPOSE: Control LLM randomness for reproducible results\n",
    "# UPDATED 2025-11-08: Separate temperatures for entity extraction & query answering\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import os\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SEPARATE TEMPERATURE CONFIGURATION (New System)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Entity Extraction Temperature (used during document ingestion)\n",
    "# Lower temperature = more reproducible knowledge graphs\n",
    "# RECOMMENDED: ‚â§0.2 for investment intelligence (consistent graphs for backtesting)\n",
    "ENTITY_EXTRACTION_TEMP = 0.2\n",
    "\n",
    "# Query Answering Temperature (used during query processing)\n",
    "# Higher temperature = more creative synthesis\n",
    "# RECOMMENDED: 0.3-0.7 for balanced insights (factual yet creative)\n",
    "QUERY_ANSWERING_TEMP = 0.0\n",
    "\n",
    "# Set environment variables for this session\n",
    "os.environ['ICE_LLM_TEMPERATURE_ENTITY_EXTRACTION'] = str(ENTITY_EXTRACTION_TEMP)\n",
    "os.environ['ICE_LLM_TEMPERATURE_QUERY_ANSWERING'] = str(QUERY_ANSWERING_TEMP)\n",
    "\n",
    "# Display configuration\n",
    "print(\"üå°Ô∏è  LLM TEMPERATURE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Entity Extraction: {ENTITY_EXTRACTION_TEMP}\")\n",
    "print(f\"   Query Answering:   {QUERY_ANSWERING_TEMP}\")\n",
    "print()\n",
    "print(\"üìå WHAT THIS AFFECTS:\")\n",
    "print(\"   ‚úÖ Entity extraction: Uses ENTITY_EXTRACTION_TEMP during graph building\")\n",
    "print(\"   ‚úÖ Query answering: Uses QUERY_ANSWERING_TEMP during queries\")\n",
    "print()\n",
    "print(\"üéØ ENTITY EXTRACTION TEMPERATURE GUIDE:\")\n",
    "print(\"   ‚Ä¢ 0.0-0.2: Deterministic (RECOMMENDED for investment intelligence)\")\n",
    "print(\"     - Same document ‚Üí same entities (reproducible graphs)\")\n",
    "print(\"     - Required for backtesting, compliance, decision validation\")\n",
    "print(\"   ‚Ä¢ 0.3-0.5: Moderate creativity\")\n",
    "print(\"     - Richer entity extraction but may lose reproducibility\")\n",
    "print(\"   ‚Ä¢ >0.5: High randomness (NOT RECOMMENDED)\")\n",
    "print(\"     - Inconsistent graphs, breaks backtesting\")\n",
    "print()\n",
    "print(\"üéØ QUERY ANSWERING TEMPERATURE GUIDE:\")\n",
    "print(\"   ‚Ä¢ 0.0-0.2: Conservative\")\n",
    "print(\"     - Factual, grounded answers, but may be too literal\")\n",
    "print(\"   ‚Ä¢ 0.3-0.7: Balanced (RECOMMENDED)\")\n",
    "print(\"     - Creative synthesis while staying grounded\")\n",
    "print(\"   ‚Ä¢ >0.7: Very creative\")\n",
    "print(\"     - Insightful but may be less consistent\")\n",
    "print()\n",
    "print(\"üí° TO CHANGE: Modify ENTITY_EXTRACTION_TEMP and QUERY_ANSWERING_TEMP\")\n",
    "print(\"   values above, then re-run this cell\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚úÖ Entity extraction temperature set to {ENTITY_EXTRACTION_TEMP}\")\n",
    "print(f\"‚úÖ Query answering temperature set to {QUERY_ANSWERING_TEMP}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:  == LLM cache == saving: default:extract:d4b884e5dd4eee6b92df1688a66233a8\n",
      "INFO: Chunk 40 of 84 extracted 13 Ent + 13 Rel chunk-c490a7d9095d762b4d0899e2d023077d\n"
     ]
    }
   ],
   "source": [
    "# Cell 15\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONFIGURATION: Set to False to skip graph building and use existing graph\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "REBUILD_GRAPH = True\n",
    "# REBUILD_GRAPH = False\n",
    "\n",
    "# Stale-graph detection: Warn if extraction code changed since last build\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Monitor multiple extraction pipeline files for changes\n",
    "files_to_monitor = [\n",
    "    'imap_email_ingestion_pipeline/table_entity_extractor.py',\n",
    "    'imap_email_ingestion_pipeline/enhanced_doc_creator.py'\n",
    "]\n",
    "combined_hash = hashlib.md5()\n",
    "for f in files_to_monitor:\n",
    "    if Path(f).exists():\n",
    "        combined_hash.update(Path(f).read_bytes())\n",
    "current_hash = combined_hash.hexdigest()[:8]\n",
    "\n",
    "version_file = Path('ice_lightrag/storage/.extractor_version')\n",
    "if version_file.exists() and version_file.read_text().strip() != current_hash and not REBUILD_GRAPH:\n",
    "    print(\"‚ö†Ô∏è  STALE GRAPH DETECTED\")\n",
    "    print(f\"   Extraction code has changed since last graph build.\")\n",
    "    print(f\"   Files monitored: table_entity_extractor.py, enhanced_doc_creator.py\")\n",
    "    print(f\"   ‚Üí Set REBUILD_GRAPH=True to get latest extraction fixes!\")\n",
    "    print(f\"   ‚Üí IMPORTANT: Restart kernel (Kernel ‚Üí Restart) before rebuilding!\\n\")\n",
    "\n",
    "if REBUILD_GRAPH:\n",
    "    # Execute data ingestion\n",
    "    # NOTE: This operation may take several minutes. If it hangs, restart kernel.\n",
    "    print(f\"\\nüì• Fetching Portfolio Data\")\n",
    "    print(f\"‚îÅ\" * 50)\n",
    "\n",
    "    if not (ice and ice.is_ready()):\n",
    "        raise RuntimeError(\"ICE system not ready for data ingestion\")\n",
    "\n",
    "    # Fetch historical data (1 year for faster processing - adjust years parameter as needed)\n",
    "    print(f\"üîÑ Fetching data for {len(test_holdings)} holdings...\")\n",
    "    ingestion_result = ice.ingest_historical_data(\n",
    "        test_holdings, \n",
    "        years=1,\n",
    "        email_limit=email_limit,\n",
    "        news_limit=news_limit,\n",
    "        financial_limit=financial_limit,\n",
    "        market_limit=market_limit,\n",
    "        sec_limit=sec_limit,\n",
    "        research_limit=research_limit,\n",
    "        email_files=email_files_to_process if email_source_enabled else None\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nüìä Ingestion Results:\")\n",
    "    print(f\"  Status: {ingestion_result['status']}\")\n",
    "    print(f\"  Holdings: {len(ingestion_result['holdings_processed'])}/{len(test_holdings)}\")\n",
    "    print(f\"  Documents: {ingestion_result['total_documents']}\")\n",
    "\n",
    "    # Show successful holdings\n",
    "    if ingestion_result['holdings_processed']:\n",
    "        print(f\"  ‚úÖ Successful: {', '.join(ingestion_result['holdings_processed'])}\")\n",
    "\n",
    "    # Show metrics\n",
    "    if 'metrics' in ingestion_result:\n",
    "        print(f\"\\n‚è±Ô∏è  Processing Time: {ingestion_result['metrics']['processing_time']:.2f}s\")\n",
    "        if 'data_sources_used' in ingestion_result['metrics']:\n",
    "            print(f\"  Data Sources: {', '.join(ingestion_result['metrics']['data_sources_used'])}\")\n",
    "        \n",
    "        # Display investment signals from Phase 2.6.1 EntityExtractor\n",
    "        if 'investment_signals' in ingestion_result['metrics']:\n",
    "            signals = ingestion_result['metrics']['investment_signals']\n",
    "            print(f\"\\nüìß Investment Signals Captured:\")\n",
    "            print(f\"  Broker emails: {signals['email_count']}\")\n",
    "            print(f\"  Tickers covered: {signals['tickers_covered']}\")\n",
    "            print(f\"  BUY ratings: {signals['buy_ratings']}\")\n",
    "            print(f\"  SELL ratings: {signals['sell_ratings']}\")\n",
    "            print(f\"  Avg confidence: {signals['avg_confidence']:.2f}\")\n",
    "\n",
    "        # Document Source Breakdown\n",
    "        if 'metrics' in ingestion_result and 'investment_signals' in ingestion_result['metrics']:\n",
    "            signals = ingestion_result['metrics']['investment_signals']\n",
    "            email_count = signals['email_count']\n",
    "            \n",
    "            # Parse remaining document types from total\n",
    "            total_docs = ingestion_result.get('total_documents', 0)\n",
    "            api_sec_count = total_docs - email_count\n",
    "            \n",
    "            print(f\"\\nüìÇ Document Source Breakdown:\")\n",
    "            print(f\"  üìß Email (broker research): {email_count} documents\")\n",
    "            print(f\"  üåê API + SEC (market data): {api_sec_count} documents\")\n",
    "            print(f\"  üìä Total documents: {total_docs}\")\n",
    "\n",
    "    # Show failures if any\n",
    "    if ingestion_result.get('failed_holdings'):\n",
    "        print(f\"\\n‚ùå Failed Holdings:\")\n",
    "        for failure in ingestion_result['failed_holdings']:\n",
    "            print(f\"  {failure['symbol']}: {failure['error']}\")\n",
    "else:\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # STALENESS WARNING: Alert user if selectors changed\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  REBUILD_GRAPH = False\")\n",
    "    print(\"‚ö†Ô∏è  Using existing graph - NOT rebuilding with current selectors!\")\n",
    "    print(\"‚ö†Ô∏è  If you changed PORTFOLIO/EMAIL/SOURCE configuration,\")\n",
    "    print(\"‚ö†Ô∏è  set REBUILD_GRAPH=True to avoid querying STALE DATA!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(\"Using existing graph from: ice_lightrag/storage/\")\n",
    "    print(\"To rebuild, set REBUILD_GRAPH = True above and re-run this cell\")\n",
    "    \n",
    "    # Create mock ingestion_result for downstream cells\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    doc_count = 0\n",
    "    if Path('ice_lightrag/storage/kv_store_doc_status.json').exists():\n",
    "        with open('ice_lightrag/storage/kv_store_doc_status.json') as f:\n",
    "            doc_count = len(json.load(f))\n",
    "    \n",
    "    ingestion_result = {\n",
    "        'status': 'skipped',\n",
    "        'total_documents': doc_count,\n",
    "        'holdings_processed': holdings,\n",
    "        'failed_holdings': [],\n",
    "        'metrics': {\n",
    "            'processing_time': 0.0,\n",
    "            'data_sources_used': [],\n",
    "            'investment_signals': {\n",
    "                'email_count': 0,\n",
    "                'tickers_covered': 0,\n",
    "                'buy_ratings': 0,\n",
    "                'sell_ratings': 0,\n",
    "                'avg_confidence': 0.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Existing graph contains {doc_count} documents\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# FILE STORAGE DIAGNOSTIC (Added for debugging)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìÅ FILE STORAGE DIAGNOSTIC\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Determine storage path (handles both script and notebook contexts)\n",
    "if '__file__' in globals():\n",
    "    storage_path = Path(__file__).parent.parent.parent / 'data' / 'attachments'\n",
    "else:\n",
    "    storage_path = Path('data/attachments')\n",
    "\n",
    "resolved_path = storage_path.resolve()\n",
    "print(f\"Expected storage path: {resolved_path}\")\n",
    "print(f\"Path exists: {resolved_path.exists()}\")\n",
    "\n",
    "if resolved_path.exists():\n",
    "    all_items = list(resolved_path.rglob(\"*\"))\n",
    "    all_files = [f for f in all_items if f.is_file()]\n",
    "    pdfs = [f for f in all_files if f.suffix == '.pdf']\n",
    "    txt_files = [f for f in all_files if f.name == 'extracted.txt']\n",
    "    metadata_files = [f for f in all_files if f.name == 'metadata.json']\n",
    "    \n",
    "    print(f\"\\nStorage contents:\")\n",
    "    print(f\"  Total items: {len(all_items)} (files + directories)\")\n",
    "    print(f\"  Total files: {len(all_files)}\")\n",
    "    print(f\"  PDF files: {len(pdfs)}\")\n",
    "    print(f\"  extracted.txt files: {len(txt_files)}\")\n",
    "    print(f\"  metadata.json files: {len(metadata_files)}\")\n",
    "    \n",
    "    if pdfs:\n",
    "        print(f\"\\n‚úÖ PDF files found:\")\n",
    "        for pdf in pdfs[:5]:\n",
    "            size_kb = pdf.stat().st_size / 1024\n",
    "            print(f\"  [{size_kb:>8.1f} KB] {pdf.name}\")\n",
    "            print(f\"              ‚Üí {pdf}\")\n",
    "        if len(pdfs) > 5:\n",
    "            print(f\"  ... and {len(pdfs) - 5} more PDFs\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No PDF files found in storage\")\n",
    "    \n",
    "    if metadata_files:\n",
    "        print(f\"\\n‚úÖ metadata.json files found:\")\n",
    "        for meta in metadata_files[:3]:\n",
    "            print(f\"  ‚Üí {meta.parent}\")\n",
    "            # Read and show key fields\n",
    "            try:\n",
    "                import json as json_module\n",
    "                with open(meta, 'r') as f:\n",
    "                    meta_data = json_module.load(f)\n",
    "                    source_type = meta_data.get('source_type', 'unknown')\n",
    "                    file_hash = meta_data.get('file_info', {}).get('file_hash', 'N/A')[:16]\n",
    "                    print(f\"     Type: {source_type}, Hash: {file_hash}...\")\n",
    "            except:\n",
    "                pass\n",
    "        if len(metadata_files) > 3:\n",
    "            print(f\"  ... and {len(metadata_files) - 3} more metadata files\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No metadata.json files found\")\n",
    "        \n",
    "    # Check if any subdirectories exist\n",
    "    subdirs = [d for d in all_items if d.is_dir()]\n",
    "    if subdirs:\n",
    "        print(f\"\\nüìÇ Subdirectories: {len(subdirs)}\")\n",
    "        for subdir in subdirs[:5]:\n",
    "            print(f\"  ‚Üí {subdir.name}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No subdirectories (expected: {'{email_uid}/{file_hash}/'} structure)\")\n",
    "else:\n",
    "    print(f\"‚ùå Storage path does not exist: {resolved_path}\")\n",
    "    print(f\"   This directory should be created by IntelligentLinkProcessor\")\n",
    "\n",
    "print(f\"{'='*70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15.5 - ENHANCED PDF Processing & Entity Extraction Diagnostics\n",
    "# Purpose: Comprehensive validation of URL PDF processing pipeline\n",
    "# Checks: PDF download ‚Üí Text extraction ‚Üí Entity extraction ‚Üí LightRAG ingestion\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç COMPREHENSIVE PDF PROCESSING DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DIAGNOSTIC 1: Check EntityExtractor Initialization (IMPROVED)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"\\n[1/6] üîß EntityExtractor Initialization Check\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Try multiple possible locations (robust against refactoring)\n",
    "entity_extractor = None\n",
    "ee_paths = [\n",
    "    ('ice.data_ingester.entity_extractor', lambda: ice.data_ingester.entity_extractor if hasattr(ice, 'data_ingester') else None),\n",
    "    ('ice.entity_extractor', lambda: ice.entity_extractor if hasattr(ice, 'entity_extractor') else None),\n",
    "]\n",
    "\n",
    "for path_name, getter in ee_paths:\n",
    "    try:\n",
    "        candidate = getter()\n",
    "        if candidate is not None:\n",
    "            entity_extractor = candidate\n",
    "            print(f\"‚úÖ EntityExtractor found at: {path_name}\")\n",
    "            break\n",
    "    except (AttributeError, TypeError):\n",
    "        continue\n",
    "\n",
    "if entity_extractor:\n",
    "    print(f\"   NLP Model (spaCy): {'‚úÖ Loaded' if hasattr(entity_extractor, 'nlp') and entity_extractor.nlp else '‚ùå Not loaded'}\")\n",
    "    if hasattr(entity_extractor, 'tickers'):\n",
    "        print(f\"   Known Tickers: {len(entity_extractor.tickers)} loaded\")\n",
    "    if hasattr(entity_extractor, 'companies'):\n",
    "        print(f\"   Companies: {len(entity_extractor.companies)} loaded\")\n",
    "else:\n",
    "    print(f\"‚ùå EntityExtractor NOT found in any expected location\")\n",
    "    print(f\"   Tried paths: {[p[0] for p in ee_paths]}\")\n",
    "    print(f\"   ‚Üí This may indicate initialization issue\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DIAGNOSTIC 2: Check Downloaded PDFs\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"\\n[2/6] üìÅ Downloaded PDF Verification\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Fixed: Changed from legacy data/downloaded_reports to unified storage\n",
    "storage_dir = Path(\"data/attachments\")\n",
    "latest_pdf = None\n",
    "\n",
    "if storage_dir.exists():\n",
    "    pdfs = list(storage_dir.glob(\"*/*/original/*.pdf\"))\n",
    "    print(f\"PDFs in directory: {len(pdfs)}\")\n",
    "    \n",
    "    if pdfs:\n",
    "        latest_pdf = max(pdfs, key=lambda p: p.stat().st_mtime)\n",
    "        size_kb = latest_pdf.stat().st_size / 1024\n",
    "        \n",
    "        print(f\"\\nLatest PDF:\")\n",
    "        print(f\"  File: {latest_pdf.name}\")\n",
    "        print(f\"  Size: {size_kb:.1f} KB\")\n",
    "        print(f\"  Path: {latest_pdf}\")\n",
    "        \n",
    "        # Verify PDF header\n",
    "        with open(latest_pdf, 'rb') as f:\n",
    "            header = f.read(4)\n",
    "            if header == b'%PDF':\n",
    "                print(f\"  ‚úÖ Valid PDF file\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Invalid PDF header: {header}\")\n",
    "                \n",
    "        # Check for metadata.json\n",
    "        metadata_path = latest_pdf.parent.parent / 'metadata.json'\n",
    "        if metadata_path.exists():\n",
    "            print(f\"  ‚úÖ metadata.json found\")\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                print(f\"     Source type: {metadata.get('source_type', 'unknown')}\")\n",
    "                print(f\"     Original URL: {metadata.get('source_context', {}).get('original_url', 'unknown')[:60]}...\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  metadata.json NOT found\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No PDFs found\")\n",
    "else:\n",
    "    print(\"‚ùå Download directory doesn't exist\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DIAGNOSTIC 3: PDF Content Verification in LightRAG (MOVED UP)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"\\n[3/6] üîó PDF Content Verification in LightRAG\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Read actual documents from LightRAG storage\n",
    "kv_path = Path('ice_lightrag/storage/kv_store_full_docs.json')\n",
    "pdf_content_found = False\n",
    "\n",
    "if kv_path.exists():\n",
    "    try:\n",
    "        with open(kv_path, 'r') as f:\n",
    "            docs = json.load(f)\n",
    "        \n",
    "        for doc_id, doc_data in docs.items():\n",
    "            content = doc_data.get('content', '')\n",
    "            \n",
    "            # Search for PDF marker\n",
    "            if '[LINKED_REPORT:' in content:\n",
    "                pdf_content_found = True\n",
    "                # Extract PDF URL from marker\n",
    "                import re\n",
    "                pdf_urls = re.findall(r'\\[LINKED_REPORT:(.+?)\\]', content)\n",
    "                \n",
    "                file_path = doc_data.get('__vector_store__', {}).get('file_path', 'unknown')\n",
    "                print(f\"‚úÖ PDF content found in document: {file_path}\")\n",
    "                print(f\"   PDF URLs embedded: {len(pdf_urls)}\")\n",
    "                for i, url in enumerate(pdf_urls[:3], 1):\n",
    "                    print(f\"      [{i}] {url[:60]}...\")\n",
    "                \n",
    "                # Estimate PDF text length\n",
    "                pdf_sections = content.split('[LINKED_REPORT:')\n",
    "                if len(pdf_sections) > 1:\n",
    "                    pdf_text_len = sum(len(s) for s in pdf_sections[1:])\n",
    "                    print(f\"   PDF text length: ~{pdf_text_len:,} chars\")\n",
    "                break  # Just show first document with PDF content\n",
    "        \n",
    "        if not pdf_content_found:\n",
    "            print(f\"‚ùå No PDF content markers found in {len(docs)} documents\")\n",
    "            print(f\"   Documents may contain only email body text\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading LightRAG documents: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå LightRAG storage not found at: {kv_path}\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DIAGNOSTIC 4: Entity Markup Analysis\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"\\n[4/6] üìÑ Entity Markup Analysis\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "entity_markup_found = False\n",
    "\n",
    "if kv_path.exists():\n",
    "    try:\n",
    "        with open(kv_path, 'r') as f:\n",
    "            docs = json.load(f)\n",
    "        \n",
    "        print(f\"Total documents in LightRAG: {len(docs)}\")\n",
    "        \n",
    "        for doc_id, doc_data in docs.items():\n",
    "            content = doc_data.get('content', '')\n",
    "            \n",
    "            # Check for entity markup patterns\n",
    "            if any(marker in content for marker in ['[TICKER:', '[COMPANY:', '[FINANCIAL]']):\n",
    "                entity_markup_found = True\n",
    "                break\n",
    "        \n",
    "        if entity_markup_found:\n",
    "            print(\"‚úÖ Entity markup found in documents\")\n",
    "        else:\n",
    "            print(\"‚ùå NO entity markup found (extraction may have failed)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading documents: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå LightRAG storage not found\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DIAGNOSTIC 5: LightRAG Graph Statistics\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"\\n[5/6] üìä LightRAG Knowledge Graph Stats\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Try to get graph stats from LightRAG\n",
    "try:\n",
    "    if kv_path.exists():\n",
    "        with open(kv_path, 'r') as f:\n",
    "            docs = json.load(f)\n",
    "            print(f\"Documents: {len(docs)}\")\n",
    "    else:\n",
    "        print(\"Documents: 0\")\n",
    "    \n",
    "    # Count chunks\n",
    "    chunks_path = Path('ice_lightrag/storage/kv_store_text_chunks.json')\n",
    "    if chunks_path.exists():\n",
    "        with open(chunks_path, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "            print(f\"Chunks: {len(chunks)}\")\n",
    "    else:\n",
    "        print(\"Chunks: 0\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting stats: {e}\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DIAGNOSTIC 6: Summary & Next Steps\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"\\n[6/6] üìã DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "issues = []\n",
    "if not entity_extractor:\n",
    "    issues.append(\"EntityExtractor NOT found in any expected location\")\n",
    "if not latest_pdf:\n",
    "    issues.append(\"No PDFs downloaded\")\n",
    "if not pdf_content_found:\n",
    "    issues.append(\"PDF content NOT found in LightRAG documents\")\n",
    "if not entity_markup_found:\n",
    "    issues.append(\"NO entity markup (extraction may have failed)\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\n‚ùå ISSUES DETECTED:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"   {i}. {issue}\")\n",
    "    \n",
    "    print(\"\\nüí° NEXT STEPS:\")\n",
    "    print(\"   1. Check logs from Cell 15 for 'üìÅ STORAGE VERIFIED:' messages\")\n",
    "    print(\"   2. Immediately run in terminal: ls -la data/attachments/*/*/original/\")\n",
    "    print(\"   3. Check for 'üóÇÔ∏è  STORAGE PATH RESOLUTION:' in Cell 15 logs\")\n",
    "    print(\"   4. If files missing, check async timing (wait 5 seconds, re-run diagnostic)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ ALL CHECKS PASSED!\")\n",
    "    print(\"   PDF processing pipeline working correctly\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "#######################################################################################\n",
    "# DIAGNOSTIC: Verify configuration is correct\n",
    "print(\"=\"*70)\n",
    "print(\"üìã CONFIGURATION CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"EMAIL_SELECTOR: {EMAIL_SELECTOR}\")\n",
    "print(f\"email_source_enabled: {email_source_enabled}\")\n",
    "print(f\"\\n‚úÖ Expected: EMAIL_SELECTOR = 'crawl4ai_test'\")\n",
    "print(f\"‚úÖ Expected: email_source_enabled = True\")\n",
    "print(f\"\\nüìß Selected email files ({len(email_sets[EMAIL_SELECTOR]['email_files'])}):\")\n",
    "for i, fname in enumerate(email_sets[EMAIL_SELECTOR]['email_files'], 1):\n",
    "    print(f\"  {i}. {fname[:70]}...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#######################################################################################\n",
    "# DIAGNOSTIC: Check if PDFs were downloaded\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Fixed: Changed from legacy data/downloaded_reports to unified storage\n",
    "storage_dir = Path(\"data/attachments\")\n",
    "all_pdfs = list(storage_dir.glob(\"*/*/original/*.pdf\"))\n",
    "\n",
    "# Check for recent downloads (last 10 minutes)\n",
    "recent_cutoff = datetime.now() - timedelta(minutes=10)\n",
    "recent_pdfs = [f for f in all_pdfs if datetime.fromtimestamp(f.stat().st_mtime) > recent_cutoff]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÅ FILE VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total PDFs in directory: {len(all_pdfs)}\")\n",
    "print(f\"New PDFs (last 10 min): {len(recent_pdfs)}\")\n",
    "\n",
    "if recent_pdfs:\n",
    "    print(f\"\\n‚úÖ URL PROCESSING WORKED! Downloaded {len(recent_pdfs)} PDFs:\")\n",
    "    for pdf in recent_pdfs:\n",
    "        size_kb = pdf.stat().st_size / 1024\n",
    "        mtime = datetime.fromtimestamp(pdf.stat().st_mtime).strftime('%H:%M:%S')\n",
    "        print(f\"   ‚Ä¢ {pdf.name} ({size_kb:.1f} KB, {mtime})\")\n",
    "else:\n",
    "    print(f\"\\n‚ÑπÔ∏è  No new downloads (may already be cached)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#######################################################################################\n",
    "# DIAGNOSTIC: Search Cell 27 output for URL processing indicators\n",
    "# Note: This only works if you saved Cell 27 output to a variable\n",
    "\n",
    "# If you can't find the prominent boxes, look for these INFO logs:\n",
    "print(\"=\"*70)\n",
    "print(\"üìã WHAT TO LOOK FOR IN CELL 27 OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ SUCCESS INDICATORS (search Cell 27 output for these):\\n\")\n",
    "print(\"1. IntelligentLinkProcessor initialized:\")\n",
    "print(\"   'INFO:...data_ingestion:‚úÖ IntelligentLinkProcessor initialized'\")\n",
    "print(\"\\n2. URL extraction (should be > 0):\")\n",
    "print(\"   'INFO:...intelligent_link_processor:Extracted X links from email'\")\n",
    "print(\"\\n3. Research reports classified (should be > 0):\")\n",
    "print(\"   'INFO:...intelligent_link_processor:Classified Y research reports'\")\n",
    "print(\"\\n4. Reports downloaded (‚â• 0):\")\n",
    "print(\"   'INFO:...intelligent_link_processor:Downloaded Z reports'\")\n",
    "print(\"\\n5. Prominent boxes (NEW - added for visibility):\")\n",
    "print(\"   'üîó URL PROCESSING: [email filename]'\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#######################################################################################\n",
    "# DIAGNOSTIC: Quick pass/fail check\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Fixed: Changed from legacy data/downloaded_reports to unified storage\n",
    "storage_dir = Path(\"data/attachments\")\n",
    "recent_cutoff = datetime.now() - timedelta(minutes=10)\n",
    "recent_pdfs = [f for f in storage_dir.glob(\"*/*/original/*.pdf\")\n",
    "               if datetime.fromtimestamp(f.stat().st_mtime) > recent_cutoff]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ QUICK SUCCESS CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check 1: Configuration\n",
    "config_ok = 'EMAIL_SELECTOR' in dir() and EMAIL_SELECTOR == 'crawl4ai_test'\n",
    "print(f\"{'‚úÖ' if config_ok else '‚ùå'} EMAIL_SELECTOR = 'crawl4ai_test'\")\n",
    "\n",
    "# Check 2: Email source enabled\n",
    "email_ok = 'email_source_enabled' in dir() and email_source_enabled == True\n",
    "print(f\"{'‚úÖ' if email_ok else '‚ùå'} email_source_enabled = True\")\n",
    "\n",
    "# Check 3: New downloads\n",
    "downloads_ok = len(recent_pdfs) > 0\n",
    "print(f\"{'‚úÖ' if downloads_ok else '‚ÑπÔ∏è '} New PDFs downloaded: {len(recent_pdfs)}\")\n",
    "\n",
    "# Overall\n",
    "if config_ok and email_ok:\n",
    "    if downloads_ok:\n",
    "        print(f\"\\nüéâ SUCCESS: URL processing is working!\")\n",
    "        print(f\"   Downloaded {len(recent_pdfs)} new PDFs\")\n",
    "    else:\n",
    "        print(f\"\\n‚ÑπÔ∏è  Configuration correct, but no new downloads\")\n",
    "        print(f\"   Either: (1) Files cached from previous run\")\n",
    "        print(f\"           (2) Check Cell 27 logs for 'Extracted 0 links'\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Configuration issue - check Cell 26 settings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "### Check if any PDFs have been downloaded from the URL links in the emails.\n",
    "\n",
    "# Check for newly downloaded PDFs\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Fixed: Changed from legacy data/downloaded_reports to unified storage\n",
    "storage_dir = Path(\"data/attachments\")\n",
    "recent_cutoff = datetime.now() - timedelta(minutes=10)\n",
    "\n",
    "recent_pdfs = [\n",
    "    f for f in storage_dir.glob(\"*/*/original/*.pdf\")\n",
    "    if datetime.fromtimestamp(f.stat().st_mtime) > recent_cutoff]\n",
    "\n",
    "print(f\"‚úÖ {len(recent_pdfs)} NEW PDFs downloaded in last 10 minutes\")\n",
    "for pdf in recent_pdfs:\n",
    "    print(f\"   ‚Ä¢ {pdf.name} ({pdf.stat().st_size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CRAWL4AI CONFIGURATION STATUS CHECK\n",
    "# Verify browser automation settings for URL processing\n",
    "#\n",
    "# INSTRUCTIONS: Copy this entire cell and paste as NEW CELL after Cell 30\n",
    "# (the cell that checks for downloaded PDFs)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üåê CRAWL4AI CONFIGURATION STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if orchestrator and link processor are available\n",
    "if 'orchestrator' in locals() and orchestrator and hasattr(orchestrator, 'link_processor'):\n",
    "    link_processor = orchestrator.link_processor\n",
    "\n",
    "    if link_processor:\n",
    "        print(f\"\\n‚úÖ IntelligentLinkProcessor: INITIALIZED\")\n",
    "        print(f\"\\nüîß Configuration:\")\n",
    "        print(f\"  Crawl4AI Enabled: {link_processor.use_crawl4ai}\")\n",
    "        print(f\"  Timeout: {link_processor.crawl4ai_timeout}s\")\n",
    "        print(f\"  Headless Mode: {link_processor.crawl4ai_headless}\")\n",
    "\n",
    "        print(f\"\\nüìä 6-Tier URL Classification System:\")\n",
    "        print(f\"  Tier 1: Direct downloads (.pdf, .xlsx) ‚Üí Simple HTTP\")\n",
    "        print(f\"  Tier 2: Token-auth (DBS ?E=) ‚Üí Simple HTTP\")\n",
    "        print(f\"  Tier 3: News sites ‚Üí {'Crawl4AI' if link_processor.use_crawl4ai else 'Simple HTTP (fallback)'}\")\n",
    "        print(f\"  Tier 4: Research portals ‚Üí {'Crawl4AI' if link_processor.use_crawl4ai else 'Simple HTTP (fallback)'}\")\n",
    "        print(f\"  Tier 5: Paywalls ‚Üí {'Crawl4AI' if link_processor.use_crawl4ai else 'Simple HTTP (fallback)'}\")\n",
    "        print(f\"  Tier 6: Social/tracking ‚Üí Skip\")\n",
    "\n",
    "        if link_processor.use_crawl4ai:\n",
    "            print(f\"\\n‚úÖ Crawl4AI is ENABLED\")\n",
    "            print(f\"   ‚Ä¢ Browser automation active for Tier 3/4/5 URLs\")\n",
    "            print(f\"   ‚Ä¢ Expected: Higher success rate on complex sites (60-80%)\")\n",
    "            print(f\"   ‚Ä¢ Impact: 70-90% more premium broker research captured\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Crawl4AI is DISABLED (default)\")\n",
    "            print(f\"   ‚Ä¢ Using simple HTTP only for all URLs\")\n",
    "            print(f\"   ‚Ä¢ Success rate: ~30-40% on Tier 3/4/5 URLs\")\n",
    "            print(f\"   ‚Ä¢ Missing: Premium portals (Goldman, Morgan Stanley, JPM)\")\n",
    "            print(f\"\\nüí° To Enable: Set USE_CRAWL4AI_LINKS=true before starting notebook\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå IntelligentLinkProcessor: NOT INITIALIZED\")\n",
    "        print(f\"   ‚Ä¢ Link processing will not work\")\n",
    "        print(f\"   ‚Ä¢ Check data_ingestion.py initialization\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orchestrator not available yet\")\n",
    "    print(f\"   ‚Ä¢ Run Cell 22 (Orchestrator Initialization) first\")\n",
    "    print(f\"   ‚Ä¢ This cell will show Crawl4AI status after initialization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "# Comprehensive 3-Tier Knowledge Graph Statistics\n",
    "stats = ice.get_comprehensive_stats()\n",
    "\n",
    "print(\"üìä ICE Knowledge Graph Statistics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TIER 1: Document Source Breakdown\n",
    "print(\"\\nüìÑ TIER 1: Document Source Breakdown\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t1 = stats['tier1']\n",
    "diversity = t1.get('source_diversity', {})\n",
    "\n",
    "print(f\"Total Documents: {t1['total']}\")\n",
    "print(f\"\\nüìä Source Distribution (Visual Breakdown):\")\n",
    "print(f\"  üìß Email:    {ice._format_progress_bar(t1['email'], t1['total'])}\")\n",
    "print(f\"  üåê API:      {ice._format_progress_bar(t1['api_total'], t1['total'])}\")\n",
    "print(f\"  üìã SEC:      {ice._format_progress_bar(t1['sec_total'], t1['total'])}\")\n",
    "\n",
    "print(f\"\\nüìà Source Diversity Metrics:\")\n",
    "print(f\"  ‚Ä¢ Unique sources detected: {diversity.get('unique_sources', 0)}\")\n",
    "print(f\"  ‚Ä¢ Expected sources (Email/API/SEC): {diversity.get('expected_sources_present', 0)}/3\")\n",
    "print(f\"  ‚Ä¢ Coverage: {diversity.get('coverage_percentage', 0.0):.1f}% ({diversity.get('documents_with_markers', 0)}/{t1['total']} docs with markers)\")\n",
    "print(f\"  ‚Ä¢ Status: {diversity.get('status', 'unknown').upper()}\")\n",
    "\n",
    "if diversity.get('status') == 'incomplete' or diversity.get('coverage_percentage', 0) < 80:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  Low coverage detected! Set REBUILD_GRAPH=True in Cell 22 to rebuild with correct markers\")\n",
    "elif diversity.get('status') == 'complete':\n",
    "    print(f\"\\n  ‚úÖ All data sources properly tagged!\")\n",
    "\n",
    "print(f\"\\nüìä Detailed Breakdown:\")\n",
    "print(f\"  üìß Email: {t1['email']} documents\")\n",
    "print(f\"     ‚Ä¢ Portfolio-wide broker research\")\n",
    "print(f\"  üåê API: {t1['api_total']} documents\")\n",
    "print(f\"     ‚Ä¢ NewsAPI: {t1.get('newsapi', 0)}\")\n",
    "print(f\"     ‚Ä¢ FMP: {t1.get('fmp', 0)}\")\n",
    "print(f\"     ‚Ä¢ Alpha Vantage: {t1.get('alpha_vantage', 0)}\")\n",
    "print(f\"     ‚Ä¢ Polygon: {t1.get('polygon', 0)}\")\n",
    "if t1.get('finnhub', 0) > 0:\n",
    "    print(f\"     ‚Ä¢ Finnhub: {t1.get('finnhub', 0)}\")\n",
    "if t1.get('marketaux', 0) > 0:\n",
    "    print(f\"     ‚Ä¢ MarketAux: {t1.get('marketaux', 0)}\")\n",
    "if t1.get('benzinga', 0) > 0:\n",
    "    print(f\"     ‚Ä¢ Benzinga: {t1.get('benzinga', 0)}\")\n",
    "print(f\"  üìã SEC: {t1['sec_total']} documents\")\n",
    "print(f\"     ‚Ä¢ SEC EDGAR filings: {t1.get('sec_edgar', 0)}\")\n",
    "\n",
    "# TIER 2: Graph Structure\n",
    "print(\"\\n\\nüï∏Ô∏è  TIER 2: Knowledge Graph Structure\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t2 = stats['tier2']\n",
    "print(f\"Total Entities: {t2['total_entities']:,}\")\n",
    "print(f\"Total Relationships: {t2['total_relationships']:,}\")\n",
    "if t2['total_entities'] > 0:\n",
    "    print(f\"Avg Connections per Entity: {t2['avg_connections']:.2f}\")\n",
    "\n",
    "# TIER 3: Investment Intelligence\n",
    "print(\"\\n\\nüíº TIER 3: Investment Intelligence\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "t3 = stats['tier3']\n",
    "if t3['tickers_covered']:\n",
    "    print(f\"Portfolio Coverage: {', '.join(t3['tickers_covered'])} ({len(t3['tickers_covered'])} tickers)\")\n",
    "else:\n",
    "    print(f\"Portfolio Coverage: No tickers detected\")\n",
    "\n",
    "print(f\"\\nInvestment Signals:\")\n",
    "print(f\"  ‚Ä¢ BUY ratings: {t3['buy_signals']}\")\n",
    "print(f\"  ‚Ä¢ SELL ratings: {t3['sell_signals']}\")\n",
    "print(f\"  ‚Ä¢ Price targets: {t3['price_targets']}\")\n",
    "print(f\"  ‚Ä¢ Risk mentions: {t3['risk_mentions']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Comprehensive statistics complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "# Knowledge Graph Building - Already completed during ingestion\n",
    "print(f\"\\nüß† Knowledge Graph Building\")\n",
    "print(f\"‚îÅ\" * 60)\n",
    "\n",
    "if not (ice and ice.core.is_ready()):\n",
    "    raise RuntimeError(\"LightRAG not ready\")\n",
    "\n",
    "print(f\"‚ÑπÔ∏è  NOTE: Knowledge graph building happens automatically during data ingestion\")\n",
    "print(f\"   The ingestion method (ingest_historical_data) already added documents\")\n",
    "print(f\"   to the graph via LightRAG. This cell validates that building succeeded.\\n\")\n",
    "\n",
    "# Validate that building succeeded by checking storage\n",
    "storage_stats = ice.core.get_storage_stats()\n",
    "\n",
    "if storage_stats['total_storage_bytes'] > 0:\n",
    "    print(f\"‚úÖ KNOWLEDGE GRAPH BUILT SUCCESSFULLY\")\n",
    "\n",
    "    # Save extraction code version (for stale-graph detection)\n",
    "    # Monitor multiple extraction pipeline files\n",
    "    files_to_monitor = [\n",
    "        \"imap_email_ingestion_pipeline/table_entity_extractor.py\",\n",
    "        \"imap_email_ingestion_pipeline/enhanced_doc_creator.py\"\n",
    "    ]\n",
    "    combined_hash = hashlib.md5()\n",
    "    for f in files_to_monitor:\n",
    "        if Path(f).exists():\n",
    "            combined_hash.update(Path(f).read_bytes())\n",
    "    current_hash = combined_hash.hexdigest()[:8]\n",
    "    version_file = Path(\"ice_lightrag/storage/.extractor_version\")\n",
    "    version_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    version_file.write_text(current_hash)\n",
    "    print(f\"‚îÅ\" * 40)\n",
    "    print(f\"   üìÑ Documents processed: {ingestion_result.get('total_documents', 0)}\")\n",
    "    print(f\"   üíæ Storage size: {storage_stats['total_storage_bytes'] / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    components_ready = sum(1 for c in storage_stats['components'].values() if c['exists'])\n",
    "    print(f\"   üîó Components ready: {components_ready}/4\")\n",
    "    \n",
    "    # Create success result for metrics tracking\n",
    "    building_result = {\n",
    "        'status': 'success',\n",
    "        'total_documents': ingestion_result.get('total_documents', 0),\n",
    "        'metrics': {\n",
    "            'building_time': ingestion_result.get('metrics', {}).get('processing_time', 0.0),\n",
    "            'graph_initialized': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ Graph Building Process:\")\n",
    "    print(f\"   1Ô∏è‚É£ Text Chunking: 1200 tokens (optimal for financial documents)\")\n",
    "    print(f\"   2Ô∏è‚É£ Entity Extraction: Companies, metrics, risks, regulations\")\n",
    "    print(f\"   3Ô∏è‚É£ Relationship Discovery: Dependencies, impacts, correlations\")\n",
    "    print(f\"   4Ô∏è‚É£ Graph Construction: LightRAG optimized structure\")\n",
    "    print(f\"   5Ô∏è‚É£ Storage: chunks_vdb, entities_vdb, relationships_vdb, graph\")\n",
    "    \n",
    "    print(f\"\\nüöÄ System ready for intelligent queries!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è NO GRAPH DATA DETECTED\")\n",
    "    print(f\"   Storage size: 0 MB\")\n",
    "    print(f\"   Check ingestion results above for errors\")\n",
    "    print(f\"   Possible causes:\")\n",
    "    print(f\"   - No API keys configured\")\n",
    "    print(f\"   - All holdings failed to fetch data\")\n",
    "    print(f\"   - Network connectivity issues\")\n",
    "    \n",
    "    building_result = {\n",
    "        'status': 'error',\n",
    "        'message': 'No graph data - check ingestion results'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìã FOOTNOTE TRACEABILITY FEATURE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Location: ice_building_workflow.ipynb Cell 31 (v1.4.9 Migration)\n",
    "# Purpose: Add footnote-style citations with knowledge graph reasoning paths\n",
    "# Why: Transparent source attribution using LightRAG v1.4.9 structured data\n",
    "# Relevant Files: citation_formatter.py, ice_rag_fixed.py\n",
    "\n",
    "from typing import Any\n",
    "from src.ice_core.citation_formatter import CitationFormatter\n",
    "\n",
    "# Quality badge mapping\n",
    "QUALITY_BADGES = {\n",
    "    'email': 'üî¥ Tertiary',\n",
    "    'api': 'üü° Secondary',\n",
    "    'entity_extraction': 'üî¥ Tertiary',\n",
    "    'sec_filing': 'üü¢ Primary',\n",
    "    'news': 'üü° Secondary',\n",
    "    'research': 'üü¢ Primary'\n",
    "}\n",
    "\n",
    "# Smart confidence mapping\n",
    "CONFIDENCE_MAP = {\n",
    "    'email': 0.85,\n",
    "    'api': 0.90,\n",
    "    'entity_extraction': 0.75,\n",
    "    'sec_filing': 0.95,\n",
    "    'news': 0.88,\n",
    "    'research': 0.92\n",
    "}\n",
    "\n",
    "def add_footnote_citations(query_result):\n",
    "    \"\"\"\n",
    "    Add footnote-style citations with knowledge graph reasoning paths.\n",
    "    \n",
    "    v1.4.9 Migration: Uses structured data from parsed_context (no regex parsing).\n",
    "    \n",
    "    Features:\n",
    "    - File-level citations (Layer 1) from chunks metadata\n",
    "    - Graph reasoning paths (Layer 2) from entities/relationships\n",
    "    - Confidence color-coding (üü¢‚â•85%, üü°70-85%, üî¥<70%)\n",
    "    - Graceful degradation when data missing\n",
    "    \"\"\"\n",
    "    import re  # Moved inside function for proper scoping\n",
    "    \n",
    "    # ========================================================================\n",
    "    # HELPER FUNCTIONS (Nested inside parent function)\n",
    "    # ========================================================================\n",
    "    \n",
    "    def build_confidence_cache(chunks):\n",
    "        \"\"\"\n",
    "        Build O(1) lookup cache for entity confidence scores from markup.\n",
    "        \n",
    "        Scans chunks once to extract confidence from inline markup like:\n",
    "        [TICKER:NVDA|confidence:0.95] or [RATING:BUY|...|confidence:0.87]\n",
    "        \n",
    "        Returns dict: {entity_name: max_confidence}\n",
    "        \"\"\"\n",
    "        confidence_cache = {}\n",
    "        \n",
    "        # Generic pattern for any markup type with confidence\n",
    "        pattern = r'\\[([A-Z_]+):([^|\\]]+)\\|[^\\]]*confidence:([\\d.]+)\\]'\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            content = chunk.get('content', '')\n",
    "            for match in re.finditer(pattern, content):\n",
    "                entity_value = match.group(2).strip()\n",
    "                confidence = float(match.group(3))\n",
    "                \n",
    "                # Take max if entity appears multiple times\n",
    "                if entity_value in confidence_cache:\n",
    "                    confidence_cache[entity_value] = max(confidence_cache[entity_value], confidence)\n",
    "                else:\n",
    "                    confidence_cache[entity_value] = confidence\n",
    "        \n",
    "        return confidence_cache\n",
    "    \n",
    "    def get_entity_confidence(entity_name, entities, confidence_cache=None):\n",
    "        \"\"\"Extract confidence with 3-tier fallback: cache ‚Üí metadata ‚Üí 0.75\"\"\"\n",
    "        # Tier 1: Check confidence cache (O(1) markup lookup)\n",
    "        if confidence_cache and entity_name in confidence_cache:\n",
    "            return confidence_cache[entity_name]\n",
    "        \n",
    "        # Tier 2: Check entity metadata (future-proof)\n",
    "        for e in entities:\n",
    "            if e.get('entity_name') == entity_name:\n",
    "                conf = e.get('confidence', e.get('score', 0.75))\n",
    "                return float(conf) if conf else 0.75\n",
    "        \n",
    "        # Tier 3: Default for LLM-extracted entities\n",
    "        return 0.75\n",
    "    \n",
    "\n",
    "    def _infer_source_type(file_path):\n",
    "        \"\"\"Infer source_type from file_path prefix (email:, api:, sec:, etc.)\"\"\"\n",
    "        if ':' not in file_path:\n",
    "            return None\n",
    "        \n",
    "        prefix = file_path.split(':', 1)[0].lower()\n",
    "        \n",
    "        # Map prefixes to QUALITY_BADGES keys\n",
    "        prefix_map = {\n",
    "            'email': 'email',\n",
    "            'api': 'api',\n",
    "            'sec': 'sec_filing',\n",
    "            'news': 'news',\n",
    "            'research': 'research',\n",
    "            'entity': 'entity_extraction'\n",
    "        }\n",
    "        \n",
    "        return prefix_map.get(prefix, None)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MAIN EXECUTION CODE\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Extract entities mentioned in answer (semantic case-insensitive matching)\n",
    "    answer = query_result.get('answer', '')\n",
    "    answer_lower = answer.lower()\n",
    "    answer_entities = set[Any]()\n",
    "    \n",
    "    parsed_context = query_result.get('parsed_context', {})\n",
    "    entities_list = parsed_context.get('entities', [])\n",
    "    \n",
    "    for e in entities_list:\n",
    "        entity_name = e.get('entity_name', '')\n",
    "        if entity_name and entity_name.lower() in answer_lower:\n",
    "            answer_entities.add(entity_name)\n",
    "    \n",
    "    print(f\"\\nüîç Answer entities extracted: {len(answer_entities)} entities\")\n",
    "    if answer_entities:\n",
    "        print(f\"   Entities: {list(answer_entities)[:]}\")\n",
    "\n",
    "    # Build confidence cache BEFORE using it in relationship loop\n",
    "    chunks = query_result.get('parsed_context', {}).get('chunks', [])\n",
    "    confidence_cache = build_confidence_cache(chunks)\n",
    "\n",
    "    \n",
    "    # Build reasoning paths from relationships\n",
    "    relationships = parsed_context.get('relationships', [])\n",
    "    graph_paths = []\n",
    "    path_candidates = []\n",
    "    \n",
    "    for rel in relationships:\n",
    "        src = rel.get('src_id', '')\n",
    "        tgt = rel.get('tgt_id', '')\n",
    "        rel_type = rel.get('relation_type', 'related_to')\n",
    "        \n",
    "        # Filter to paths involving answer entities\n",
    "        if src in answer_entities or tgt in answer_entities:\n",
    "            # Get confidence for both entities\n",
    "            src_conf = get_entity_confidence(src, entities_list, confidence_cache)\n",
    "            tgt_conf = get_entity_confidence(tgt, entities_list, confidence_cache)\n",
    "            avg_conf = (src_conf + tgt_conf) / 2\n",
    "            \n",
    "            # Format confidence with color coding\n",
    "            if avg_conf >= 0.85:\n",
    "                conf_str = f\"üü¢ {avg_conf:.0%}\"\n",
    "            elif avg_conf >= 0.70:\n",
    "                conf_str = f\"üü° {avg_conf:.0%}\"\n",
    "            else:\n",
    "                conf_str = f\"üî¥ {avg_conf:.0%}\"\n",
    "            \n",
    "            path = f\"{src} ‚Üí {rel_type} ‚Üí {tgt} (Cof: {conf_str})\"\n",
    "            path_candidates.append((path, avg_conf))\n",
    "    \n",
    "    # Sort by confidence (highest first), limit to top 5\n",
    "    path_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    graph_paths = [p[0] for p in path_candidates[:]]\n",
    "    \n",
    "    print(f\"üîó Graph paths built: {len(graph_paths)} paths\")\n",
    "    \n",
    "    # Build confidence cache once for O(1) lookups (performance optimization)\n",
    "    \n",
    "    if not chunks:\n",
    "        return query_result\n",
    "    \n",
    "    seen_sources = set()\n",
    "    enriched_sources = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        file_path = chunk.get('file_path', 'unknown_source')\n",
    "        \n",
    "        if file_path not in seen_sources:\n",
    "            seen_sources.add(file_path)\n",
    "            \n",
    "            # Get source type and assign quality badge\n",
    "            source_type = _infer_source_type(file_path) or chunk.get('source_type', 'unknown')\n",
    "            quality_badge = QUALITY_BADGES.get(source_type, '‚ö™ Unknown')\n",
    "            \n",
    "            # Get confidence from cache or default\n",
    "            confidence = CONFIDENCE_MAP.get(source_type, 0.75)\n",
    "            \n",
    "            enriched_sources.append({\n",
    "                'file_path': file_path,\n",
    "                'source_type': source_type,\n",
    "                'quality_badge': quality_badge,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Create footnotes section\n",
    "    footnotes = []\n",
    "    for idx, source in enumerate(enriched_sources, 1):\n",
    "        footnotes.append(\n",
    "            f\"[{idx}] {source['quality_badge']} | {source['file_path']} \"\n",
    "            f\"(Confidence: {source['confidence']:.0%})\"\n",
    "        )\n",
    "    \n",
    "    # Append formatted citations to answer\n",
    "    citations_text = \"\\n\\n\" + \"=\"*80 + \"\\n\"\n",
    "    citations_text += \"üìö SOURCES & REASONING PATHS\\n\"\n",
    "    citations_text += \"=\"*80 + \"\\n\\n\"\n",
    "    \n",
    "    if footnotes:\n",
    "        citations_text += \"üìÑ Document Sources:\\n\"\n",
    "        citations_text += \"\\n\".join(footnotes)\n",
    "    \n",
    "    if graph_paths:\n",
    "        citations_text += \"\\n\\nüß† Knowledge Graph Paths:\\n\"\n",
    "        for path in graph_paths:\n",
    "            citations_text += f\"   ‚Ä¢ {path}\\n\"\n",
    "    \n",
    "    query_result['citation_display'] = query_result.get('result', '') + citations_text\n",
    "    \n",
    "    return query_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "\n",
    "def clear_llm_cache():\n",
    "    \"\"\"\n",
    "    Clear LLM response cache using actual ICE storage path.\n",
    "    \n",
    "    Use when:\n",
    "    - Testing traceability features\n",
    "    - Graph structure has changed\n",
    "    - Want to see updated KG sections in query results\n",
    "    \n",
    "    Fix: Dynamically resolves cache path from initialized ICE system\n",
    "    instead of hardcoding path (avoids path mismatch issues)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        # Get actual working directory from initialized ICE system\n",
    "        working_dir = str(ice.core._system_manager.working_dir)\n",
    "        cache_file = os.path.join(working_dir, \"kv_store_llm_response_cache.json\")\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            os.remove(cache_file)\n",
    "            print(f\"‚úÖ LLM cache cleared: {cache_file}\")\n",
    "            print(\"   Next query will retrieve fresh KG sections\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è  No cache file found at: {cache_file}\")\n",
    "            print(\"   (This is normal if cache was already cleared or no queries run yet)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not clear cache: {e}\")\n",
    "        print(\"   Make sure ICE system is initialized first\")\n",
    "\n",
    "print(\"üìã Cache management tool loaded\")\n",
    "print(\"   Run clear_llm_cache() to clear LLM response cache\")\n",
    "print(\"   Useful when testing traceability features or after graph rebuild\")\n",
    "\n",
    "# Uncomment to clear cache before testing:\n",
    "clear_llm_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üß™ COMPREHENSIVE QUERY TESTING WITH GRANULAR TRACEABILITY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Location: ice_building_workflow.ipynb Cell 31 (FIXED)\n",
    "# Purpose: Test knowledge graph with complete granular source attribution\n",
    "# Why: Sentence-level attribution, per-hop tracking, beautiful display (Phases 1-5)\n",
    "# Relevant Files: context_parser.py, sentence_attributor.py, graph_path_attributor.py, granular_display_formatter.py\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ COMPREHENSIVE QUERY TESTING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Portfolio: {', '.join(test_holdings)}\")\n",
    "print(f\"üí° Example queries:\")\n",
    "print(f\"   - Historical: 'What was Tencent's Q2 2025 operating margin?'\")\n",
    "print(f\"   - Current: 'What are the current headwinds for NVDA?'\")\n",
    "print(f\"   - Trend: 'How has AAPL revenue been trending?'\")\n",
    "print(f\"   - Multi-hop: 'How does China risk impact NVDA through TSMC?'\")\n",
    "print()\n",
    "\n",
    "###\n",
    "# query = input(\"üí¨ Enter your question: \") or \"Does the email contain any URL link? What does the URL contain?\"\n",
    "query = input(\"üí¨ Enter your question: \") or \"What is Tencent's operating margin in Q2 2025?\"\n",
    "# query = input(\"üí¨ Enter your question: \") or \"What are Tencent's international games?\"\n",
    "\n",
    "### EMAIL: 'CH_HK_ Tencent Music Entertainment (1698 HK)_ Stronger growth with expanding revenue streams  (NOT RATED).eml'\n",
    "# query = input(\"üí¨ Enter your question: \") or \"What is Tencent Music Entertainment's 2Q 2024 paying users?\"\n",
    "# query = input(\"üí¨ Enter your question: \") or \"What is TME's tiered monetization strategy?\"\n",
    "# query = input(\"üí¨ Enter your question: \") or \"What is TME's music streaming service revenue?\"\n",
    "\n",
    "### EMAIL: 'FW_ RHB | Singapore Morning Cuppa _ 15 August 2025 (ST Engineering, First Resources, Golden Agri-Resources, StarHub).eml'\n",
    "# query = input(\"üí¨ Enter your question: \") or \"What is the TP of DBS?\"\n",
    "# query = input(\"üí¨ Enter your question: \") or \"What is the dividend yield of Singtel?\"\n",
    "\n",
    "mode = input(\"üîç Mode (naive/local/global/hybrid/mix) [hybrid]: \") or \"hybrid\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# STEP 1: Query (Dual Strategy Happens Internally)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Display query configuration\n",
    "print(f\"\\nüìã Query Configuration:\")\n",
    "print(f\"   Query: {query}\")\n",
    "print(f\"   Mode: {mode}\")\n",
    "print(f\"\\n‚è≥ Querying graph (mode: {mode})...\")\n",
    "\n",
    "# Single query - ice_rag_fixed.py handles dual query strategy internally:\n",
    "# 1. Retrieves context with SOURCE markers (only_need_context=True)\n",
    "# 2. Generates answer (normal query)\n",
    "# 3. Returns both context and parsed_context in result dict\n",
    "result = ice.core.query(query, mode=mode)\n",
    "# result = ice.query_with_router(query) # to delete later.\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# FOOTNOTE CITATIONS: Add source attribution\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "result = add_footnote_citations(result)  # Add footnote-style citations\n",
    "\n",
    "if result.get('status') != 'success':\n",
    "    print(f\"‚ùå Query failed: {result.get('error', 'Unknown error')}\")\n",
    "else:\n",
    "    answer = result.get('result', '')\n",
    "    \n",
    "    # Extract context (already retrieved by dual strategy)\n",
    "    raw_context = result.get('context', '')  # Raw LightRAG markdown with SOURCE markers\n",
    "    parsed_context = result.get('parsed_context')  # Already parsed by context_parser!\n",
    "    \n",
    "    # Get causal paths if available (for multi-hop queries)\n",
    "    causal_paths = result.get('graph_context', {}).get('causal_paths', [])\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # DISPLAY CITATIONS: Show footnote-style source attribution\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if 'citation_display' in result:\n",
    "        print('\\n' + '='*80)\n",
    "        print('üìö Generated Response')\n",
    "        print('='*80)\n",
    "        print(result['citation_display'])\n",
    "        print('='*80)\n",
    "    else:\n",
    "        print('\\n‚ö†Ô∏è  No citation_display field available')\n",
    "\n",
    "\n",
    "\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # CHUNK QUALITY METRICS: Show top chunk similarities (NEW: 2025-11-02)\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    chunks = parsed_context.get('chunks', [])\n",
    "    if chunks:\n",
    "        chunks_with_scores = [c for c in chunks if c.get('distance') is not None]\n",
    "        if chunks_with_scores:\n",
    "            print('\\n' + '='*80)\n",
    "            print('üìä CHUNK QUALITY METRICS')\n",
    "            print('='*80)\n",
    "            # for idx, chunk in enumerate(chunks_with_scores[:3], 1):\n",
    "            for idx, chunk in enumerate(chunks_with_scores[:], 1):\n",
    "                distance = chunk['distance']  # Guaranteed by pre-filtering\n",
    "                similarity = (1 - distance) * 100\n",
    "                quality = \"üü¢\" if distance < 0.2 else \"üü°\" if distance < 0.4 else \"üü†\"\n",
    "                print(f\"{quality} Chunk {idx}: {similarity:.1f}% similar (distance: {distance:.3f})\")\n",
    "            \n",
    "            avg_dist = sum(c['distance'] for c in chunks_with_scores) / len(chunks_with_scores)\n",
    "            avg_sim = (1 - avg_dist) * 100\n",
    "            print(f\"\\n   Average similarity: {avg_sim:.1f}% across {len(chunks_with_scores)} chunks\")\n",
    "            print('='*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24\n",
    "# result['parsed_context']['chunks']\n",
    "chunks = [c for c in result['parsed_context']['chunks'] if c.get('distance') is not None]\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üîç DEBUG: CHUNK SIMILARITY SCORES INSPECTOR\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Location: ice_building_workflow.ipynb Cell 34 (NEW)\n",
    "# Purpose: Display chunk similarity scores from LightRAG distance field\n",
    "# Why: Expose cosine similarity metrics for chunk relevance transparency\n",
    "# Relevant Files: lightrag/utils.py:2929 (distance field modification)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç CHUNK SIMILARITY SCORES (LightRAG Distance Field)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chunks = result.get('parsed_context', {}).get('chunks', [])\n",
    "print(f\"Total chunks retrieved: {len(chunks)}\")\n",
    "print(f\"Showing top 5 chunks by similarity:\\n\")\n",
    "\n",
    "for idx, chunk in enumerate(chunks[:5], 1):\n",
    "    distance = chunk.get('distance')\n",
    "    \n",
    "    # Calculate similarity percentage (distance = 1 - cosine_similarity)\n",
    "    if distance is not None:\n",
    "        similarity_pct = (1 - distance) * 100\n",
    "        \n",
    "        # Color-code by similarity level\n",
    "        if distance < 0.2:\n",
    "            quality = \"üü¢ HIGH\"\n",
    "        elif distance < 0.4:\n",
    "            quality = \"üü° MODERATE\"\n",
    "        else:\n",
    "            quality = \"üü† LOW\"\n",
    "    else:\n",
    "        similarity_pct = None\n",
    "        quality = \"‚ö™ N/A (graph-traversal chunk)\"\n",
    "    \n",
    "    print(f\"Chunk #{idx}:\")\n",
    "    print(f\"  Quality: {quality}\")\n",
    "    print(f\"  Distance: {distance:.4f}\" if distance is not None else \"  Distance: N/A\")\n",
    "    print(f\"  Similarity: {similarity_pct:.1f}%\" if similarity_pct is not None else \"  Similarity: N/A\")\n",
    "    print(f\"  File: {chunk.get('file_path', 'unknown')[-50:]}\")\n",
    "    print(f\"  Content: {chunk.get('content', '')[:150]}...\")\n",
    "    print()\n",
    "\n",
    "# Statistics\n",
    "chunks_with_distance = [c for c in chunks if c.get('distance') is not None]\n",
    "if chunks_with_distance:\n",
    "    avg_distance = sum(c['distance'] for c in chunks_with_distance) / len(chunks_with_distance)\n",
    "    avg_similarity = (1 - avg_distance) * 100\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä CHUNK QUALITY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Chunks with similarity scores: {len(chunks_with_distance)}/{len(chunks)}\")\n",
    "    print(f\"Average distance: {avg_distance:.4f}\")\n",
    "    print(f\"Average similarity: {avg_similarity:.1f}%\")\n",
    "    print(f\"\\nüí° Interpretation:\")\n",
    "    print(f\"   ‚Ä¢ Distance 0.0-0.2 (80-100% similar): Highly relevant\")\n",
    "    print(f\"   ‚Ä¢ Distance 0.2-0.4 (60-80% similar): Relevant context\")\n",
    "    print(f\"   ‚Ä¢ Distance 0.4-0.6 (40-60% similar): Tangentially related\")\n",
    "    print(f\"   ‚Ä¢ Distance >0.6 (<40% similar): Filtered out by threshold\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No chunks with distance scores (all from graph traversal)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26\n",
    "# Add this as a new cell in the notebook right after Cell  33 to debug\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç DEBUGGING parsed_context\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check what's in the result\n",
    "print(\"\\n1Ô∏è‚É£ Keys in result:\")\n",
    "for key in result.keys():\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Checking 'context' field:\")\n",
    "if 'context' in result:\n",
    "    context = result['context']\n",
    "    print(f\"   Type: {type(context)}\")\n",
    "    print(f\"   Length: {len(context) if  isinstance(context, str) else 'N/A'}\")\n",
    "    print(f\"   Has 'Entities(KG)': {'Entities(KG)' in  context if isinstance(context, str) else 'N/A'}\")\n",
    "    print(f\"   Has 'Relationships(KG)':  {'Relationships(KG)' in context if isinstance(context,  str) else 'N/A'}\")\n",
    "    print(f\"\\n   First 500 chars of context:\")\n",
    "    print(context[:500] if isinstance(context, str) else context)\n",
    "else:\n",
    "    print(\"   ‚ùå 'context' field missing!\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Checking 'parsed_context' field:\")\n",
    "if 'parsed_context' in result:\n",
    "    parsed = result['parsed_context']\n",
    "    print(f\"   Type: {type(parsed)}\")\n",
    "    if isinstance(parsed, dict):\n",
    "        print(f\"   Keys: {list(parsed.keys())}\")\n",
    "        if 'entities' in parsed:\n",
    "            print(f\"   Entities count: {len(parsed['entities'])}\")\n",
    "        if 'relationships' in parsed:\n",
    "            print(f\"   Relationships count: {len(parsed['relationships'])}\")\n",
    "        if 'chunks' in parsed:\n",
    "            print(f\"   Chunks count: {len(parsed['chunks'])}\")\n",
    "else:\n",
    "    print(\"   ‚ùå 'parsed_context' field missing!\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Checking 'citation_display' field:\")\n",
    "if 'citation_display' in result:\n",
    "    print(\"   ‚úÖ citation_display exists\")\n",
    "    print(f\"\\n   Content:\\n{result['citation_display']}\")\n",
    "else:\n",
    "    print(\"   ‚ùå 'citation_display' field missing!\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27\n",
    "# Deep debug - check why graph paths aren't being built\n",
    "# Run this in a new notebook cell after the previous debug cell\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç DEEP DEBUGGING - Why no graph paths?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Extract reference entities from answer\n",
    "answer = result.get('answer', '')\n",
    "print(\"\\n1Ô∏è‚É£ Reference entity extraction:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "ref_match = re.search(r'#{0,3}\\s*References?:?\\s*\\n(?:-  \\[(?:KG|DC)\\][^\\n]*(?:\\n|$))+', answer, re.DOTALL)\n",
    "if ref_match:\n",
    "    print(f\"‚úÖ References matched:\")\n",
    "    print(ref_match.group())\n",
    "    refs = re.findall(r'\\[KG\\]\\s*([^\\n\\-\\[]+)',ref_match.group())\n",
    "    ref_entities = set(r.strip() for r in refs)\n",
    "    print(f\"\\n‚úÖ Extracted ref_entities: {ref_entities}\")\n",
    "else:\n",
    "    print(\"‚ùå References NOT matched\")\n",
    "    ref_entities = set()\n",
    "\n",
    "# Step 2: Check parsed_context entities\n",
    "print(\"\\n2Ô∏è‚É£ Entities from parsed_context:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "parsed = result.get('parsed_context', {})\n",
    "entities_list = parsed.get('entities', [])\n",
    "print(f\"Total entities: {len(entities_list)}\")\n",
    "print(f\"\\nFirst 10 entities:\")\n",
    "for i, e in enumerate(entities_list[:10]):\n",
    "    entity_id = e.get('id', e.get('entity_id', 'unknown'))\n",
    "    entity_name = e.get('entity', e.get('name',\n",
    "'unknown'))\n",
    "    print(f\"   {i}: id={entity_id}, name={entity_name}\")\n",
    "\n",
    "# Build entities dict\n",
    "entities = {}\n",
    "for e in entities_list:\n",
    "    entity_id = e.get('id', e.get('entity_id'))\n",
    "    entity_name = e.get('entity', e.get('name',\n",
    "'unknown'))\n",
    "    if entity_id is not None:\n",
    "        entities[entity_id] = entity_name\n",
    "\n",
    "print(f\"\\nBuilt entities dict with {len(entities)} entries\")\n",
    "\n",
    "# Step 3: Check relationships\n",
    "print(\"\\n3Ô∏è‚É£ Relationships from parsed_context:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "relationships = parsed.get('relationships', [])\n",
    "print(f\"Total relationships: {len(relationships)}\")\n",
    "print(f\"\\nFirst 10 relationships:\")\n",
    "for i, rel in enumerate(relationships[:10]):\n",
    "    src_id = rel.get('src_id', rel.get('source_id'))\n",
    "    tgt_id = rel.get('tgt_id', rel.get('target_id'))\n",
    "    desc = rel.get('description', rel.get('type', 'RELATED'))\n",
    "\n",
    "    src_name = entities.get(src_id, f\"Unknown({src_id})\")\n",
    "    tgt_name = entities.get(tgt_id, f\"Unknown({tgt_id})\")\n",
    "\n",
    "    print(f\"   {i}: [{src_name}] --{desc}-->  [{tgt_name}]\")\n",
    "\n",
    "# Step 4: Test matching logic\n",
    "print(\"\\n4Ô∏è‚É£ Testing matching logic (ref_entities vs  relationships):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if ref_entities and entities and relationships:\n",
    "    print(f\"‚úÖ All preconditions met\")\n",
    "    print(f\"   ref_entities: {ref_entities}\")\n",
    "\n",
    "    matched_count = 0\n",
    "    print(f\"\\n   Checking first 20 relationships for  matches:\")\n",
    "    for i, rel in enumerate(relationships[:20]):\n",
    "        src_id = rel.get('src_id', rel.get('source_id'))\n",
    "        tgt_id = rel.get('tgt_id', rel.get('target_id'))\n",
    "        src = entities.get(src_id, f\"Unknown({src_id})\")\n",
    "        tgt = entities.get(tgt_id, f\"Unknown({tgt_id})\")\n",
    "\n",
    "        # Test matching logic\n",
    "        matches = []\n",
    "        for ref in ref_entities:\n",
    "            if ref.lower() in src.lower():\n",
    "                matches.append(f\"'{ref}' in src '{src}'\")\n",
    "            if ref.lower() in tgt.lower():\n",
    "                matches.append(f\"'{ref}' in tgt '{tgt}'\")\n",
    "\n",
    "        if matches:\n",
    "            matched_count += 1\n",
    "            desc = rel.get('description', rel.get('type', 'RELATED'))\n",
    "            print(f\"   ‚úÖ Match {matched_count}: [{src}]  --{desc}--> [{tgt}]\")\n",
    "            print(f\"      Reason: {', '.join(matches)}\")\n",
    "\n",
    "            if matched_count >= 5:\n",
    "                break\n",
    "\n",
    "    if matched_count == 0:\n",
    "        print(f\"\\n   ‚ùå NO MATCHES FOUND!\")\n",
    "        print(f\"   This is why graph paths aren't  showing!\")\n",
    "        print(f\"\\n   Let me check entity names more  carefully:\")\n",
    "        print(f\"   ref_entities to match: {ref_entities}\")\n",
    "        print(f\"\\n   Sample entity names in graph:\")\n",
    "        for i in range(min(20, len(entities))):\n",
    "            print(f\"      {i}:  {list(entities.values())[i]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Preconditions NOT met:\")\n",
    "    print(f\"   ref_entities: {len(ref_entities)}\")\n",
    "    print(f\"   entities: {len(entities)}\")\n",
    "    print(f\"   relationships: {len(relationships)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28\n",
    "# Check the raw answer field vs citation_display\n",
    "# Run this in notebook\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç Comparing 'answer' vs 'citation_display'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Raw 'answer' field from LightRAG:\")\n",
    "print(\"-\" * 80)\n",
    "raw_answer = result.get('answer', '')\n",
    "print(f\"Length: {len(raw_answer)}\")\n",
    "print(f\"\\nContent:\\n{raw_answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n2Ô∏è‚É£ Processed 'citation_display' field:\")\n",
    "print(\"-\" * 80)\n",
    "citation_display = result.get('citation_display', '')\n",
    "print(f\"Length: {len(citation_display)}\")\n",
    "print(f\"\\nContent:\\n{citation_display}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n3Ô∏è‚É£ Testing regex on BOTH fields:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = r'#{0,3}\\s*References?:?\\s*\\n(?:- \\[(?:KG|DC)\\][^\\n]*(?:\\n|$))+'\n",
    "\n",
    "print(\"\\nTesting on raw 'answer':\")\n",
    "match1 = re.search(pattern, raw_answer, re.DOTALL)\n",
    "if match1:\n",
    "    print(f\"‚úÖ MATCHED in answer\")\n",
    "    print(f\"Matched text:\\n{match1.group()}\")\n",
    "else:\n",
    "    print(f\"‚ùå NOT matched in answer\")\n",
    "    # Check if References exists at all\n",
    "    if \"References\" in raw_answer:\n",
    "        print(f\"   But 'References' keyword EXISTS in answer\")\n",
    "        idx = raw_answer.find(\"References\")\n",
    "        print(f\"   Context around References:\")\n",
    "        print(f\"   {raw_answer[max(0,idx-50):idx+150]}\")\n",
    "    else:\n",
    "        print(f\"   'References' keyword does NOT exist in raw answer\")\n",
    "\n",
    "print(\"\\nTesting on 'citation_display':\")\n",
    "match2 = re.search(pattern, citation_display, re.DOTALL)\n",
    "if match2:\n",
    "    print(f\"‚úÖ MATCHED in citation_display\")\n",
    "    print(f\"Matched text:\\n{match2.group()}\")\n",
    "else:\n",
    "    print(f\"‚ùå NOT matched in citation_display\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29\n",
    "# Add this as a new cell after Cell 33\n",
    "print(\"üîç DEBUGGING INFO:\")\n",
    "print(f\"1. result keys: {list(result.keys())}\")\n",
    "print(f\"2. Has parsed_context: {'parsed_context' in result}\")\n",
    "if 'parsed_context' in result and result['parsed_context']:\n",
    "    print(f\"3. parsed_context keys: {list(result['parsed_context'].keys())}\")\n",
    "    print(f\"4. Number of chunks: {len(result['parsed_context'].get('chunks', []))}\")\n",
    "print(f\"5. Has citation_display: {'citation_display' in result}\")\n",
    "if 'context' in result:\n",
    "    has_entities = '-----Entities(KG)-----' in result['context']\n",
    "    has_relationships = '-----Relationships(KG)-----' in result['context']\n",
    "    print(f\"6. Context has Entities section: {has_entities}\")\n",
    "    print(f\"7. Context has Relationships section: {has_relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30\n",
    "result.keys()\n",
    "\n",
    "num_keys = len(result.keys())\n",
    "print(f\"Number of keys in result dict: {num_keys}\")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30.5: Confidence-Based Entity Filtering\n",
    "# Purpose: Filter query results by entity confidence scores\n",
    "# Why: Separate validated entities (Layer 1) from LightRAG automatic extraction (Layer 2)\n",
    "\n",
    "import re\n",
    "from lightrag import QueryParam\n",
    "\n",
    "def analyze_entity_confidence(query_text: str, min_confidence: float = 0.80):\n",
    "    \"\"\"\n",
    "    Show entity quality breakdown: validated vs automatic extraction.\n",
    "    \n",
    "    High (>=0.80): EntityExtractor + TickerValidator validated\n",
    "    Low (<0.80): LightRAG automatic (verify manually)\n",
    "    \"\"\"\n",
    "    # Validate dependencies\n",
    "    if 'rag' not in globals():\n",
    "        print(\"‚ùå Error: 'rag' not defined. Run earlier cells first (Cell 22).\")\n",
    "        return None, {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"üîç Query: {query_text}\")\n",
    "    print(f\"üìä Threshold: {min_confidence}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Run query\n",
    "    result = rag.query(query_text, param=QueryParam(mode=\"hybrid\"))\n",
    "    \n",
    "    # Extract entities: [TYPE:value|confidence:0.XX]\n",
    "    pattern = r'\\[([A-Z_]+):([^\\|]+)\\|confidence:([0-9.]+)\\]'\n",
    "    entities = {}\n",
    "    \n",
    "    for match in re.finditer(pattern, result):\n",
    "        etype, value, conf = match.groups()\n",
    "        conf = float(conf)\n",
    "        \n",
    "        if etype not in entities:\n",
    "            entities[etype] = {'high': [], 'low': []}\n",
    "        \n",
    "        category = 'high' if conf >= min_confidence else 'low'\n",
    "        if value not in [v for v, _ in entities[etype][category]]:\n",
    "            entities[etype][category].append((value, conf))\n",
    "    \n",
    "    # Summary\n",
    "    total_high = sum(len(e['high']) for e in entities.values())\n",
    "    total_low = sum(len(e['low']) for e in entities.values())\n",
    "    \n",
    "    print(f\"\\nüìà Summary: {total_high} validated, {total_low} unvalidated\")\n",
    "    \n",
    "    if entities:\n",
    "        print(f\"\\nüìã Breakdown:\")\n",
    "        for etype in sorted(entities.keys()):\n",
    "            high, low = entities[etype]['high'], entities[etype]['low']\n",
    "            if high or low:\n",
    "                print(f\"\\n  {etype}:\")\n",
    "                if high:\n",
    "                    print(f\"    ‚úÖ Validated: {[v for v, _ in high]}\")\n",
    "                if low:\n",
    "                    print(f\"    ‚ö†Ô∏è  Auto-extracted: {[v for v, _ in low]}\")\n",
    "    \n",
    "    print(f\"\\nüí° Answer:\")\n",
    "    print(\"-\"*80)\n",
    "    print(result)\n",
    "    \n",
    "    return result, entities\n",
    "\n",
    "print(\"‚úÖ Confidence filtering ready. Usage:\")\n",
    "print(\"   analyze_entity_confidence('your query here')\")\n",
    "print(\"\\nTest with: analyze_entity_confidence('What is the investment rating for Tencent Music?')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31\n",
    "# result['citation_display']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32\n",
    "\n",
    "# Only visualize if query was successful\n",
    "if result.get('status') == 'success':\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    import re\n",
    "    from pathlib import Path\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    def extract_entities_from_answer(answer_text, graph):\n",
    "        \"\"\"Extract entity names mentioned in the answer by matching graph nodes.\"\"\"\n",
    "        found_entities = []\n",
    "        text_upper = answer_text.upper()\n",
    "\n",
    "        # Organize nodes by entity type\n",
    "        nodes_by_type = {}\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            entity_type = data.get('entity_type', 'Unknown')\n",
    "            if entity_type not in nodes_by_type:\n",
    "                nodes_by_type[entity_type] = []\n",
    "            nodes_by_type[entity_type].append(node)\n",
    "\n",
    "        # Priority entity types (most likely to appear in answers)\n",
    "        priority_types = ['Organization', 'Person', 'Product', 'Technology']\n",
    "\n",
    "        # Search priority types first\n",
    "        for entity_type in priority_types:\n",
    "            if entity_type in nodes_by_type:\n",
    "                for entity in nodes_by_type[entity_type]:\n",
    "                    if len(entity) >= 2:\n",
    "                        pattern = r'\\b' + re.escape(entity.upper()) + r'\\b'\n",
    "                        if re.search(pattern, text_upper):\n",
    "                            found_entities.append(entity)\n",
    "\n",
    "        # If no priority entities found, search all other types\n",
    "        if not found_entities:\n",
    "            for entity_type, entities in nodes_by_type.items():\n",
    "                if entity_type not in priority_types:\n",
    "                    for entity in entities:\n",
    "                        if len(entity) >= 3:\n",
    "                            pattern = r'\\b' + re.escape(entity.upper()) + r'\\b'\n",
    "                            if re.search(pattern, text_upper):\n",
    "                                found_entities.append(entity)\n",
    "\n",
    "        return list(set(found_entities))\n",
    "\n",
    "    def build_subgraph(graph, seed_entities, max_hops=2, max_nodes=30):\n",
    "        \"\"\"Build k-hop neighborhood subgraph from seed entities.\"\"\"\n",
    "        if not seed_entities:\n",
    "            return nx.Graph()\n",
    "\n",
    "        # Verify seed entities exist in graph\n",
    "        seed_nodes = set(e for e in seed_entities if e in graph)\n",
    "        if not seed_nodes:\n",
    "            return nx.Graph()\n",
    "\n",
    "        # Expand to k-hop neighborhood\n",
    "        subgraph_nodes = set(seed_nodes)\n",
    "        current_frontier = set(seed_nodes)\n",
    "\n",
    "        for hop in range(max_hops):\n",
    "            if len(subgraph_nodes) >= max_nodes:\n",
    "                break\n",
    "\n",
    "            next_frontier = set()\n",
    "            for node in current_frontier:\n",
    "                neighbors = set(graph.neighbors(node))\n",
    "                next_frontier.update(neighbors)\n",
    "\n",
    "            # Add nodes within budget\n",
    "            remaining_budget = max_nodes - len(subgraph_nodes)\n",
    "            new_nodes = list(next_frontier - subgraph_nodes)[:remaining_budget]\n",
    "            subgraph_nodes.update(new_nodes)\n",
    "            current_frontier = set(new_nodes)\n",
    "\n",
    "        return graph.subgraph(subgraph_nodes).copy()\n",
    "\n",
    "    # Load knowledge graph\n",
    "    graph_path = Path(\"ice_lightrag/storage/graph_chunk_entity_relation.graphml\")\n",
    "    if graph_path.exists():\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üé® KNOWLEDGE GRAPH VISUALIZATION\")\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            G = nx.read_graphml(str(graph_path))\n",
    "            print(f\"üìä Full graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "            # Extract entities from answer\n",
    "            answer_text = result.get('answer', '')\n",
    "            seed_entities = extract_entities_from_answer(answer_text, G)\n",
    "\n",
    "            if seed_entities:\n",
    "                print(f\"üéØ Entities found in answer: {len(seed_entities)}\")\n",
    "                print(f\"   {', '.join(seed_entities[:10])}\")\n",
    "                if len(seed_entities) > 10:\n",
    "                    print(f\"   ... and {len(seed_entities) - 10} more\")\n",
    "\n",
    "                # Build subgraph (2-hop neighborhood, max 30 nodes)\n",
    "                subgraph = build_subgraph(G, seed_entities, max_hops=2, max_nodes=30)\n",
    "\n",
    "                if subgraph.number_of_nodes() > 0:\n",
    "                    print(f\"üîó Subgraph: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\")\n",
    "\n",
    "                    # Create visualization\n",
    "                    # === SIMPLIFIED MATPLOTLIB VISUALIZATION (15 lines vs 80 lines) ===\n",
    "                    \n",
    "                    plt.figure(figsize=(14, 10))\n",
    "\n",
    "                    # Color code: Red = entities in answer, Teal = 2-hop neighbors\n",
    "                    colors = ['#E74C3C' if n in seed_entities else '#2874A6' for n in subgraph.nodes()]\n",
    "\n",
    "                    # Single unified draw call (replaces 4 separate draw_networkx_* calls)\n",
    "                    nx.draw(subgraph,\n",
    "                            pos=nx.spring_layout(subgraph, k=2, iterations=50, seed=42),\n",
    "                            node_color=colors,\n",
    "                            node_size=1200,\n",
    "                            with_labels=True,\n",
    "                            font_size=9,\n",
    "                            font_weight='bold',\n",
    "                            font_color='black',\n",
    "                            edge_color='#95A5A6',\n",
    "                            width=2,\n",
    "                            alpha=0.9,\n",
    "                            arrows=True,\n",
    "                            arrowsize=15)\n",
    "\n",
    "                    plt.title(f\"Knowledge Graph: {query[:60]}... | {len(seed_entities)} seed ‚Üí {len(subgraph.nodes())} total nodes\",\n",
    "                              fontsize=14, fontweight='bold', pad=20)\n",
    "                    plt.axis('off')\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    # Legend\n",
    "                    legend_elements = [\n",
    "                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#E74C3C',\n",
    "                                   markersize=12, label='Entities in Answer'),\n",
    "                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#2874A6',\n",
    "                                   markersize=12, label='2-hop Neighbors')\n",
    "                    ]\n",
    "                    plt.legend(handles=legend_elements, loc='upper left', fontsize=10, framealpha=0.9)\n",
    "\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  No connected subgraph found for these entities\")\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è  No entities found in answer to visualize\")\n",
    "                print(\"   Tip: Try queries mentioning specific companies, people, or products\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Visualization error: {e}\")\n",
    "            print(f\"   Graph file exists but visualization failed\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Graph file not found: {graph_path}\")\n",
    "        print(f\"   Make sure REBUILD_GRAPH=True was used to create the graph\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping visualization (query did not succeed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32.2: Entity Graph Analysis (FIXED - Handles Undirected Graphs)\n",
    "# Location: ice_building_workflow.ipynb\n",
    "# Purpose: Analyze any entity in knowledge graph - relationships, metadata, sources\n",
    "# Why: Investment intelligence - understand entity connections and context\n",
    "# Relevant Files: ice_rag_fixed.py, graph_chunk_entity_relation.graphml\n",
    "\n",
    "def analyze_entity(entity_name, max_relationships=20):\n",
    "    \"\"\"\n",
    "    Analyze entity in LightRAG knowledge graph\n",
    "\n",
    "    Args:\n",
    "        entity_name: Entity to analyze (case-insensitive, fuzzy matching)\n",
    "        max_relationships: Max relationships to display per direction\n",
    "\n",
    "    Returns:\n",
    "        dict: Entity metadata, relationships, sources\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import networkx as nx\n",
    "    from difflib import get_close_matches\n",
    "    from collections import Counter\n",
    "\n",
    "    # Load graph from LightRAG storage\n",
    "    storage_path = Path(ice.config.working_dir)\n",
    "    graph_file = storage_path / 'graph_chunk_entity_relation.graphml'\n",
    "\n",
    "    if not graph_file.exists():\n",
    "        print(\"‚ùå Graph not found. Run Cell 15 to build knowledge graph first.\")\n",
    "        return {'error': 'Graph file missing', 'action': 'Run Cell 15'}\n",
    "\n",
    "    try:\n",
    "        G = nx.read_graphml(str(graph_file))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load graph: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "    if len(G.nodes()) == 0:\n",
    "        print(\"‚ùå Graph is empty. Rebuild with Cell 15.\")\n",
    "        return {'error': 'Empty graph'}\n",
    "\n",
    "    # FIX: Get all nodes (ALL nodes in GraphML are entities, not chunks)\n",
    "    # GraphML has entity_type = \"organization\", \"content\", \"service\", \"concept\", etc.\n",
    "    # There is NO entity_type = \"entity\" - that was the first bug!\n",
    "    all_entities = list(G.nodes())\n",
    "    search_lower = entity_name.lower().strip()\n",
    "\n",
    "    # Priority 1: Exact match\n",
    "    exact = [e for e in all_entities if e.lower() == search_lower]\n",
    "    if exact:\n",
    "        entity = exact[0]\n",
    "    else:\n",
    "        # Priority 2: Partial match (contains)\n",
    "        partial = [e for e in all_entities if search_lower in e.lower()]\n",
    "        if partial:\n",
    "            entity = partial[0]\n",
    "            if len(partial) > 1:\n",
    "                print(f\"‚ÑπÔ∏è  Found {len(partial)} partial matches, using: {entity}\")\n",
    "        else:\n",
    "            # Priority 3: Fuzzy similarity\n",
    "            matches = get_close_matches(entity_name, all_entities, n=5, cutoff=0.6)\n",
    "            if not matches:\n",
    "                print(f\"‚ùå Entity '{entity_name}' not found in graph\")\n",
    "                print(f\"\\nüí° Try one of these entities:\")\n",
    "                for e in all_entities[:10]:\n",
    "                    print(f\"   ‚Ä¢ {e}\")\n",
    "                return {'error': 'Entity not found', 'suggestions': all_entities[:20]}\n",
    "            entity = matches[0]\n",
    "            print(f\"‚ÑπÔ∏è  Using fuzzy match: {entity}\")\n",
    "\n",
    "    # Extract entity metadata\n",
    "    node_data = dict(G.nodes[entity])\n",
    "    entity_type = node_data.get('entity_type', 'unknown')\n",
    "    description = node_data.get('description', 'No description')\n",
    "\n",
    "    # FIX #2: Handle both directed and undirected graphs\n",
    "    # LightRAG creates undirected GraphML, but code assumed DiGraph\n",
    "    # Use .is_directed() check and .neighbors() for compatibility\n",
    "    if G.is_directed():\n",
    "        # Directed graph: preserve incoming/outgoing semantics\n",
    "        outgoing = [(entity, G.edges[entity, t].get('keywords', 'RELATES_TO'), t)\n",
    "                    for t in G.successors(entity)]\n",
    "        incoming = [(s, G.edges[s, entity].get('keywords', 'RELATES_TO'), entity)\n",
    "                    for s in G.predecessors(entity)]\n",
    "    else:\n",
    "        # Undirected graph: all neighbors are bidirectional connections\n",
    "        outgoing = [(entity, G.edges[entity, t].get('keywords', 'RELATES_TO'), t)\n",
    "                    for t in G.neighbors(entity)]\n",
    "        incoming = []  # No \"incoming\" concept for undirected graphs\n",
    "\n",
    "    # Relationship type statistics\n",
    "    outgoing_types = Counter([rel for _, rel, _ in outgoing])\n",
    "    incoming_types = Counter([rel for _, rel, _ in incoming])\n",
    "\n",
    "    # Display formatted analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîç ENTITY ANALYSIS: {entity}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nüìã Overview:\")\n",
    "    print(f\"   Type: {entity_type}\")\n",
    "    print(f\"   Description: {description[:200]}{'...' if len(description) > 200 else ''}\")\n",
    "\n",
    "    print(f\"\\nüìä Connections:\")\n",
    "    if G.is_directed():\n",
    "        print(f\"   Outgoing: {len(outgoing)} relationships\")\n",
    "        print(f\"   Incoming: {len(incoming)} relationships\")\n",
    "    else:\n",
    "        print(f\"   Total: {len(outgoing)} relationships (undirected graph)\")\n",
    "    print(f\"   Total: {len(outgoing) + len(incoming)} connections\")\n",
    "\n",
    "    # Outgoing relationships (grouped by type)\n",
    "    if outgoing:\n",
    "        # Adaptive label based on graph type\n",
    "        label = \"Relationships\" if not G.is_directed() else \"Outgoing Relationships\"\n",
    "        print(f\"\\nüì§ {label} (Top {min(max_relationships, len(outgoing))}):\")\n",
    "        for rel_type, count in outgoing_types.most_common(3):\n",
    "            print(f\"\\n   [{rel_type}] ({count}):\")\n",
    "            rels = [r for r in outgoing if r[1] == rel_type][:5]\n",
    "            for src, rel, tgt in rels:\n",
    "                print(f\"      ‚Üí {tgt}\")\n",
    "    else:\n",
    "        print(f\"\\nüì§ Relationships: None\")\n",
    "\n",
    "    # Incoming relationships (only for directed graphs)\n",
    "    if incoming:\n",
    "        print(f\"\\nüì• Incoming Relationships (Top {min(max_relationships, len(incoming))}):\")\n",
    "        for rel_type, count in incoming_types.most_common(3):\n",
    "            print(f\"\\n   [{rel_type}] ({count}):\")\n",
    "            rels = [r for r in incoming if r[1] == rel_type][:5]\n",
    "            for src, rel, tgt in rels:\n",
    "                print(f\"      ‚Üê {src}\")\n",
    "    elif G.is_directed():\n",
    "        print(f\"\\nüì• Incoming Relationships: None\")\n",
    "    # else: skip incoming section for undirected graphs\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "    # Return structured data for programmatic use\n",
    "    return {\n",
    "        'entity': entity,\n",
    "        'type': entity_type,\n",
    "        'description': description,\n",
    "        'metadata': node_data,\n",
    "        'outgoing': outgoing,\n",
    "        'incoming': incoming,\n",
    "        'outgoing_types': dict(outgoing_types),\n",
    "        'incoming_types': dict(incoming_types),\n",
    "        'total_connections': len(outgoing) + len(incoming),\n",
    "        'is_directed': G.is_directed()\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = analyze_entity('NVDA')\n",
    "# result = analyze_entity('Tencent')\n",
    "# result = analyze_entity('semiconductor', max_relationships=30)\n",
    "\n",
    "print(\"‚úÖ Entity analyzer loaded (FIXED - handles both directed and undirected graphs)\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"   analyze_entity('NVDA')\")\n",
    "print(\"   analyze_entity('Tencent')\")\n",
    "print(\"   analyze_entity('semiconductor', max_relationships=30)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = analyze_entity('tencent')\n",
    "result = analyze_entity('operating margin')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33\n",
    "# Entity & Relationship Inspection\n",
    "import networkx as nx\n",
    "\n",
    "graph_file = \"./ice_lightrag/storage/graph_chunk_entity_relation.graphml\"\n",
    "\n",
    "try:\n",
    "    G = nx.read_graphml(graph_file)\n",
    "    \n",
    "    print(f\"\\nüîç Graph Content Inspection\")\n",
    "    print(f\"üìä Entities: {len(G.nodes):,} | üîó Relationships: {len(G.edges):,}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Sample entities\n",
    "    print(f\"\\nSample Entities (first 15):\")\n",
    "    # for i, node in enumerate(list(G.nodes())[:15], 1):\n",
    "    for i, node in enumerate(list(G.nodes())[:], 1):\n",
    "        print(f\"  {i:2d}. {node}\")\n",
    "    \n",
    "    # Sample relationships\n",
    "    print(f\"\\nSample Relationships (first 15):\")\n",
    "    # for i, (src, tgt) in enumerate(list(G.edges())[:15], 1):\n",
    "    for i, (src, tgt) in enumerate(list(G.edges())[:], 1):\n",
    "        print(f\"  {i:2d}. {src} ‚Üí {tgt}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inspection complete\\n\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n‚ùå Graph not found. Run Cell 28 (data ingestion) first.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 5. Storage Architecture Validation & Monitoring -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34\n",
    "# Comprehensive storage validation and metrics\n",
    "print(f\"\\nüîç Storage Architecture Validation\")\n",
    "print(f\"‚îÅ\" * 40)\n",
    "\n",
    "if not (ice and ice.core.is_ready()):\n",
    "    raise RuntimeError(\"Cannot validate storage without initialized system\")\n",
    "\n",
    "# Get detailed storage statistics\n",
    "storage_stats = ice.core.get_storage_stats()\n",
    "graph_stats = ice.core.get_graph_stats()\n",
    "\n",
    "print(f\"üì¶ LightRAG Storage Components Status:\")\n",
    "for component_name, component_info in storage_stats['components'].items():\n",
    "    status_icon = \"‚úÖ\" if component_info['exists'] else \"‚ö†Ô∏è\"\n",
    "    size_mb = component_info['size_bytes'] / (1024 * 1024) if component_info['size_bytes'] > 0 else 0\n",
    "    \n",
    "    print(f\"  {status_icon} {component_name}:\")\n",
    "    print(f\"    File: {component_info['file']}\")\n",
    "    print(f\"    Purpose: {component_info['description']}\")\n",
    "    print(f\"    Size: {size_mb:.2f} MB\" if size_mb > 0 else \"    Size: Not created yet\")\n",
    "\n",
    "print(f\"\\nüìä Storage Summary:\")\n",
    "print(f\"  Working Directory: {storage_stats['working_dir']}\")\n",
    "print(f\"  Total Storage: {storage_stats['total_storage_bytes'] / (1024 * 1024):.2f} MB\")\n",
    "print(f\"  System Initialized: {storage_stats['is_initialized']}\")\n",
    "\n",
    "print(f\"\\nüï∏Ô∏è Knowledge Graph Status:\")\n",
    "print(f\"  Graph Ready: {graph_stats['is_ready']}\")\n",
    "if graph_stats.get('storage_indicators'):\n",
    "    indicators = graph_stats['storage_indicators']\n",
    "    print(f\"  All Components Present: {indicators['all_components_present']}\")\n",
    "    print(f\"  Chunks Storage: {indicators['chunks_file_size']:.2f} MB\")\n",
    "    print(f\"  Entity Storage: {indicators['entities_file_size']:.2f} MB\")\n",
    "    print(f\"  Relationship Storage: {indicators['relationships_file_size']:.2f} MB\")\n",
    "    print(f\"  Graph Structure: {indicators['graph_file_size']:.2f} MB\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"\\n‚úÖ Validation Checks:\")\n",
    "validation_score = 0\n",
    "max_score = 4\n",
    "\n",
    "# Check 1: System ready\n",
    "if storage_stats['is_initialized']:\n",
    "    print(f\"  ‚úÖ System initialization: PASSED\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå System initialization: FAILED\")\n",
    "\n",
    "# Check 2: Storage exists\n",
    "if storage_stats['storage_exists']:\n",
    "    print(f\"  ‚úÖ Storage directory: PASSED\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå Storage directory: FAILED\")\n",
    "\n",
    "# Check 3: Components created\n",
    "components_exist = sum(1 for c in storage_stats['components'].values() if c['exists'])\n",
    "if components_exist > 0:\n",
    "    print(f\"  ‚úÖ Storage components: PASSED ({components_exist}/4 created)\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå Storage components: FAILED (no components created)\")\n",
    "\n",
    "# Check 4: Has storage content\n",
    "if storage_stats['total_storage_bytes'] > 0:\n",
    "    print(f\"  ‚úÖ Storage content: PASSED\")\n",
    "    validation_score += 1\n",
    "else:\n",
    "    print(f\"  ‚ùå Storage content: FAILED (no data stored)\")\n",
    "\n",
    "print(f\"\\nüìä Validation Score: {validation_score}/{max_score} ({(validation_score/max_score)*100:.0f}%)\")\n",
    "\n",
    "if validation_score == max_score:\n",
    "    print(f\"üéâ All validations passed! Knowledge graph is ready for queries.\")\n",
    "elif validation_score >= max_score * 0.75:\n",
    "    print(f\"‚úÖ Most validations passed. System is functional.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Some validations failed. Check configuration and retry building.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 6. Building Metrics & Performance Analysis -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35\n",
    "# Comprehensive building session metrics\n",
    "print(f\"\\nüìä Building Session Metrics & Performance\")\n",
    "print(f\"‚îÅ\" * 50)\n",
    "\n",
    "session_metrics = {\n",
    "    'holdings_count': len(holdings),\n",
    "    'total_processing_time': 0.0,\n",
    "    'documents_processed': 0,\n",
    "    'building_successful': False\n",
    "}\n",
    "\n",
    "# Collect metrics from ingestion and building\n",
    "if 'ingestion_result' in locals() and ingestion_result:\n",
    "    if 'metrics' in ingestion_result:\n",
    "        session_metrics['ingestion_time'] = ingestion_result['metrics'].get('processing_time', 0.0)\n",
    "    session_metrics['documents_processed'] = ingestion_result.get('total_documents', 0)\n",
    "\n",
    "if 'building_result' in locals() and building_result:\n",
    "    if building_result.get('status') == 'success':\n",
    "        session_metrics['building_successful'] = True\n",
    "    if 'metrics' in building_result:\n",
    "        building_time = building_result['metrics'].get('building_time', building_result['metrics'].get('update_time', 0.0))\n",
    "        session_metrics['building_time'] = building_time\n",
    "\n",
    "# Calculate total time\n",
    "if 'pipeline_stats' in locals():\n",
    "    session_metrics['total_processing_time'] = pipeline_stats.get('processing_time', 0.0)\n",
    "\n",
    "print(f\"üéØ Session Overview:\")\n",
    "print(f\"  Holdings Processed: {session_metrics['holdings_count']}\")\n",
    "print(f\"  Documents Processed: {session_metrics['documents_processed']}\")\n",
    "print(f\"  Building Successful: {session_metrics['building_successful']}\")\n",
    "\n",
    "if session_metrics.get('ingestion_time', 0) > 0:\n",
    "    print(f\"\\n‚è±Ô∏è Performance Metrics:\")\n",
    "    print(f\"  Data Ingestion Time: {session_metrics['ingestion_time']:.2f}s\")\n",
    "    if session_metrics.get('building_time', 0) > 0:\n",
    "        print(f\"  Graph Building Time: {session_metrics['building_time']:.2f}s\")\n",
    "        print(f\"  Total Processing Time: {session_metrics['ingestion_time'] + session_metrics['building_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìà Efficiency Analysis:\")\n",
    "    if session_metrics['documents_processed'] > 0:\n",
    "        docs_per_second = session_metrics['documents_processed'] / session_metrics['ingestion_time']\n",
    "        print(f\"  Processing Rate: {docs_per_second:.2f} documents/second\")\n",
    "    \n",
    "    holdings_per_second = session_metrics['holdings_count'] / session_metrics['ingestion_time']\n",
    "    print(f\"  Holdings Rate: {holdings_per_second:.2f} holdings/second\")\n",
    "\n",
    "# Architecture efficiency comparison\n",
    "print(f\"\\nüèóÔ∏è Architecture Efficiency:\")\n",
    "print(f\"  ICE Simplified: 2,508 lines of code\")\n",
    "print(f\"  Code Reduction: 83% (vs 15,000 line original)\")\n",
    "print(f\"  Files Count: 5 core modules\")\n",
    "print(f\"  Dependencies: Direct LightRAG wrapper\")\n",
    "print(f\"  Token Efficiency: 4,000x better than GraphRAG\")\n",
    "\n",
    "# Success summary\n",
    "print(f\"\\n‚úÖ Building Session Summary:\")\n",
    "if session_metrics['building_successful']:\n",
    "    print(f\"  üéâ Knowledge graph building completed successfully\")\n",
    "    print(f\"  üìä {session_metrics['documents_processed']} documents processed\")\n",
    "    print(f\"  üöÄ System ready for intelligent investment queries\")\n",
    "    print(f\"  üí° Proceed to ice_query_workflow.ipynb for analysis\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Building completed with warnings or in demo mode\")\n",
    "    print(f\"  üìã Review configuration and API settings\")\n",
    "    print(f\"  üîß Consider running with fresh data if issues persist\")\n",
    "\n",
    "print(f\"\\nüîó Next Steps:\")\n",
    "print(f\"  1. Review building metrics and validate storage\")\n",
    "print(f\"  2. Run ice_query_workflow.ipynb for portfolio analysis\")\n",
    "print(f\"  3. Test different LightRAG query modes\")\n",
    "print(f\"  4. Monitor system performance and optimize as needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ice_lightrag.model_provider import get_llm_provider, get_extraction_temperature, get_query_temperature\n",
    "\n",
    "# Check current temperatures (get_llm_provider now returns 4 items)\n",
    "llm_func, embed_func, config, base_kwargs_template = get_llm_provider()\n",
    "\n",
    "extraction_temp = get_extraction_temperature()\n",
    "query_temp = get_query_temperature()\n",
    "\n",
    "config_temp = config.get(\"llm_model_kwargs\", {}).get(\"temperature\", \"NOT SET\")\n",
    "\n",
    "print(\"üå°Ô∏è  Temperature Configuration Check:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Entity Extraction Temperature: {extraction_temp}\")\n",
    "print(f\"Query Answering Temperature:   {query_temp}\")\n",
    "print(f\"Initial Config Temperature:    {config_temp} (should match extraction)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"‚úÖ Temperature system is using separate values for:\")\n",
    "print(\"   ‚Ä¢ Entity extraction during document ingestion\")\n",
    "print(\"   ‚Ä¢ Query answering during query processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to verify Docling URL configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß CONFIGURATION VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check environment variables\n",
    "print(\"\\n1Ô∏è‚É£ Environment Variables:\")\n",
    "print(f\"   USE_DOCLING_EMAIL: {os.environ.get('USE_DOCLING_EMAIL', 'NOT SET')}\")\n",
    "print(f\"   USE_DOCLING_URLS: {os.environ.get('USE_DOCLING_URLS',  'NOT SET')}\")\n",
    "print(f\"   USE_CRAWL4AI_LINKS: {os.environ.get('USE_CRAWL4AI_LINKS', 'NOT SET')}\")\n",
    "\n",
    "# 2. Check storage directory\n",
    "storage = Path('data/attachments')\n",
    "if storage.exists():\n",
    "    pdf_count = len(list(storage.glob('*/*/original/*.pdf')))\n",
    "    email_folders = len([d for d in storage.iterdir() if d.is_dir() and not d.name.startswith('.')])\n",
    "    print(f\"\\n2Ô∏è‚É£ PDF Storage:\")\n",
    "    print(f\"   ‚úÖ Directory exists: {storage}\")\n",
    "    print(f\"   üìä Email folders: {email_folders}\")\n",
    "    print(f\"   üìÑ Total PDFs stored: {pdf_count}\")\n",
    "\n",
    "    # Show recent PDFs\n",
    "    pdfs = sorted(storage.glob('*/*/original/*.pdf'), key=lambda p: p.stat().st_mtime, reverse=True)[:5]\n",
    "    if pdfs:\n",
    "        print(f\"\\n   üì• Recent PDFs (last 5):\")\n",
    "        for i, pdf in enumerate(pdfs, 1):\n",
    "            size_mb = pdf.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   [{i}] {pdf.name[:50]}... ({size_mb:.1f}MB)\")\n",
    "else:\n",
    "    print(f\"\\n2Ô∏è‚É£ PDF Storage:\")\n",
    "    print(f\"   ‚ùå Directory does not exist: {storage}\")\n",
    "    print(f\"   (Will be created when first URL is processed)\")\n",
    "\n",
    "# 3. Check if ICE system is initialized\n",
    "try:\n",
    "    print(f\"\\n3Ô∏è‚É£ ICE System:\")\n",
    "    if 'ice' in globals():\n",
    "        print(f\"   ‚úÖ ICE system initialized\")\n",
    "        if hasattr(ice, 'data_ingester'):\n",
    "            if hasattr(ice.data_ingester, 'link_processor'):\n",
    "                print(f\"   ‚úÖ IntelligentLinkProcessor available\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå IntelligentLinkProcessor NOT available\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è ICE system not initialized yet (run earlier cells)\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è Cannot check ICE system (run earlier cells first)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph Validation Smoke Test\n",
    "\n",
    "**Quick Quality Check**: Run 3-5 test queries to validate the built knowledge graph works correctly.\n",
    "\n",
    "### Purpose\n",
    "- Verify graph can answer queries\n",
    "- Check entity extraction worked\n",
    "- Validate relationship building\n",
    "- Ensure system is ready for full query workflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run smoke test queries to validate graph\n",
    "# print(\"üß™ Running Smoke Test Queries\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# smoke_test_queries = [\n",
    "#     \"What are the key risks for NVDA?\",\n",
    "#     \"Which companies have positive analyst ratings?\",\n",
    "#     \"Summarize the latest market trends\",\n",
    "#     \"What are TSMC's competitive advantages?\",\n",
    "#     \"Give me an overview of semiconductor supply chain risks\"\n",
    "# ]\n",
    "\n",
    "# print(f\"üìä Testing {len(smoke_test_queries)} queries to validate knowledge graph\\n\")\n",
    "\n",
    "# smoke_results = []\n",
    "# for i, query in enumerate(smoke_test_queries, 1):\n",
    "#     print(f\"{i}. Testing: '{query[:50]}...'\")\n",
    "#     try:\n",
    "#         result = ice.core.query(query, mode='hybrid')\n",
    "        \n",
    "#         if result.get('status') == 'success' and result.get('answer'):\n",
    "#             answer_len = len(result['answer'])\n",
    "#             print(f\"   ‚úÖ Success: {answer_len} chars\")\n",
    "#             smoke_results.append({\n",
    "#                 'query': query,\n",
    "#                 'status': 'SUCCESS',\n",
    "#                 'answer_length': answer_len\n",
    "#             })\n",
    "#         else:\n",
    "#             print(f\"   ‚ùå Failed: {result.get('message', 'No answer')}\")\n",
    "#             smoke_results.append({\n",
    "#                 'query': query,\n",
    "#                 'status': 'FAILED',\n",
    "#                 'error': result.get('message', 'No answer')\n",
    "#             })\n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ùå Error: {str(e)[:80]}\")\n",
    "#         smoke_results.append({\n",
    "#             'query': query,\n",
    "#             'status': 'ERROR',\n",
    "#             'error': str(e)[:100]\n",
    "#         })\n",
    "\n",
    "# # Summary\n",
    "# print(f\"\\nüìä Smoke Test Summary\")\n",
    "# print(\"=\" * 60)\n",
    "# successful = sum(1 for r in smoke_results if r['status'] == 'SUCCESS')\n",
    "# print(f\"‚úÖ Passed: {successful}/{len(smoke_results)} ({successful/len(smoke_results)*100:.0f}%)\")\n",
    "\n",
    "# if successful == len(smoke_results):\n",
    "#     print(\"\\nüéâ All smoke tests passed! Knowledge graph is ready for queries.\")\n",
    "#     print(\"   Proceed to ice_query_workflow.ipynb for full analysis\")\n",
    "# elif successful >= len(smoke_results) * 0.6:\n",
    "#     print(\"\\n‚ö†Ô∏è Most smoke tests passed, but some issues detected.\")\n",
    "#     print(\"   Graph is functional but review failed queries\")\n",
    "# else:\n",
    "#     print(\"\\n‚ùå Many smoke tests failed. Check:\")\n",
    "#     print(\"   - LightRAG storage exists and is not corrupted\")\n",
    "#     print(\"   - API keys are configured correctly\")\n",
    "#     print(\"   - Data ingestion completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query Temperature Effects Validation\n",
    "\n",
    "**Purpose**: Empirically test how query answering temperature affects response creativity and consistency.\n",
    "\n",
    "### What This Tests\n",
    "- **Query Answering Temperature**: Controls creativity in synthesizing answers from knowledge graph\n",
    "  - **Low (0.0-0.2)**: Deterministic, factual, consistent phrasing\n",
    "  - **Medium (0.3-0.5)**: Balanced creativity with factual grounding (RECOMMENDED)\n",
    "  - **High (0.6-1.0)**: Creative synthesis, varied phrasing\n",
    "\n",
    "### Test Approach\n",
    "1. Use same query across multiple temperature settings\n",
    "2. Compare answer consistency and creativity\n",
    "3. Validate that facts remain accurate across all temperatures\n",
    "\n",
    "### Expected Outcomes\n",
    "- **Temp 0.0**: Identical or near-identical answers (reproducible)\n",
    "- **Temp 0.5**: Varied phrasing but consistent facts (creative synthesis)\n",
    "- **Temp 1.0**: Maximum creativity, most varied responses\n",
    "\n",
    "### Note\n",
    "Entity extraction temperature is already fixed during graph building phase. This test only affects how the system synthesizes answers from the built knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Query Temperature Effects Test\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# # Run same query multiple times per temperature to demonstrate effects\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# import os\n",
    "# from src.ice_lightrag.ice_rag_fixed import JupyterICERAG  # ‚Üê MOVED OUTSIDE LOOP\n",
    "\n",
    "# # Test configuration\n",
    "# TEST_QUERY = \"What was tencent's operating margin in Q2 2025?\"\n",
    "# TEMPERATURES_TO_TEST = [0.0, 0.5, 1.0]\n",
    "# RUNS_PER_TEMPERATURE = 2\n",
    "# TEST_MODE = \"hybrid\"\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"üå°Ô∏è  QUERY TEMPERATURE EFFECTS TEST\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"üìù Query: {TEST_QUERY}\")\n",
    "# print(f\"üéØ Mode: {TEST_MODE}\")\n",
    "# print(f\"üß™ Testing: {TEMPERATURES_TO_TEST} ({RUNS_PER_TEMPERATURE} runs each)\")\n",
    "# print()\n",
    "\n",
    "# results = {}\n",
    "\n",
    "# for temp in TEMPERATURES_TO_TEST:\n",
    "#     print(f\"\\n{'‚îÄ'*80}\")\n",
    "#     print(f\"üå°Ô∏è  Temperature: {temp} ({RUNS_PER_TEMPERATURE} runs)\")\n",
    "#     print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "#     # Set temperature for THIS iteration only\n",
    "#     os.environ['ICE_LLM_TEMPERATURE_QUERY_ANSWERING'] = str(temp)\n",
    "    \n",
    "#     # Create fresh instance (import already done above)\n",
    "#     temp_ice = JupyterICERAG()\n",
    "    \n",
    "#     if not await temp_ice._ensure_initialized():\n",
    "#         print(f\"‚ùå Failed to initialize ICE\")\n",
    "#         results[temp] = None\n",
    "#         continue\n",
    "    \n",
    "#     # Disable cache on this instance\n",
    "#     try:\n",
    "#         if hasattr(temp_ice._rag, 'llm_response_cache'):\n",
    "#             temp_ice._rag.llm_response_cache.global_config[\"enable_llm_cache\"] = False\n",
    "#             print(f\"‚úÖ ICE initialized with query temp = {temp} (cache disabled)\")\n",
    "#         else:\n",
    "#             print(f\"‚úÖ ICE initialized with query temp = {temp}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Cache disable failed: {e}, continuing anyway\")\n",
    "#         print(f\"‚úÖ ICE initialized with query temp = {temp}\")\n",
    "    \n",
    "#     # Run multiple times\n",
    "#     run_results = []\n",
    "#     for run_num in range(1, RUNS_PER_TEMPERATURE + 1):\n",
    "#         print(f\"\\n  üìù Run {run_num}/{RUNS_PER_TEMPERATURE}...\")\n",
    "        \n",
    "#         # Run query\n",
    "#         try:\n",
    "#             result = await temp_ice.query(question=TEST_QUERY, mode=TEST_MODE)\n",
    "            \n",
    "#             if result.get('status') == 'success':\n",
    "#                 answer = result.get('answer', result.get('result', ''))\n",
    "#                 run_results.append(answer)\n",
    "#                 print(f\"  ‚úÖ Got answer ({len(answer)} chars)\")\n",
    "#             else:\n",
    "#                 print(f\"  ‚ùå Failed: {result.get('message', 'Unknown error')}\")\n",
    "#                 run_results.append(None)\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ‚ùå Error: {e}\")\n",
    "#             run_results.append(None)\n",
    "    \n",
    "#     results[temp] = run_results\n",
    "\n",
    "# # Display comparison\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"üìä TEMPERATURE EFFECTS COMPARISON\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# for temp in TEMPERATURES_TO_TEST:\n",
    "#     run_results = results.get(temp)\n",
    "#     if not run_results or None in run_results:\n",
    "#         print(f\"\\nüå°Ô∏è  Temp {temp}: ‚ùå Failed\")\n",
    "#         continue\n",
    "    \n",
    "#     print(f\"\\nüå°Ô∏è  Temperature: {temp}\")\n",
    "#     print(\"‚îÄ\" * 70)\n",
    "    \n",
    "#     for i, answer in enumerate(run_results, 1):\n",
    "#         preview = answer[:120] + \"...\" if len(answer) > 120 else answer\n",
    "#         print(f\"  Run {i}: {preview}\")\n",
    "    \n",
    "#     if len(run_results) == 2:\n",
    "#         identical = run_results[0] == run_results[1]\n",
    "#         symbol = \"‚úÖ IDENTICAL\" if identical else \"üîÑ DIFFERENT\"\n",
    "#         print(f\"\\n  {symbol}: {identical}\")\n",
    "        \n",
    "#         if not identical:\n",
    "#             chars_diff = sum(1 for a, b in zip(run_results[0], run_results[1]) if a != b)\n",
    "#             total_chars = min(len(run_results[0]), len(run_results[1]))\n",
    "#             pct = (chars_diff / total_chars * 100) if total_chars > 0 else 0\n",
    "#             print(f\"  üìè Difference: {chars_diff}/{total_chars} chars ({pct:.1f}%)\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"üí° KEY OBSERVATIONS\")\n",
    "# print(\"=\"*80)\n",
    "# print(\"‚úÖ Temp 0.0: Runs should be IDENTICAL (deterministic)\")\n",
    "# print(\"üîÑ Temp 0.5: Runs may vary slightly (balanced)\")\n",
    "# print(\"üåà Temp 1.0: Runs should DIFFER noticeably (creative)\")\n",
    "# print()\n",
    "# print(\"‚ö†Ô∏è  All temperatures provide same FACTS, only phrasing varies\")\n",
    "# print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Module Reload Utility - Troubleshooting Stale Code\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# # Purpose: Reload ICE modules after editing .py files (fixes kernel caching)\n",
    "# # Usage: Run this cell if you edited ice_rag_fixed.py, model_provider.py, etc.\n",
    "# #        and want changes to take effect WITHOUT restarting kernel\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# import sys\n",
    "# import importlib\n",
    "\n",
    "# def reload_ice_modules():\n",
    "#     \"\"\"Reload ICE core modules in dependency order\"\"\"\n",
    "#     modules_to_reload = [\n",
    "#         'src.ice_lightrag.model_provider',      # No dependencies\n",
    "#         'src.ice_lightrag.ice_rag_fixed',       # Depends on model_provider\n",
    "#         'src.ice_lightrag.context_parser',      # Traceability parser\n",
    "#     ]\n",
    "    \n",
    "#     print(\"üîÑ Reloading ICE modules...\")\n",
    "#     for module_name in modules_to_reload:\n",
    "#         if module_name in sys.modules:\n",
    "#             importlib.reload(sys.modules[module_name])\n",
    "#             print(f\"  ‚úÖ Reloaded: {module_name}\")\n",
    "#         else:\n",
    "#             print(f\"  ‚è≠Ô∏è  Skipped (not imported): {module_name}\")\n",
    "#     print(\"‚úÖ Reload complete! Kernel now using latest code.\\n\")\n",
    "\n",
    "# # Uncomment to enable auto-reload for ALL future code changes:\n",
    "# # %load_ext autoreload\n",
    "# # %autoreload 2\n",
    "\n",
    "# # Run reload now:\n",
    "# reload_ice_modules()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Entity Extraction Temperature Effects Test\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# # Test how extraction temperature affects entity/relationship extraction\n",
    "# # Creates isolated graphs for each temperature to enable side-by-side comparison\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# import os\n",
    "# import email\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# from pathlib import Path\n",
    "# from bs4 import BeautifulSoup\n",
    "# from src.ice_lightrag.ice_rag_fixed import JupyterICERAG  # Import OUTSIDE loop\n",
    "\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# # Email Text Extraction Helper (Production-Aligned)\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# def extract_email_text(email_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Extract full text content from email file.\n",
    "#     Handles HTML-only, plaintext-only, and multipart emails.\n",
    "\n",
    "#     Strategy:\n",
    "#     1. Prefer text/plain parts (if available)\n",
    "#     2. Fall back to HTML ‚Üí text conversion (for HTML-only emails)\n",
    "#     3. Combine multiple parts for multipart emails\n",
    "\n",
    "#     Args:\n",
    "#         email_path: Path to .eml file\n",
    "\n",
    "#     Returns:\n",
    "#         Full email text with subject line\n",
    "#     \"\"\"\n",
    "#     with open(email_path, 'r', encoding='utf-8') as f:\n",
    "#         msg = email.message_from_file(f)\n",
    "\n",
    "#     subject = msg.get('Subject', 'No Subject')\n",
    "#     text_parts = []\n",
    "#     html_parts = []\n",
    "\n",
    "#     # Collect all text and HTML parts\n",
    "#     if msg.is_multipart():\n",
    "#         for part in msg.walk():\n",
    "#             content_type = part.get_content_type()\n",
    "#             payload = part.get_payload(decode=True)\n",
    "#             if not payload:\n",
    "#                 continue\n",
    "\n",
    "#             if content_type == \"text/plain\":\n",
    "#                 text_parts.append(payload.decode('utf-8', errors='replace'))\n",
    "#             elif content_type == \"text/html\":\n",
    "#                 html_parts.append(payload.decode('utf-8', errors='replace'))\n",
    "#     else:\n",
    "#         payload = msg.get_payload(decode=True)\n",
    "#         if payload:\n",
    "#             content_type = msg.get_content_type()\n",
    "#             if content_type == \"text/plain\":\n",
    "#                 text_parts.append(payload.decode('utf-8', errors='replace'))\n",
    "#             elif content_type == \"text/html\":\n",
    "#                 html_parts.append(payload.decode('utf-8', errors='replace'))\n",
    "\n",
    "#     # Prefer plaintext, fall back to HTML ‚Üí text\n",
    "#     if text_parts:\n",
    "#         body = \"\\n\\n\".join(text_parts)\n",
    "#     elif html_parts:\n",
    "#         # Convert HTML to text using BeautifulSoup (production standard)\n",
    "#         body_parts = []\n",
    "#         for html in html_parts:\n",
    "#             try:\n",
    "#                 soup = BeautifulSoup(html, 'html.parser')\n",
    "#                 text = soup.get_text(separator='\\n', strip=True)\n",
    "#                 body_parts.append(text)\n",
    "#             except Exception as e:\n",
    "#                 # Fall back to raw HTML if BeautifulSoup fails\n",
    "#                 body_parts.append(html)\n",
    "#         body = \"\\n\\n\".join(body_parts)\n",
    "#     else:\n",
    "#         body = \"\"\n",
    "\n",
    "#     return f\"Subject: {subject}\\n\\n{body}\"\n",
    "\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# # Test Configuration\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# TEMPERATURES = [0.0, 0.5, 1.0]\n",
    "# TEST_EMAIL = \"data/emails_samples/Tencent Q2 2025 Earnings.eml\"\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"üß¨ ENTITY EXTRACTION TEMPERATURE EFFECTS TEST\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"üìù Test Document: {TEST_EMAIL}\")\n",
    "# print(f\"üå°Ô∏è  Temperatures: {TEMPERATURES}\")\n",
    "# print(f\"üîß Isolation: WORKSPACE per temperature (prevents document deduplication conflicts)\")\n",
    "# print()\n",
    "\n",
    "# results = {}\n",
    "\n",
    "# for temp in TEMPERATURES:\n",
    "#     print(f\"\\n{'‚îÄ'*80}\")\n",
    "#     print(f\"üå°Ô∏è  Processing Temperature: {temp}\")\n",
    "#     print(f\"{'‚îÄ'*80}\")\n",
    "\n",
    "#     reload_ice_modules()\n",
    "\n",
    "#     # Set extraction temperature\n",
    "#     os.environ['ICE_LLM_TEMPERATURE_ENTITY_EXTRACTION'] = str(temp)\n",
    "\n",
    "#     # üîß FIX: Isolate document status tracking per temperature iteration\n",
    "#     # This prevents \"document already exists\" errors across temp tests\n",
    "#     os.environ['WORKSPACE'] = f\"temp_{temp}\"\n",
    "\n",
    "#     # Create isolated ICE instance with separate working directory\n",
    "#     working_dir = f\"extraction_temp_test/temp_{temp}\"\n",
    "#     print(f\"  üìÅ Working directory: {working_dir}\")\n",
    "#     print(f\"  üîß Workspace: {os.environ['WORKSPACE']}\")\n",
    "\n",
    "#     ice_temp = JupyterICERAG(working_dir=working_dir)\n",
    "\n",
    "#     if not await ice_temp._ensure_initialized():\n",
    "#         print(f\"  ‚ùå Failed to initialize ICE at temp={temp}\")\n",
    "#         results[temp] = None\n",
    "#         continue\n",
    "\n",
    "#     print(f\"  ‚úÖ ICE initialized with extraction temp = {temp}\")\n",
    "\n",
    "#     # Add test document to isolated graph\n",
    "#     try:\n",
    "#         print(f\"  üì® Adding document: {TEST_EMAIL}\")\n",
    "\n",
    "#         # Extract email text using production-aligned helper\n",
    "#         full_text = extract_email_text(TEST_EMAIL)\n",
    "#         print(f\"  üìä Extracted {len(full_text)} characters ({len(full_text.split())} words)\")\n",
    "\n",
    "#         # Call add_document with correct parameters (text is required)\n",
    "#         result = await ice_temp.add_document(\n",
    "#             text=full_text,\n",
    "#             doc_type=\"email\",\n",
    "#             file_path=TEST_EMAIL  # For traceability\n",
    "#         )\n",
    "\n",
    "#         if result.get('status') != 'success':\n",
    "#             print(f\"  ‚ùå Document addition failed: {result.get('message')}\")\n",
    "#             results[temp] = None\n",
    "#             continue\n",
    "\n",
    "#         print(f\"  ‚úÖ Document added successfully\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ‚ùå Error adding document: {e}\")\n",
    "#         results[temp] = None\n",
    "#         continue\n",
    "\n",
    "#     # Read graph from GraphML file\n",
    "#     # Note: When WORKSPACE is set, LightRAG creates subdirectory: working_dir/workspace/graph_*.graphml\n",
    "#     try:\n",
    "#         workspace = f\"temp_{temp}\"\n",
    "#         graph_path = Path(working_dir) / workspace / \"graph_chunk_entity_relation.graphml\"\n",
    "\n",
    "#         if not graph_path.exists():\n",
    "#             print(f\"  ‚ö†Ô∏è  Graph file not found: {graph_path}\")\n",
    "#             results[temp] = None\n",
    "#             continue\n",
    "\n",
    "#         graph = nx.read_graphml(graph_path)\n",
    "#         print(f\"  üìä Graph loaded from {graph_path}\")\n",
    "\n",
    "#         # Extract entity information\n",
    "#         all_nodes = list(graph.nodes())\n",
    "#         all_edges = list(graph.edges())\n",
    "\n",
    "#         # Get entity names (node names in LightRAG graph)\n",
    "#         entity_names = sorted(set(all_nodes))\n",
    "\n",
    "#         # Capture comprehensive metrics\n",
    "#         results[temp] = {\n",
    "#             'total_entities': len(entity_names),\n",
    "#             'total_relationships': len(all_edges),\n",
    "#             'entity_names': entity_names,\n",
    "#             'sample_entities': entity_names[:15]  # First 15 for preview\n",
    "#         }\n",
    "\n",
    "#         print(f\"  üìà Extracted {len(entity_names)} entities\")\n",
    "#         print(f\"  üìà Extracted {len(all_edges)} relationships\")\n",
    "#         print(f\"  üìù Sample entities: {', '.join(entity_names[:5])}...\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ‚ùå Error reading graph: {e}\")\n",
    "#         results[temp] = None\n",
    "#         continue\n",
    "\n",
    "# # Filter out failed temperatures\n",
    "# valid_temps = [t for t in TEMPERATURES if results.get(t) is not None]\n",
    "\n",
    "# if not valid_temps:\n",
    "#     print(\"\\n‚ùå All temperature tests failed\")\n",
    "# else:\n",
    "#     # Display quantitative comparison\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"üìä QUANTITATIVE COMPARISON\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "#     comparison_df = pd.DataFrame({\n",
    "#         'Temperature': valid_temps,\n",
    "#         'Entities': [results[t]['total_entities'] for t in valid_temps],\n",
    "#         'Relationships': [results[t]['total_relationships'] for t in valid_temps]\n",
    "#     })\n",
    "\n",
    "#     print(comparison_df.to_string(index=False))\n",
    "#     print()\n",
    "\n",
    "#     # Display qualitative comparison\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"üìä ENTITY PRESENCE MATRIX - Extraction by Temperature\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"Entities sorted by: Frequency (how many temps extracted it), then alphabetically\")\n",
    "#     print()\n",
    "\n",
    "#     # Collect all unique entities\n",
    "#     all_entities = set()\n",
    "#     for temp in valid_temps:\n",
    "#         all_entities.update(results[temp]['entity_names'])\n",
    "\n",
    "#     # Build entity data with frequency and presence\n",
    "#     entity_data = []\n",
    "#     for entity in all_entities:\n",
    "#         presence = {temp: entity in results[temp]['entity_names'] for temp in valid_temps}\n",
    "#         frequency = sum(presence.values())\n",
    "#         entity_data.append({\n",
    "#             'name': entity,\n",
    "#             'frequency': frequency,\n",
    "#             'presence': presence\n",
    "#         })\n",
    "\n",
    "#     # Sort by frequency (descending), then alphabetically\n",
    "#     entity_data.sort(key=lambda x: (-x['frequency'], x['name']))\n",
    "\n",
    "#     # Build and display table\n",
    "#     print(f\"{'Entity':<40} \" + \" \".join([f\"Temp {t:>3.1f}\" for t in valid_temps]))\n",
    "#     print(\"‚îÄ\" * 80)\n",
    "\n",
    "#     for data in entity_data:\n",
    "#         entity_display = data['name'][:37] + '...' if len(data['name']) > 40 else data['name']\n",
    "#         marks = \" \".join([\"    ‚úÖ    \" if data['presence'][t] else \"    ‚ùå    \" for t in valid_temps])\n",
    "#         print(f\"{entity_display:<40} {marks}\")\n",
    "\n",
    "#     print(f\"\\nüìä Total unique entities: {len(all_entities)}\")\n",
    "#     print()\n",
    "\n",
    "#     # Summary insights\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"üîç QUALITATIVE COMPARISON - Unique Entities per Temperature\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "#     for temp in valid_temps:\n",
    "#         # Find entities unique to this temperature\n",
    "#         unique_entities = set(results[temp]['entity_names'])\n",
    "#         for other_temp in valid_temps:\n",
    "#             if other_temp != temp:\n",
    "#                 unique_entities -= set(results[other_temp]['entity_names'])\n",
    "\n",
    "#         print(f\"\\nüå°Ô∏è  Temperature {temp}:\")\n",
    "#         print(f\"   Unique entities: {len(unique_entities)}\")\n",
    "#         if unique_entities:\n",
    "#             print(f\"   Examples: {', '.join(sorted(unique_entities)[:10])}\")\n",
    "\n",
    "#     # Common entities across all temperatures\n",
    "#     print(f\"\\n{'‚îÄ'*80}\")\n",
    "#     common_entities = set(results[valid_temps[0]]['entity_names'])\n",
    "#     for temp in valid_temps[1:]:\n",
    "#         common_entities &= set(results[temp]['entity_names'])\n",
    "\n",
    "#     print(f\"ü§ù Common entities across all temperatures: {len(common_entities)}\")\n",
    "#     if common_entities:\n",
    "#         print(f\"   Examples: {', '.join(sorted(common_entities)[:10])}\")\n",
    "\n",
    "#     # Entity Presence Matrix\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     # Unique Entities Matrix (Temperature-Specific)\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"üìä UNIQUE ENTITIES MATRIX - Temperature-Specific Extractions\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"Entities sorted by: Frequency (how many temps extracted it), then alphabetically\")\n",
    "#     print(\"Shows only entities NOT common across all temperatures\")\n",
    "#     print()\n",
    "\n",
    "#     # Filter out common entities (those present in ALL temps)\n",
    "#     unique_entity_data = [\n",
    "#         data for data in entity_data \n",
    "#         if data['frequency'] < len(valid_temps)  # Not in all temps\n",
    "#     ]\n",
    "\n",
    "#     if not unique_entity_data:\n",
    "#         print(\"‚ÑπÔ∏è  All entities are common across all temperatures\")\n",
    "#     else:\n",
    "#         # Build and display table (same format as full matrix)\n",
    "#         print(f\"{'Entity':<40} \" + \" \".join([f\"Temp {t:>3.1f}\" for t in valid_temps]))\n",
    "#         print(\"‚îÄ\" * 80)\n",
    "\n",
    "#         for data in unique_entity_data:\n",
    "#             entity_display = data['name'][:37] + '...' if len(data['name']) > 40 else data['name']\n",
    "#             marks = \" \".join([\"    ‚úÖ    \" if data['presence'][t] else \"    ‚ùå    \" for t in valid_temps])\n",
    "#             print(f\"{entity_display:<40} {marks}\")\n",
    "\n",
    "#         print(f\"\\nüìä Total unique entities: {len(unique_entity_data)}\")\n",
    "#         print(f\"   (Excludes {len(entity_data) - len(unique_entity_data)} common entities present in all temps)\")\n",
    "#         print()\n",
    "\n",
    "#     print(\"üí° KEY INSIGHTS\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"‚úÖ Temp 0.0: Deterministic extraction (most reproducible)\")\n",
    "#     print(\"üîÑ Temp 0.3: Balanced extraction (current default)\")\n",
    "#     print(\"üåà Temp 0.5: Creative extraction (more implied entities)\")\n",
    "#     print()\n",
    "#     print(\"‚ö†Ô∏è  Higher temperatures ‚Üí more entities but less reproducibility\")\n",
    "#     print(\"‚ö†Ô∏è  For backtesting & compliance: Use temp ‚â§0.2\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# # Cleanup: Restore production environment\n",
    "# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# os.environ.pop('WORKSPACE', None)  # Remove workspace isolation\n",
    "# print(\"\\n‚úÖ Testing complete - workspace isolation removed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

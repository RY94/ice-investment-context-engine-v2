# RAGAS Overview: Architecture and Concepts

**Location**: `/project_information/about_ragas/01_overview.md`
**Purpose**: Foundational understanding of RAGAS framework
**Last Updated**: 2025-11-07

---

## ğŸ¯ What is RAGAS?

**RAGAS** (Retrieval-Augmented Generation Assessment) is an **open-source Python framework** specifically designed to evaluate the quality of Retrieval-Augmented Generation (RAG) systems.

### Key Characteristics

- **License**: Apache-2.0
- **GitHub**: explodinggradients/ragas (11.4k+ stars, 260+ contributors)
- **Latest Release**: v0.3.8 (as of 2025-11-07)
- **Primary Focus**: RAG pipeline evaluation (not general LLM evaluation)
- **Core Innovation**: "Reference-free" evaluation (doesn't always need human-annotated ground truth)

---

## ğŸ” Why RAGAS Exists

### The Problem RAGAS Solves

Traditional LLM evaluation methods fall short for RAG systems because:

1. **Retrieval Quality â‰  Generation Quality**: A system can retrieve perfect documents but generate poor answers
2. **Hallucination Risk**: LLMs can fabricate information even with good context
3. **Manual Evaluation Doesn't Scale**: Human review of every RAG output is expensive and slow
4. **Black-Box Problem**: Hard to diagnose WHY a RAG system fails (retrieval vs generation vs both?)

### RAGAS Solution

RAGAS addresses these challenges by:

âœ… **Component-Level Evaluation**: Separately assesses retrieval and generation quality
âœ… **Automated Metrics**: Uses LLMs as evaluators (LLM-as-Judge pattern)
âœ… **Synthetic Test Generation**: Can create test datasets automatically
âœ… **Framework Integration**: Works with LangChain, LlamaIndex, and custom RAG systems

---

## ğŸ—ï¸ Architecture Overview

### High-Level Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAGAS Framework                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Metrics    â”‚  â”‚   TestSet    â”‚  â”‚ Integrations â”‚     â”‚
â”‚  â”‚   Engine     â”‚  â”‚  Generator   â”‚  â”‚    Layer     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â–²                 â–²                  â–²              â”‚
â”‚         â”‚                 â”‚                  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                   â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                   â”‚   Evaluation  â”‚                         â”‚
â”‚                   â”‚     Core      â”‚                         â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                          â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â–¼                                  â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  LLM        â”‚                   â”‚  Embeddings â”‚        â”‚
â”‚  â”‚  Providers  â”‚                   â”‚  Providers  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Source Code Organization

Located in `/src/ragas/` with the following structure:

```
ragas/
â”œâ”€â”€ metrics/              # Evaluation metrics implementations
â”‚   â”œâ”€â”€ _answer_correctness.py
â”‚   â”œâ”€â”€ _faithfulness.py
â”‚   â”œâ”€â”€ _context_precision.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ llms/                 # LLM provider wrappers
â”‚   â”œâ”€â”€ langchain.py
â”‚   â”œâ”€â”€ llama_index.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ embeddings/           # Embedding model interfaces
â”œâ”€â”€ testset/              # Synthetic test generation
â”œâ”€â”€ integrations/         # Third-party integrations
â”œâ”€â”€ evaluation.py         # Core evaluation orchestration
â”œâ”€â”€ dataset.py            # Dataset handling and validation
â”œâ”€â”€ executor.py           # Async execution engine
â””â”€â”€ cost.py              # Token usage tracking
```

---

## ğŸ­ Core Components

### 1. Metrics Engine

**Purpose**: Calculates evaluation scores across multiple dimensions

**Key Metrics Categories**:
- **Retrieval**: Context Precision, Context Recall, Context Relevance
- **Generation**: Faithfulness, Answer Relevancy, Answer Correctness
- **RAG-Specific**: Noise Sensitivity, Response Groundedness
- **Traditional NLP**: BLEU, ROUGE, CHRF, Semantic Similarity
- **Agentic**: Tool Call Accuracy, Agent Goal Accuracy, Topic Adherence

**Implementation Pattern**:
```python
from ragas.metrics import Faithfulness, AnswerRelevancy

class Metric:
    name: str
    _required_columns: tuple  # Dataset fields needed

    async def _ascore(self, row, callbacks) -> float:
        # LLM-based scoring logic
        pass
```

### 2. TestSet Generator

**Purpose**: Automatically create evaluation datasets from source documents

**Capabilities**:
- Generates diverse question types (simple, reasoning, multi-hop)
- Creates reference answers
- Produces distractor documents
- Supports multiple languages

**Workflow**:
```
Documents â†’ Chunk â†’ Extract Entities â†’ Generate Questions â†’ Create Test Set
```

### 3. Evaluation Core

**Purpose**: Orchestrates metric calculation across datasets

**Key Classes**:
- `EvaluationDataset`: Structured container for evaluation data
- `evaluate()`: Main function to run metrics on datasets
- `SingleTurnSample`: Single query-response evaluation unit
- `MultiTurnSample`: Conversational evaluation unit

**Usage Pattern**:
```python
from ragas import evaluate, EvaluationDataset

results = evaluate(
    dataset=eval_dataset,
    metrics=[faithfulness, answer_relevancy],
    llm=evaluator_llm,
    callbacks=[tracer]  # Optional observability
)
```

### 4. LLM Provider Layer

**Purpose**: Abstract LLM provider differences (OpenAI, Anthropic, local models)

**Supported Wrappers**:
- `LangchainLLMWrapper`: For LangChain LLMs
- `LlamaIndexLLMWrapper`: For LlamaIndex LLMs
- Direct API wrappers: OpenAI, Azure, Anthropic

**Why This Matters**:
- JSON output formatting varies by provider
- Different models have different prompt requirements
- Wrapper handles schema validation and retry logic

### 5. Integrations Layer

**Purpose**: Connect RAGAS with observability and development platforms

**Supported Platforms**:
- **LangSmith**: LangChain's tracing platform
- **Opik**: Open-source experiment tracking
- **Helicone**: LLM observability and caching
- **Arize Phoenix**: ML observability
- **Athina**: LLM testing platform

---

## ğŸ¯ When to Use RAGAS

### âœ… RAGAS is Great For:

1. **RAG Pipeline Evaluation**: If you built a RAG system and need systematic evaluation
2. **Component-Level Diagnosis**: When you need to isolate retrieval vs generation failures
3. **Automated Testing**: CI/CD integration for regression detection
4. **Benchmarking**: Comparing multiple RAG configurations or LLM models
5. **Research & Development**: Iterating on RAG architectures with quantitative feedback

### âŒ RAGAS is NOT Ideal For:

1. **Non-RAG LLM Apps**: General chatbots, text generation, summarization (use DeepEval instead)
2. **Real-Time Production Monitoring**: High-latency LLM-as-Judge pattern (use TruLens instead)
3. **Business Outcome Metrics**: Investment decision quality, user satisfaction (needs human evaluation)
4. **Zero-Ground-Truth Scenarios**: Most metrics still need reference answers
5. **Ultra-Low-Cost Requirements**: RAGAS makes many LLM calls (expensive)

---

## ğŸ”„ RAGAS Workflow (End-to-End)

### Standard Evaluation Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Prepare Your RAG System                                  â”‚
â”‚  - Build retriever (vector DB, BM25, hybrid)                      â”‚
â”‚  - Build generator (LLM with prompt template)                     â”‚
â”‚  - Test that it returns responses                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Collect Evaluation Data                                  â”‚
â”‚  - Run queries through your RAG system                            â”‚
â”‚  - Capture: query, retrieved_contexts, response                   â”‚
â”‚  - Optionally: Add reference answers (ground truth)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Create EvaluationDataset                                 â”‚
â”‚  - Format data into RAGAS schema                                  â”‚
â”‚  - Fields: user_input, response, retrieved_contexts, reference    â”‚
â”‚  - Load from dict, CSV, JSON, or HuggingFace                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Select Metrics                                           â”‚
â”‚  - Choose based on what you want to measure                       â”‚
â”‚  - Retrieval: context_precision, context_recall                   â”‚
â”‚  - Generation: faithfulness, answer_relevancy                     â”‚
â”‚  - Hybrid: answer_correctness, factual_correctness                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Run Evaluation                                           â”‚
â”‚  - Call evaluate() with dataset + metrics                         â”‚
â”‚  - RAGAS makes LLM calls to score each sample                     â”‚
â”‚  - Handles async execution, batching, retries                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: Analyze Results                                          â”‚
â”‚  - Convert to pandas: results.to_pandas()                         â”‚
â”‚  - Identify low-scoring samples                                   â”‚
â”‚  - Debug failures (retrieval vs generation issues)                â”‚
â”‚  - Track token costs: results.total_tokens                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 7: Iterate & Improve                                        â”‚
â”‚  - Improve retrieval (chunking, indexing, reranking)              â”‚
â”‚  - Improve generation (prompts, LLM selection, context filtering) â”‚
â”‚  - Re-evaluate and compare versions                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š RAGAS vs Manual Evaluation

| **Aspect** | **Manual Evaluation** | **RAGAS Evaluation** |
|------------|-----------------------|----------------------|
| **Speed** | Slow (hours per 20 queries) | Fast (minutes per 100 queries) |
| **Cost** | High (human time) | Medium (LLM API calls) |
| **Consistency** | Variable (human bias) | Consistent (algorithmic) |
| **Scalability** | Poor | Excellent |
| **Business Metrics** | âœ… Can capture | âŒ Cannot capture |
| **Technical Metrics** | âŒ Hard to quantify | âœ… Precise scores |
| **Debugging** | âœ… Contextual insights | âŒ Opaque scores |
| **CI/CD Integration** | âŒ Not feasible | âœ… Automated |

**Conclusion**: Use BOTH - RAGAS for automated technical metrics, manual evaluation for business quality

---

## ğŸ”® Evolution and Roadmap

### RAGAS Timeline

- **2023 Q3**: Initial release (v0.1.0) - Basic RAG metrics
- **2024 Q1**: Agent evaluation metrics added
- **2024 Q3**: Multi-turn conversation support
- **2025 Q1**: v0.3.0 - Synthetic testset generation improvements
- **2025 Q4** (Planned): Real-time evaluation, advanced agentic metrics

### Current State (v0.3.8)

âœ… **Mature**:
- Core RAG metrics (Faithfulness, Context Precision, Answer Relevancy)
- LangChain/LlamaIndex integration
- HuggingFace dataset support
- Basic observability integration

âš ï¸ **In Development**:
- Agent evaluation metrics (stable but evolving)
- Multi-language support (experimental)
- Custom metric creation (API stabilizing)

âŒ **Not Yet Supported**:
- Streaming evaluation
- Multi-modal RAG (images, audio)
- Fine-grained source attribution

---

## ğŸ“ Learning Path

**Beginner** (1-2 hours):
1. Read this overview
2. Run official quickstart: `ragas quickstart rag_eval`
3. Evaluate a simple RAG with 5 queries

**Intermediate** (1 day):
1. Read `02_metrics_deep_dive.md`
2. Implement custom RAG evaluation with 20+ queries
3. Integrate with LangSmith for tracing

**Advanced** (1 week):
1. Read `04_implementation_guide.md` and `05_challenges_and_pitfalls.md`
2. Deploy RAGAS in production CI/CD pipeline
3. Create custom metrics for domain-specific needs

---

## ğŸ“š Additional Resources

### Official Documentation
- **Main Docs**: https://docs.ragas.io/
- **API Reference**: https://docs.ragas.io/en/latest/references/
- **Examples**: https://github.com/explodinggradients/ragas/tree/main/docs/howtos/

### Community
- **Discord**: https://discord.gg/5djav8GGNZ (800+ members)
- **GitHub Discussions**: https://github.com/explodinggradients/ragas/discussions

### Related Papers
- **Original Paper**: [Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)
- **RAG Survey**: [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)

---

**Next**: Read `02_metrics_deep_dive.md` to understand each metric in detail.

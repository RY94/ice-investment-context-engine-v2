# docs/LOCAL_LLM_GUIDE.md
# Complete local LLM setup guide using Ollama for cost-optimized ICE deployment
# Covers installation, model selection, hybrid configurations, and integration patterns
# Relevant files: setup/local_llm_setup.py, setup/local_llm_adapter.py, ice_lightrag/ice_rag.py, CLAUDE.md

# Local LLM Guide

This guide provides comprehensive instructions for setting up local LLM infrastructure using Ollama for cost-optimized ICE deployment, including hybrid configurations and integration patterns.

## Overview

Local LLM setup using Ollama can reduce ICE operational costs from $50-200/month to $0-7/month while maintaining good performance for financial analysis tasks.

## üöÄ Ollama Installation

### macOS/Linux Installation

```bash
# Install Ollama using the official installer
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version

# Start Ollama service (runs on localhost:11434)
ollama serve
```

### Windows Installation

```bash
# Download from https://ollama.com/download
# Run the installer and follow setup instructions

# Verify installation in Command Prompt or PowerShell
ollama --version

# Start Ollama service
ollama serve
```

### Docker Installation (Alternative)

```bash
# Run Ollama in Docker container
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Pull models
docker exec -it ollama ollama pull llama3.1:8b
```

## üì¶ Recommended Models for Financial Analysis

### Primary Models

```bash
# Best balance of speed/quality for financial reasoning
ollama pull llama3.1:8b          # 4.7GB - Recommended primary model

# Better financial domain understanding
ollama pull qwen2.5:7b           # 4.4GB - Alternative for complex financial analysis

# Lightweight option for development
ollama pull llama3.2:3b          # 2.0GB - Fast but lower quality
```

### Embedding Models (Optional for Full Local Setup)

```bash
# Local embeddings model (optional - can use OpenAI embeddings)
ollama pull nomic-embed-text     # 274MB - Local text embeddings

# Alternative embedding model
ollama pull all-minilm          # 46MB - Smaller but lower quality
```

### Testing Model Installation

```bash
# Test model functionality with financial query
ollama run llama3.1:8b "What is the P/E ratio and how is it calculated?"

# Test reasoning capability
ollama run qwen2.5:7b "Explain the relationship between interest rates and tech stock valuations"
```

## ‚öôÔ∏è Configuration Options

### Option 1: Pure Local Setup (Zero API Costs)

```bash
# Set environment variables for pure local setup
export LLM_MODE="local"
export OLLAMA_MODEL="llama3.1:8b"
export OLLAMA_EMBEDDING_MODEL="nomic-embed-text"
export OPENAI_API_KEY=""  # Not needed for pure local
```

**Benefits:**
- $0/month operational costs
- Complete data privacy
- No internet dependency for inference
- Unlimited usage

**Limitations:**
- Lower reasoning quality for complex financial analysis
- Slower processing for large documents
- Requires local compute resources

### Option 2: Hybrid Local/Remote Setup (Recommended)

```bash
# Hybrid setup: Local for extraction, OpenAI for reasoning
export LLM_MODE="hybrid"
export OLLAMA_MODEL="llama3.1:8b"
export OPENAI_API_KEY="sk-..."  # For complex reasoning only
export HYBRID_THRESHOLD="complex"  # When to use OpenAI
```

**Benefits:**
- ~$7/month costs (entity extraction free, reasoning paid)
- Good balance of cost and performance
- Privacy for entity extraction tasks
- High quality for complex financial reasoning

**Cost Breakdown:**
- Entity extraction: $0 (local)
- Complex reasoning: ~$5/month (OpenAI GPT-4o-mini)
- Embeddings: ~$2/month (OpenAI text-embedding-3-small)

### Option 3: Local + Cloud Embeddings

```bash
# Local LLM with cloud embeddings for better retrieval
export LLM_MODE="local"
export OLLAMA_MODEL="llama3.1:8b"
export OPENAI_API_KEY="sk-..."  # For embeddings only
export USE_LOCAL_EMBEDDINGS="false"
```

**Benefits:**
- ~$2-3/month costs (embeddings only)
- Better retrieval quality than local embeddings
- Complete LLM privacy

## üîÑ Provider Switching Methods

Once you have Ollama installed and models pulled, you can switch between providers using 3 different methods depending on your workflow.

### Method 1: Terminal Environment Variables (Recommended for Scripts)

**Best for**: Running Python scripts directly, automated workflows, production deployments

Set environment variables **before** starting Jupyter or running scripts:

**Switch to OpenAI** (default, $5/mo):
```bash
export OPENAI_API_KEY="sk-YOUR-API-KEY"
# LLM_PROVIDER defaults to "openai" if not set

python updated_architectures/implementation/ice_simplified.py
```

**Switch to Hybrid** (recommended, $2/mo, 60% savings):
```bash
export LLM_PROVIDER="ollama"
export EMBEDDING_PROVIDER="openai"
export OPENAI_API_KEY="sk-YOUR-API-KEY"

# Make sure Ollama is running
ollama serve  # In separate terminal if not already running

jupyter notebook ice_building_workflow.ipynb
```

**Switch to Full Ollama** ($0/mo, requires graph clearing):
```bash
export LLM_PROVIDER="ollama"
export EMBEDDING_PROVIDER="ollama"

# ‚ö†Ô∏è IMPORTANT: Clear graph if switching from OpenAI/Hybrid
# See graph clearing section below

jupyter notebook ice_building_workflow.ipynb
```

**Verify configuration**:
```bash
echo "LLM Provider: $LLM_PROVIDER"
echo "Embedding Provider: $EMBEDDING_PROVIDER"
echo "OpenAI Key Set: $([ -n "$OPENAI_API_KEY" ] && echo 'Yes' || echo 'No')"
```

---

### Method 2: Jupyter Notebook Cell (Recommended for Interactive Work)

**Best for**: Interactive development, testing, experimenting with different providers

**Location**: Both `ice_building_workflow.ipynb` and `ice_query_workflow.ipynb` include provider switching cells.

**How to use**:

1. **Find the switching cell** - Located after provider configuration docs:
   - `ice_building_workflow.ipynb` - Cell 7.5 (right after Cell 7)
   - `ice_query_workflow.ipynb` - Cell 5.5 (right after Cell 5)

2. **Uncomment ONE option**:

```python
# Provider Switching - Uncomment ONE option below, then restart kernel

# Option 1: OpenAI ($5/mo, highest quality)
# import os; os.environ['LLM_PROVIDER'] = 'openai'; os.environ['OPENAI_API_KEY'] = 'sk-YOUR-KEY'; print("‚úÖ Switched to OpenAI")

# Option 2: Hybrid ($2/mo, 60% savings, recommended)
# import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'openai'; os.environ['OPENAI_API_KEY'] = 'sk-YOUR-KEY'; print("‚úÖ Switched to Hybrid")

# Option 3: Full Ollama ($0/mo, requires graph clearing)
# import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'ollama'; print("‚úÖ Switched to Full Ollama - Clear graph if needed")
```

3. **Remove `#` from your chosen option**:

Example - switching to Hybrid:
```python
# Option 2: Hybrid ($2/mo, 60% savings, recommended)
import os; os.environ['LLM_PROVIDER'] = 'ollama'; os.environ['EMBEDDING_PROVIDER'] = 'openai'; os.environ['OPENAI_API_KEY'] = 'sk-YOUR-KEY'; print("‚úÖ Switched to Hybrid")
```

4. **Run the cell** - Press `Shift+Enter` or click Run button

5. **Restart kernel** - Go to `Kernel ‚Üí Restart Kernel` to apply changes

6. **Continue with workflow** - Run subsequent cells normally

**Advantages**:
- ‚úÖ No terminal commands needed
- ‚úÖ Changes documented in notebook
- ‚úÖ Easy to share configurations
- ‚úÖ Visual confirmation of active provider

---

### Method 3: Jupyter Magic Commands (Quick Switching)

**Best for**: Quick testing, temporary switches, debugging

**How to use**: Add at the top of any notebook cell:

**OpenAI**:
```python
%env LLM_PROVIDER=openai
%env OPENAI_API_KEY=sk-YOUR-API-KEY

print("‚úÖ Switched to OpenAI")
```

**Hybrid**:
```python
%env LLM_PROVIDER=ollama
%env EMBEDDING_PROVIDER=openai
%env OPENAI_API_KEY=sk-YOUR-API-KEY

print("‚úÖ Switched to Hybrid (Ollama LLM + OpenAI embeddings)")
```

**Full Ollama**:
```python
%env LLM_PROVIDER=ollama
%env EMBEDDING_PROVIDER=ollama

print("‚úÖ Switched to Full Ollama")
```

**Verify configuration**:
```python
import os
print(f"LLM Provider: {os.getenv('LLM_PROVIDER', 'openai')}")
print(f"Embedding Provider: {os.getenv('EMBEDDING_PROVIDER', os.getenv('LLM_PROVIDER', 'openai'))}")
print(f"OpenAI Key Set: {bool(os.getenv('OPENAI_API_KEY'))}")
```

**Advantages**:
- ‚úÖ Fastest method (one cell)
- ‚úÖ No kernel restart needed (usually)
- ‚úÖ Good for A/B testing

**Note**: For best results, restart kernel after switching to ensure clean provider initialization.

---

### When to Clear the Knowledge Graph

**CRITICAL**: Switching between providers may require clearing the graph depending on embedding dimensions.

**‚úÖ Clear graph when**:
- Switching **to** Full Ollama (from OpenAI or Hybrid)
  - Reason: 1536-dim ‚Üí 768-dim incompatibility

**‚ùå Don't clear when**:
- Switching between OpenAI ‚Üî Hybrid
  - Reason: Same 1536-dim embeddings
- Just testing different LLM providers (same embeddings)

**How to clear**:
- **Notebook**: `ice_building_workflow.ipynb` Cell 9 (uncomment clearing code)
- **Python**:
  ```python
  from pathlib import Path
  import shutil
  shutil.rmtree('src/ice_lightrag/storage')
  Path('src/ice_lightrag/storage').mkdir(parents=True, exist_ok=True)
  ```

**After clearing**: Re-run `ice_building_workflow.ipynb` to rebuild graph with new embedding dimension.

---

### Comparison Table

| Method | Best For | Speed | Persistence | Recommended For |
|--------|----------|-------|-------------|-----------------|
| **Terminal** | Scripts, automation | Fast | Session | Production, scripting |
| **Notebook Cell** | Interactive work | Fast | Notebook saved | Development, sharing |
| **Magic Commands** | Quick testing | Fastest | Temporary | A/B testing, debugging |

**Recommendation**: Use **Method 2 (Notebook Cell)** for most interactive work, **Method 1 (Terminal)** for scripts and automation.

---

## üîß Integration with ICE

### ICELightRAG Configuration

```python
from ice_lightrag.ice_rag import ICELightRAG
import os

# Configure ICE for local LLM usage
ice_rag = ICELightRAG(
    working_dir="./ice_lightrag/storage",
    llm_mode=os.getenv("LLM_MODE", "hybrid"),
    ollama_model=os.getenv("OLLAMA_MODEL", "llama3.1:8b"),
    use_local_embeddings=os.getenv("USE_LOCAL_EMBEDDINGS", "false").lower() == "true"
)

# Test the setup
result = ice_rag.query(
    "What are the key risks for NVIDIA?",
    mode="hybrid"
)
```

### Custom Ollama Integration

```python
import ollama
from typing import Optional, Dict, Any

class OllamaLLMAdapter:
    """Adapter for integrating Ollama with ICE LightRAG"""

    def __init__(self, model_name: str = "llama3.1:8b", host: str = "localhost:11434"):
        self.model_name = model_name
        self.host = host
        self.client = ollama.Client(host=host)

    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response using local Ollama model"""
        try:
            response = await self.client.chat(
                model=self.model_name,
                messages=[{'role': 'user', 'content': prompt}],
                options={
                    'temperature': kwargs.get('temperature', 0.1),
                    'top_p': kwargs.get('top_p', 0.9),
                    'num_ctx': kwargs.get('max_tokens', 4096)
                }
            )
            return response['message']['content']
        except Exception as e:
            raise Exception(f"Ollama generation failed: {e}")

    def is_available(self) -> bool:
        """Check if Ollama service is running and model is available"""
        try:
            models = self.client.list()
            return any(model['name'] == self.model_name for model in models.get('models', []))
        except:
            return False
```

## üìä Performance Optimization

### Model-Specific Optimizations

```python
# Optimized parameters for different Ollama models
MODEL_CONFIGS = {
    "llama3.1:8b": {
        "temperature": 0.1,
        "top_p": 0.9,
        "num_ctx": 4096,
        "repeat_penalty": 1.1,
        "recommended_batch_size": 1
    },
    "qwen2.5:7b": {
        "temperature": 0.05,  # More deterministic for financial analysis
        "top_p": 0.8,
        "num_ctx": 8192,      # Better context for complex documents
        "repeat_penalty": 1.05,
        "recommended_batch_size": 1
    },
    "llama3.2:3b": {
        "temperature": 0.2,
        "top_p": 0.9,
        "num_ctx": 2048,      # Smaller context window
        "repeat_penalty": 1.2,
        "recommended_batch_size": 2
    }
}
```

### Hardware Requirements

```bash
# Minimum requirements for different models
# llama3.1:8b:  8GB RAM, 4GB VRAM (GPU optional)
# qwen2.5:7b:   8GB RAM, 4GB VRAM (GPU optional)
# llama3.2:3b:  4GB RAM, 2GB VRAM (GPU optional)

# Check system resources
free -h                    # Linux
vm_stat                    # macOS
Get-ComputerInfo          # Windows PowerShell
```

### GPU Acceleration (Optional)

```bash
# Enable GPU acceleration if NVIDIA GPU available
export OLLAMA_GPU=1

# Check GPU usage during inference
nvidia-smi

# Monitor Ollama GPU usage
ollama ps
```

## üîÄ Hybrid Decision Logic

### Automatic Task Classification

```python
def classify_task_complexity(prompt: str) -> str:
    """Classify prompt complexity to decide local vs remote processing"""

    # Simple tasks for local processing
    simple_patterns = [
        'extract', 'list', 'identify', 'find', 'what is',
        'define', 'entities', 'relationships', 'keywords'
    ]

    # Complex tasks requiring OpenAI
    complex_patterns = [
        'analyze', 'compare', 'evaluate', 'reasoning', 'strategy',
        'implications', 'forecast', 'recommend', 'synthesize'
    ]

    prompt_lower = prompt.lower()

    if any(pattern in prompt_lower for pattern in complex_patterns):
        return "complex"
    elif any(pattern in prompt_lower for pattern in simple_patterns):
        return "simple"
    else:
        return "medium"  # Default to local for cost efficiency

# Usage in hybrid setup
async def hybrid_llm_processing(prompt: str) -> str:
    complexity = classify_task_complexity(prompt)

    if complexity == "complex" and os.getenv("OPENAI_API_KEY"):
        # Use OpenAI for complex financial reasoning
        return await openai_generate(prompt)
    else:
        # Use local Ollama for simple/medium tasks
        return await ollama_generate(prompt)
```

## üõ†Ô∏è Troubleshooting

### Common Issues

**Ollama Service Not Running:**
```bash
# Check if Ollama is running
ps aux | grep ollama

# Start Ollama service
ollama serve

# Check service status
curl http://localhost:11434/api/version
```

**Model Not Found:**
```bash
# List available models
ollama list

# Pull missing model
ollama pull llama3.1:8b

# Remove and re-pull corrupted model
ollama rm llama3.1:8b
ollama pull llama3.1:8b
```

**Memory Issues:**
```bash
# Monitor memory usage during inference
top -p $(pgrep ollama)

# Reduce model context size if OOM errors
export OLLAMA_NUM_CTX=2048  # Reduce from default 4096
```

**Slow Performance:**
```bash
# Enable GPU acceleration
export OLLAMA_GPU=1

# Increase thread count for CPU inference
export OLLAMA_NUM_THREAD=8

# Use smaller model for faster inference
ollama pull llama3.2:3b
```

### Health Monitoring

```python
import requests
import asyncio

async def check_ollama_health() -> Dict[str, Any]:
    """Monitor Ollama service health"""
    try:
        # Check service availability
        response = requests.get("http://localhost:11434/api/version", timeout=5)
        service_healthy = response.status_code == 200

        # Check model availability
        models_response = requests.get("http://localhost:11434/api/tags", timeout=5)
        models = models_response.json().get('models', []) if models_response.status_code == 200 else []

        return {
            "service_healthy": service_healthy,
            "models_available": len(models),
            "models": [model['name'] for model in models],
            "version": response.json().get('version', 'unknown') if service_healthy else 'unknown'
        }
    except Exception as e:
        return {
            "service_healthy": False,
            "error": str(e),
            "models_available": 0
        }

# Usage
health_status = await check_ollama_health()
print(f"Ollama Health: {health_status}")
```

## üí∞ Cost Analysis

### Monthly Cost Comparison

| Configuration | LLM Costs | Embedding Costs | Total/Month | Use Case |
|---------------|-----------|-----------------|-------------|----------|
| **Pure Local** | $0 | $0 | **$0** | Development, privacy-focused |
| **Hybrid Recommended** | $5 | $2 | **$7** | Small fund, balanced performance |
| **Local + Cloud Embeddings** | $0 | $3 | **$3** | Privacy with good retrieval |
| **Full Cloud** | $50 | $8 | **$58** | Production, maximum performance |

### ROI Calculation

```python
# Calculate savings from local LLM deployment
def calculate_local_llm_savings():
    """Calculate monthly savings from local LLM vs cloud-only"""

    cloud_only_costs = {
        "llm_processing": 50,    # GPT-4 for all tasks
        "embeddings": 8,         # text-embedding-3-large
        "api_calls": 12          # API overhead
    }

    hybrid_local_costs = {
        "llm_processing": 5,     # GPT-4o-mini for complex only
        "embeddings": 2,         # text-embedding-3-small
        "api_calls": 0           # Local processing
    }

    monthly_savings = sum(cloud_only_costs.values()) - sum(hybrid_local_costs.values())
    annual_savings = monthly_savings * 12

    return {
        "monthly_savings": monthly_savings,
        "annual_savings": annual_savings,
        "cost_reduction_percentage": (monthly_savings / sum(cloud_only_costs.values())) * 100
    }

# Results: ~$63/month savings (~90% cost reduction)
```

## üîó Integration Scripts

### Automated Setup Script

```bash
#!/bin/bash
# setup_local_llm.sh - Automated Ollama setup for ICE

echo "üöÄ Setting up Ollama for ICE Investment Context Engine"

# Install Ollama
if ! command -v ollama &> /dev/null; then
    echo "Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# Start Ollama service
echo "Starting Ollama service..."
ollama serve &
sleep 5

# Pull recommended models
echo "Downloading recommended models..."
ollama pull llama3.1:8b
ollama pull qwen2.5:7b

# Optional: Pull embedding model for full local setup
read -p "Install local embedding model? (y/n): " install_embeddings
if [[ $install_embeddings == "y" ]]; then
    ollama pull nomic-embed-text
fi

# Set environment variables
echo "Setting up environment variables..."
echo 'export LLM_MODE="hybrid"' >> ~/.bashrc
echo 'export OLLAMA_MODEL="llama3.1:8b"' >> ~/.bashrc

echo "‚úÖ Ollama setup complete! Restart your terminal to use new environment variables."
```

## üè∑Ô∏è Hybrid Entity Categorization (Qwen2.5-3B)

**Purpose**: Improve entity categorization accuracy using small local LLM for ambiguous cases

### Architecture

```
Categorization Pipeline:
1. Keyword Matching (Fast) ‚Üí 85-90% of entities (1ms each)
2. LLM Fallback (Accurate) ‚Üí 10-15% ambiguous cases (40ms each)
3. Total: ~5s overhead per 100 entities
```

### Results

- **Accuracy**: 100% (vs 82% keyword-only)
- **Speed**: 41ms per entity average
- **LLM Usage**: Only 18% of entities (high efficiency)
- **Cost**: $0 (local Qwen2.5-3B)

### Configuration

**File**: `src/ice_lightrag/graph_categorization.py`

```python
# Configuration constants
CATEGORIZATION_MODE = 'keyword'  # 'keyword' | 'hybrid' | 'llm'
HYBRID_CONFIDENCE_THRESHOLD = 0.70  # LLM fallback trigger
OLLAMA_MODEL = 'qwen2.5:3b'  # Small, fast, accurate
OLLAMA_HOST = 'http://localhost:11434'
```

### Usage

```python
from src.ice_lightrag.graph_categorization import (
    categorize_entity_with_confidence,  # Returns (category, confidence)
    categorize_entity_hybrid  # Hybrid: keyword + LLM fallback
)

# Keyword with confidence
category, conf = categorize_entity_with_confidence("NVIDIA Corporation")
# ‚Üí ("Company", 0.95)

# Hybrid mode (auto LLM fallback for low confidence)
category, conf = categorize_entity_hybrid("Goldman Sachs", confidence_threshold=0.70)
# ‚Üí ("Company", 0.90) - Used LLM
```

### Testing

```bash
cd /path/to/project
python src/ice_lightrag/test_hybrid_categorization.py
```

**Expected Output**:
- Keyword-only: 82% accuracy
- Hybrid mode: 100% accuracy
- LLM calls: ~18% of entities

### Why Qwen2.5-3B?

| Model | Size | Speed | Accuracy | Reason |
|-------|------|-------|----------|--------|
| **Qwen2.5-3B** | 1.9GB | 41ms | 100% | ‚úÖ Recommended: Best balance |
| Qwen2.5-7B | 4.7GB | 120ms | 100% | Slower, same accuracy |
| Phi-3-mini | 2.2GB | 50ms | 95% | Less financial knowledge |

**Qwen2.5-3B advantages**:
- Small memory footprint (1.9GB)
- Fast inference (41ms vs 120ms for 7B)
- Financial domain training (Alibaba dataset)
- Perfect for 9/10 category classification

---

**Related Documentation:**
- [LightRAG Setup](LIGHTRAG_SETUP.md) - Complete LightRAG configuration guide
- [Query Patterns](QUERY_PATTERNS.md) - Query optimization and mode selection
- [CLAUDE.md](../CLAUDE.md) - Main development guide
- [Setup Scripts](../setup/) - Automated configuration utilities